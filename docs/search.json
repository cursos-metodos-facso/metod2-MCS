[
  {
    "objectID": "trabajos.html",
    "href": "trabajos.html",
    "title": "Trabajos",
    "section": "",
    "text": "La presente evaluación tiene por objetivo que las y los estudiantes apliquen de forma integral los contenidos del curso a una temática de interés específica utilizando la base de datos del Estudio Social Longitudinal de Chile (ELSOC) 2022. Se espera un ejercicio de investigación que logre dar cuenta del aprendizaje de las herramientas de análisis estadístico que configuran las habilidades básicas para desarrollar procesos de investigación social, analizando fuentes de información de carácter cuantitativo desde una perspectiva sociológica.\n\n\n\nEl trabajo debe ser realizado en parejas.\nSe realizará una entrega final tipo reporte de investigación breve.\nSe recomienda que elaboren un problema de investigación que trate sobre las características sociales de algún problema actual de la sociedad chilena, y que sea factible de investigar con los datos del Estudio Social Longitudinal de Chile (ELSOC) 2022.\n\n\n\n\nPara la entrega del informe se espera que, habiendo seleccionado un fenómeno específico de interés, se desarrollen los siguientes temas:\n\n\n\n\n\n\n\n\nComponente\nDetalle\nPuntaje\n\n\n\n\nFormulación del problema\nFormular una pregunta y un objetivo general de investigación. Fundamentar el interés sociológico que habilita el estudio de la temática elegida, utilizando al menos 3 referencias bibliográficas. El objetivo de investigación formulado debe permitir entender cómo se analizará la temática elegida. (1 plana)\n2.0\n\n\nDefinición base de datos\nDefinir la fuente de información a utilizar indicando el nombre de la base de datos y la institución que la disponibiliza, la población que busca representar el estudio, el procedimiento de muestreo utilizado y el tamaño de la muestra. Dado que trabajaremos solo con el Estudio Social Longitudinal de Chile (ELSOC) 2022, se espera que las parejas describan qué busca medir y representar el módulo (o selección de variables) elegido. (½ plana)\n1.0\n\n\nOperacionalización y descripción de variables\nOperacionalizar correctamente las variables de interés. Esta sección también incluye una tabla de descriptivos básicos, y una descripción detallada de la operacionalización y medición de las variables. - Tabla de descriptivos: etiquetas claras, debe ser posible identificar cada variable - Descripción de la operacionalización de cada variable y recodificación apropiada (por ejemplo, de menos a más presencia del atributo medido)\n3.0\n\n\nInferencia y Correlación\nEstimar, visualizar e interpretar los coeficientes de correlación entre variables. - Matriz de correlaciones (tabla): 1 - interpretación de tamaño de efecto: 1 - inferencia: 1\n3.0\n\n\nRegresión lineal\nEstimar, visualizar e interpretar coeficientes de regresión lineal - tabla de regresión: 0.5 - interpretación de coeficientes (interpretación de efectos de interacción es opcional, pero se valorará positivamente): 1 - inferencia/significación: 1 - ajuste global del modelo (R2): 0.5.\n3.0\n\n\nRegresión logística\nEstimar, visualizar e interpretar coeficientes de regresión logística - tabla de regresión: 0.5 - interpretación coeficientes (interpretación de efectos de interacción es opcional, pero se valorará positivamente): 1 - inferencia: 1 - ajuste: 0.5\n3.0\n\n\nConclusiones\nEn las conclusiones se deben sintetizar los aspectos más relevantes de los análisis estadísticos ya expuestos, articulando toda la información trabajada debe presentarse una respuesta tentativa a la pregunta de investigación. Además, debe reflexionar sobre los límites de su diseño de investigación y señalar posibles ejes de investigación que podrían ser considerados en futuras indagaciones (1 plana).\n2.0\n\n\nTrabajo en R\nAdemás del informe, entregar la carpeta con el Proyecto R, que contenga un archivo .Rproject, y las carpetas input, procesamiento y output. - La carpeta input debe contener la base de datos, manual de usuario, libro de códigos. - La carpeta procesamiento debe contener un archivo de sintaxis para el procesamiento y otro para el análisis de los datos. - La carpeta output debe contener la base de datos procesada y el resto de salidas asociadas (tablas, gráficos, etc.). - Se evaluará que los códigos utilizados generen las salidas (tablas, gráficos, etc.) que se presentan en el informe.\n1.0\n\n\n\n\n\n\nPara la construcción del reporte de investigación por favor considere:\n● El trabajo debe tener una portada que incluya: título, logo de la universidad, nombre de los estudiantes, profesor/a, fecha y ayudantes.\n● Debe incluirse un índice. Tablas, referencias y bibliografía utilizada debe presentarse en formato APA.\n● La fuente a utilizar debe ser Letra Times New Roman 12, interlineado simple y justificado. Notas a pie de página en tamaño 10, en mismo formato que el texto central.\n● El trabajo debe tener una redacción adecuada y sin errores de ortografía.\n● Se descontarán hasta 5 décimas sobre la nota final por errores de este tipo.\n\n\n\nEl formato de entrega del informe debe ser un documento en formato PDF (.pdf), que debe estar alojado en la carpeta output del Proyecto R. Fecha de entrega: jueves 18 de julio hasta las 12:00AM vía módulo Tareas en plataforma U-Cursos.\n● Entregas atrasadas hasta las 23:59 del jueves 18 de julio tendrán 0,5 puntos de descuento sobre la nota final.\n● Entregas atrasadas hasta el viernes 19 de julio hasta las 23:59 tendrán 1,0 punto de descuento sobre la nota final. No serán evaluadas entregas posteriores a esta fecha.\n\n\n\nTodos los trabajos se procesan en software para detección de plagio: evidencia de una situación de plagio implica obtención de la nota mínima en la evaluación (1,0) junto con constituirse como causal de reprobación de la asignatura.\n\n\n\n● Máxima de escritura: una idea por párrafo. Si comienza una idea nueva, se recomienda comenzar otro párrafo. Al revés, si el párrafo siguiente habla de lo mismo, sumarlo al párrafo anterior.\n● La idea del párrafo se resume en la primera parte del párrafo, lo que en inglés se llama “topic sentence”.\n● Declarar domicilio disciplinar: ej, mencionar la palabra “sociología” en el primer/segundo párrafo, esto fuerza que la investigación se enmarque en la disciplina.\nEn caso de tener dudas, no dude en contactar a sus ayudantes respectivos, o bien, vía foro U-Cursos al equipo docente de la asignatura."
  },
  {
    "objectID": "trabajos.html#cuestiones-a-considerar",
    "href": "trabajos.html#cuestiones-a-considerar",
    "title": "Trabajos",
    "section": "",
    "text": "El trabajo debe ser realizado en parejas.\nSe realizará una entrega final tipo reporte de investigación breve.\nSe recomienda que elaboren un problema de investigación que trate sobre las características sociales de algún problema actual de la sociedad chilena, y que sea factible de investigar con los datos del Estudio Social Longitudinal de Chile (ELSOC) 2022."
  },
  {
    "objectID": "trabajos.html#instrucciones-para-el-informe",
    "href": "trabajos.html#instrucciones-para-el-informe",
    "title": "Trabajos",
    "section": "",
    "text": "Para la entrega del informe se espera que, habiendo seleccionado un fenómeno específico de interés, se desarrollen los siguientes temas:\n\n\n\n\n\n\n\n\nComponente\nDetalle\nPuntaje\n\n\n\n\nFormulación del problema\nFormular una pregunta y un objetivo general de investigación. Fundamentar el interés sociológico que habilita el estudio de la temática elegida, utilizando al menos 3 referencias bibliográficas. El objetivo de investigación formulado debe permitir entender cómo se analizará la temática elegida. (1 plana)\n2.0\n\n\nDefinición base de datos\nDefinir la fuente de información a utilizar indicando el nombre de la base de datos y la institución que la disponibiliza, la población que busca representar el estudio, el procedimiento de muestreo utilizado y el tamaño de la muestra. Dado que trabajaremos solo con el Estudio Social Longitudinal de Chile (ELSOC) 2022, se espera que las parejas describan qué busca medir y representar el módulo (o selección de variables) elegido. (½ plana)\n1.0\n\n\nOperacionalización y descripción de variables\nOperacionalizar correctamente las variables de interés. Esta sección también incluye una tabla de descriptivos básicos, y una descripción detallada de la operacionalización y medición de las variables. - Tabla de descriptivos: etiquetas claras, debe ser posible identificar cada variable - Descripción de la operacionalización de cada variable y recodificación apropiada (por ejemplo, de menos a más presencia del atributo medido)\n3.0\n\n\nInferencia y Correlación\nEstimar, visualizar e interpretar los coeficientes de correlación entre variables. - Matriz de correlaciones (tabla): 1 - interpretación de tamaño de efecto: 1 - inferencia: 1\n3.0\n\n\nRegresión lineal\nEstimar, visualizar e interpretar coeficientes de regresión lineal - tabla de regresión: 0.5 - interpretación de coeficientes (interpretación de efectos de interacción es opcional, pero se valorará positivamente): 1 - inferencia/significación: 1 - ajuste global del modelo (R2): 0.5.\n3.0\n\n\nRegresión logística\nEstimar, visualizar e interpretar coeficientes de regresión logística - tabla de regresión: 0.5 - interpretación coeficientes (interpretación de efectos de interacción es opcional, pero se valorará positivamente): 1 - inferencia: 1 - ajuste: 0.5\n3.0\n\n\nConclusiones\nEn las conclusiones se deben sintetizar los aspectos más relevantes de los análisis estadísticos ya expuestos, articulando toda la información trabajada debe presentarse una respuesta tentativa a la pregunta de investigación. Además, debe reflexionar sobre los límites de su diseño de investigación y señalar posibles ejes de investigación que podrían ser considerados en futuras indagaciones (1 plana).\n2.0\n\n\nTrabajo en R\nAdemás del informe, entregar la carpeta con el Proyecto R, que contenga un archivo .Rproject, y las carpetas input, procesamiento y output. - La carpeta input debe contener la base de datos, manual de usuario, libro de códigos. - La carpeta procesamiento debe contener un archivo de sintaxis para el procesamiento y otro para el análisis de los datos. - La carpeta output debe contener la base de datos procesada y el resto de salidas asociadas (tablas, gráficos, etc.). - Se evaluará que los códigos utilizados generen las salidas (tablas, gráficos, etc.) que se presentan en el informe.\n1.0"
  },
  {
    "objectID": "trabajos.html#aspectos-formales",
    "href": "trabajos.html#aspectos-formales",
    "title": "Trabajos",
    "section": "",
    "text": "Para la construcción del reporte de investigación por favor considere:\n● El trabajo debe tener una portada que incluya: título, logo de la universidad, nombre de los estudiantes, profesor/a, fecha y ayudantes.\n● Debe incluirse un índice. Tablas, referencias y bibliografía utilizada debe presentarse en formato APA.\n● La fuente a utilizar debe ser Letra Times New Roman 12, interlineado simple y justificado. Notas a pie de página en tamaño 10, en mismo formato que el texto central.\n● El trabajo debe tener una redacción adecuada y sin errores de ortografía.\n● Se descontarán hasta 5 décimas sobre la nota final por errores de este tipo."
  },
  {
    "objectID": "trabajos.html#fecha-y-formato-de-entrega",
    "href": "trabajos.html#fecha-y-formato-de-entrega",
    "title": "Trabajos",
    "section": "",
    "text": "El formato de entrega del informe debe ser un documento en formato PDF (.pdf), que debe estar alojado en la carpeta output del Proyecto R. Fecha de entrega: jueves 18 de julio hasta las 12:00AM vía módulo Tareas en plataforma U-Cursos.\n● Entregas atrasadas hasta las 23:59 del jueves 18 de julio tendrán 0,5 puntos de descuento sobre la nota final.\n● Entregas atrasadas hasta el viernes 19 de julio hasta las 23:59 tendrán 1,0 punto de descuento sobre la nota final. No serán evaluadas entregas posteriores a esta fecha."
  },
  {
    "objectID": "trabajos.html#sobre-plagio",
    "href": "trabajos.html#sobre-plagio",
    "title": "Trabajos",
    "section": "",
    "text": "Todos los trabajos se procesan en software para detección de plagio: evidencia de una situación de plagio implica obtención de la nota mínima en la evaluación (1,0) junto con constituirse como causal de reprobación de la asignatura."
  },
  {
    "objectID": "trabajos.html#recomendaciones-para-la-entrega",
    "href": "trabajos.html#recomendaciones-para-la-entrega",
    "title": "Trabajos",
    "section": "",
    "text": "● Máxima de escritura: una idea por párrafo. Si comienza una idea nueva, se recomienda comenzar otro párrafo. Al revés, si el párrafo siguiente habla de lo mismo, sumarlo al párrafo anterior.\n● La idea del párrafo se resume en la primera parte del párrafo, lo que en inglés se llama “topic sentence”.\n● Declarar domicilio disciplinar: ej, mencionar la palabra “sociología” en el primer/segundo párrafo, esto fuerza que la investigación se enmarque en la disciplina.\nEn caso de tener dudas, no dude en contactar a sus ayudantes respectivos, o bien, vía foro U-Cursos al equipo docente de la asignatura."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Planificación",
    "section": "",
    "text": "Los dos componentes centrales del curso son las clases teóricas y las actividades prácticas. Las clases se realizarán los días Viernes 09:00 a 10:50 en sala 329\n\nClases ( ): Lecturas, documentos de presentación y video (en caso que la sesión sea grabada)\nPrácticas y evaluaciones (): Actividades prácticas a desarrollar durante la semana.\nLecturas (): Llegar a la clase con los textos leídos.\n\n\n\n\n\n Clases\n Prácticas y evaluaciones\n Lecturas y material adicional\n\n\n\n\n Marzo \n\n\n\n\n\nViernes 17\n00. Presentación e 01. introducción\n1. Aproximación inicial a R\n- Leer detalladamente programa del curso\n\n\n\n\nUNIDAD 1: Estadística descriptiva\n\n\n\nViernes 24\n2.Operacionalización de variables \n2.Operacionalización de variables\n- Wickham & Grolemund, (2017). cap. 1 Introducción \n\n\nViernes 31\n3. Visualización de datos\n-\n- Wickham & Grolemund, (2017). cap. 2 Explorar\n\n\n Abril \n\n\n\n\n\nViernes 07\nViernes santo\n\n\n\n\nViernes 14\n1° semana presencial\n\n\n\n\nViernes 21\n\nEvaluación\n\n\n\n\n\n\nUNIDAD 2: Estadística Correlacional\n\n\n\nViernes 28\n4. Introducción a la inferencia estadística\n\n\n\n\n Mayo \n\n\n\n\n\nViernes 05\n1° semana de pausa reflexiva\n\n\n\n\nViernes 12\n5. Correlación\n\n\n\n\nViernes 19\n6. Índices y análisis factorial\n\n\n\n\nViernes 26\n2° semana presencial\n\n\n\n\n\n\n\nUNIDAD 3: Regresión lineal y regresión logística\n\n\n\n Junio \n\n\n\n\n\nViernes 02\n7. Regresión lineal de mínimos cuadrados\n\n\n\n\nViernes 09\n2° semana de pausa reflexiva\n\n\n\n\nViernes 16\n**8. Regresión logística binaria: interpretación de coeficientes y cálculo de probabilidades\n\n\n\n\nViernes 23\n10. Análisis factorial y regresión lineal en R\n\n\n\n\nViernes 30\n11. Supuestos de regresión y valores predichos\n\n\n\n\n Julio \n\n\n\n\n\nViernes 07\n3° semana presencial\n\n\n\n\nViernes 14\n\nEvaluación"
  },
  {
    "objectID": "resource/07-resource.html",
    "href": "resource/07-resource.html",
    "title": "Práctica 7 Correlación y regresióna",
    "section": "",
    "text": "La siguiente práctica tiene el objetivo de repasar en la interpretación de coeficientes de correlación y la construcción de índices, así como también en la interpretación de coeficientes de regresión lineal y logística. Para ello, utilizaremos la base de datos de la tercera ola del Estudio Longitudinal Social del Chile 2018 con el objetivo de analizar los determinantes de la Participación Ciudadana.\nLa versión original de este ejercicio proviene del curso de Estadística multivariada versión 2022."
  },
  {
    "objectID": "resource/07-resource.html#explorar-datos",
    "href": "resource/07-resource.html#explorar-datos",
    "title": "Práctica 7 Correlación y regresióna",
    "section": "Explorar datos",
    "text": "Explorar datos\nA partir de la siguiente tabla se obtienen estadísticos descriptivos que luego serán relevantes para realizar las transformaciones y análisis posteriores.\n\nview_df(elsoc,max.len = 50)\n\n\n\nData frame: elsoc\n\n\n\n\n\n\n\n\n\nID\nName\nLabel\nValues\nValue Labels\n\n\n1\nsexo\nSexo entrevistado\n0\n1\nHombre\nMujer\n\n\n2\nedad\nEdad entrevistado\nrange: 18-90\n\n\n3\neduc\nNivel educacional\n1\n2\n3\n4\n5\nPrimaria incompleta menos\nPrimaria y secundaria baja\nSecundaria alta\nTerciaria ciclo corto\nTerciaria y Postgrado\n\n\n4\npospol\nAutoubicacion escala izquierda-derecha\n1\n2\n3\n4\nDerecha\nCentro\nIzquierda\nIndep./Ninguno\n\n\n5\npart01\nFrecuencia: Firma carta o peticion apoyando causa\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n6\npart02\nFrecuencia: Asiste a mbackground-color:#eeeeeeha o manifestacion\npacifica\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n7\npart03\nFrecuencia: Participa en huelga\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n8\npart04\nFrecuencia: Usa redes sociales para opinar en\ntemas publicos\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n9\ninghogar\nIngreso total del hogar\nrange: 30000-17000000\n\n\n10\ninghogar_t\nIngreso total del hogar (en tramos)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nMenos de $220.000 mensuales liquidos\nDe $220.001 a $280.000 mensuales liquidos\nDe $280.001 a $330.000 mensuales liquidos\nDe $330.001 a $380.000 mensuales liquidos\nDe $380.001 a $420.000 mensuales liquidos\nDe $420.001 a $470.000 mensuales liquidos\nDe $470.001 a $510.000 mensuales liquidos\nDe $510.001 a $560.000 mensuales liquidos\nDe $560.001 a $610.000 mensuales liquidos\nDe $610.001 a $670.000 mensuales liquidos\nDe $670.001 a $730.000 mensuales liquidos\nDe $730.001 a $800.000 mensuales liquidos\nDe $800.001 a $890.000 mensuales liquidos\nDe $890.001 a $980.000 mensuales liquidos\nDe $980.001 a $1.100.000 mensuales liquidos\nDe $1.100.001 a $1.260.000 mensuales liquidos\nDe $1.260.001 a $1.490.000 mensuales liquidos\nDe $1.490.001 a $1.850.000 mensuales liquidos\nDe $1.850.001 a $2.700.000 mensuales liquidos\nMas de $2.700.000 a mensuales liquidos\n\n\n11\ntamhogar\nHabitantes del hogar\nrange: 1-14"
  },
  {
    "objectID": "resource/07-resource.html#variable-dependiente-participación-política",
    "href": "resource/07-resource.html#variable-dependiente-participación-política",
    "title": "Práctica 7 Correlación y regresióna",
    "section": "Variable dependiente: participación política",
    "text": "Variable dependiente: participación política\n\nplot_stackfrq(elsoc[,c(\"part01\",\"part02\",\"part03\",\"part04\")]) + theme(legend.position=\"bottom\")\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(cor(select(elsoc,part01,part02,part03,part04),\n                   use = \"complete.obs\"))\n\n\n\n\n\n\n\n\n\nelsoc &lt;- elsoc %&gt;% mutate(partpol=rowSums(select(., part01,part02,part03,part04)))\nsummary(elsoc$partpol)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  4.000   4.000   4.000   5.473   6.000  20.000       8"
  },
  {
    "objectID": "resource/07-resource.html#variable-independiente-ingresos",
    "href": "resource/07-resource.html#variable-independiente-ingresos",
    "title": "Práctica 7 Correlación y regresióna",
    "section": "Variable independiente: ingresos",
    "text": "Variable independiente: ingresos\ningresos hogar variable continua\n\nsummary(elsoc$inghogar)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n   30000   300000   500000   678843   800000 17000000      668 \n\n\ningreso hogar en tramos\n\nsjmisc::frq(elsoc$inghogar_t,\n            out = \"txt\",\n            show.na = T) %&gt;% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nval\nlabel\nfrq\nraw.prc\nvalid.prc\ncum.prc\n\n\n\n\n1\nMenos de $220.000 mensuales liquidos\n62\n1.65\n13.00\n13.00\n\n\n2\nDe $220.001 a $280.000 mensuales liquidos\n46\n1.23\n9.64\n22.64\n\n\n3\nDe $280.001 a $330.000 mensuales liquidos\n57\n1.52\n11.95\n34.59\n\n\n4\nDe $330.001 a $380.000 mensuales liquidos\n40\n1.07\n8.39\n42.98\n\n\n5\nDe $380.001 a $420.000 mensuales liquidos\n38\n1.01\n7.97\n50.94\n\n\n6\nDe $420.001 a $470.000 mensuales liquidos\n37\n0.99\n7.76\n58.70\n\n\n7\nDe $470.001 a $510.000 mensuales liquidos\n27\n0.72\n5.66\n64.36\n\n\n8\nDe $510.001 a $560.000 mensuales liquidos\n15\n0.40\n3.14\n67.51\n\n\n9\nDe $560.001 a $610.000 mensuales liquidos\n24\n0.64\n5.03\n72.54\n\n\n10\nDe $610.001 a $670.000 mensuales liquidos\n12\n0.32\n2.52\n75.05\n\n\n11\nDe $670.001 a $730.000 mensuales liquidos\n15\n0.40\n3.14\n78.20\n\n\n12\nDe $730.001 a $800.000 mensuales liquidos\n16\n0.43\n3.35\n81.55\n\n\n13\nDe $800.001 a $890.000 mensuales liquidos\n8\n0.21\n1.68\n83.23\n\n\n14\nDe $890.001 a $980.000 mensuales liquidos\n14\n0.37\n2.94\n86.16\n\n\n15\nDe $980.001 a $1.100.000 mensuales liquidos\n14\n0.37\n2.94\n89.10\n\n\n16\nDe $1.100.001 a $1.260.000 mensuales liquidos\n10\n0.27\n2.10\n91.19\n\n\n17\nDe $1.260.001 a $1.490.000 mensuales liquidos\n7\n0.19\n1.47\n92.66\n\n\n18\nDe $1.490.001 a $1.850.000 mensuales liquidos\n11\n0.29\n2.31\n94.97\n\n\n19\nDe $1.850.001 a $2.700.000 mensuales liquidos\n14\n0.37\n2.94\n97.90\n\n\n20\nMas de $2.700.000 a mensuales liquidos\n10\n0.27\n2.10\n100.00\n\n\nNA\nNA\n3271\n87.27\nNA\nNA\n\n\n\n\n\n\n\n\n\npodemos obtener la mediana de cada tramo\n\nelsoc$inghogar_t[elsoc$inghogar_t==1] &lt;-(       220000 )    # [1]  \"Menos de $220.000 mensuales liquidos\"          \nelsoc$inghogar_t[elsoc$inghogar_t==2] &lt;-(220001 +280000 )/2 # [2]  \"De $220.001 a $280.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==3] &lt;-(280001 +330000 )/2 # [3]  \"De $280.001 a $330.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==4] &lt;-(330001 +380000 )/2 # [4]  \"De $330.001 a $380.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==5] &lt;-(380001 +420000 )/2 # [5]  \"De $380.001 a $420.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==6] &lt;-(420001 +470000 )/2 # [6]  \"De $420.001 a $470.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==7] &lt;-(470001 +510000 )/2 # [7]  \"De $470.001 a $510.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==8] &lt;-(510001 +560000 )/2 # [8]  \"De $510.001 a $560.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==9] &lt;-(560001 +610000 )/2 # [9]  \"De $560.001 a $610.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==10]&lt;-(610001 +670000 )/2 # [10] \"De $610.001 a $670.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==11]&lt;-(670001 +730000 )/2 # [11] \"De $670.001 a $730.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==12]&lt;-(730001 +800000 )/2 # [12] \"De $730.001 a $800.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==13]&lt;-(800001 +890000 )/2 # [13] \"De $800.001 a $890.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==14]&lt;-(890001 +980000 )/2 # [14] \"De $890.001 a $980.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==15]&lt;-(980001 +1100000)/2 # [15] \"De $980.001 a $1.100.000 mensuales liquidos\"      \nelsoc$inghogar_t[elsoc$inghogar_t==16]&lt;-(1100001+1260000)/2 # [16] \"De $1.100.001 a $1.260.000 mensuales liquidos\"    \nelsoc$inghogar_t[elsoc$inghogar_t==17]&lt;-(1260001+1490000)/2 # [17] \"De $1.260.001 a $1.490.000 mensuales liquidos\"    \nelsoc$inghogar_t[elsoc$inghogar_t==18]&lt;-(1490001+1850000)/2 # [18] \"De $1.490.001 a $1.850.000 mensuales liquidos\"    \nelsoc$inghogar_t[elsoc$inghogar_t==19]&lt;-(1850001+2700000)/2 # [19] \"De $1.850.001 a $2.700.000 mensuales liquidos\"    \nelsoc$inghogar_t[elsoc$inghogar_t==20]&lt;-(2700000)           # [20] \"Mas de $2.700.000 a mensuales liquidos\"\n\ny luego imputar este valor medio a los casos NA\n\nelsoc$inghogar_i &lt;- ifelse(test = (is.na(elsoc$inghogar)), #¿existen NA en ingresos?\n                           yes = elsoc$inghogar_t,         #VERDADERO, remplazar con la media del tramo\n                           no = elsoc$inghogar)            #FALSE, mantener la variable original.\n\nelsoc$inghogar_i &lt;- set_label(elsoc$inghogar_i,\"Ingreso total del hogar (imputada)\")\n\n\nelsoc$ing_pcap &lt;- elsoc$inghogar_i/elsoc$tamhogar\nelsoc$ing_pcap &lt;- set_label(elsoc$ing_pcap,\"Ingreso per cápita del hogar\")\n\n\nelsoc$quintile&lt;- dplyr::ntile(x = elsoc$ing_pcap,\n                              n = 5) # n de categorias, para quintiles usamos 5 \nelsoc$quintile &lt;- factor(elsoc$quintile,c(1,2,3,4,5), c(\"Quintil 1\",\"Quintil 2\",\"Quintil 3\",\"Quintil 4\",\"Quintil 5\")) \nelsoc %&gt;% \n  group_by(quintile) %&gt;% \n  summarise(n=n(),\n            Media=mean(ing_pcap,na.rm = T),\n            Mediana=median(ing_pcap,na.rm = T)) %&gt;% \n  knitr::kable()\n\n\n\n\nquintile\nn\nMedia\nMediana\n\n\n\n\nQuintil 1\n711\n62859.09\n66666.67\n\n\nQuintil 2\n711\n112218.97\n111250.12\n\n\nQuintil 3\n710\n167748.23\n166666.67\n\n\nQuintil 4\n710\n262710.27\n250000.50\n\n\nQuintil 5\n710\n710246.41\n500000.00\n\n\nNA\n196\nNaN\nNA\n\n\n\n\n\n\nelsoc$quintilemiss &lt;- factor(elsoc$quintile,ordered = T)\nelsoc$quintilemiss &lt;- ifelse(test=is.na(elsoc$quintilemiss),yes = 6,no = elsoc$quintilemiss)\nelsoc$quintilemiss &lt;- factor(elsoc$quintilemiss ,levels = c(1,2,3,4,5,6),labels =  c(\"Quintil 1\",\"Quintil 2\",\"Quintil 3\",\"Quintil 4\",\"Quintil 5\",\"Missing\")) \nelsoc %&gt;% group_by(quintilemiss) %&gt;% summarise(n=n())\n\n# A tibble: 6 × 2\n  quintilemiss     n\n  &lt;fct&gt;        &lt;int&gt;\n1 Quintil 1      711\n2 Quintil 2      711\n3 Quintil 3      710\n4 Quintil 4      710\n5 Quintil 5      710\n6 Missing        196"
  },
  {
    "objectID": "resource/07-resource.html#variables-dummy",
    "href": "resource/07-resource.html#variables-dummy",
    "title": "Práctica 7 Correlación y regresióna",
    "section": "Variables dummy",
    "text": "Variables dummy\nUna forma de pasar una variable categórica a dummies es con la función dummy_cols del paquete fastDummies\n\nelsoc &lt;- dummy_cols(elsoc, select_columns = \"quintilemiss\")\nhead(elsoc[,16:22])\n\n  quintilemiss quintilemiss_Quintil 1 quintilemiss_Quintil 2\n1    Quintil 1                      1                      0\n2    Quintil 5                      0                      0\n3    Quintil 1                      1                      0\n4    Quintil 5                      0                      0\n5      Missing                      0                      0\n6    Quintil 3                      0                      0\n  quintilemiss_Quintil 3 quintilemiss_Quintil 4 quintilemiss_Quintil 5\n1                      0                      0                      0\n2                      0                      0                      1\n3                      0                      0                      0\n4                      0                      0                      1\n5                      0                      0                      0\n6                      1                      0                      0\n  quintilemiss_Missing\n1                    0\n2                    0\n3                    0\n4                    0\n5                    1\n6                    0\n\n\n¿cómo hacerlo para una variable numérica?\nTambién existen muchas formas, como por ejemplo establecer como punto de corte la media o la mediana, o ver la distribución de las respuestas y tratar de establecer una distribución homogénea entre las dos nuevas categorías.\nSi recordamos la distribución de nuestra variable dependiente antes de construir el índice de participación:\n\nplot_stackfrq(elsoc[,c(\"part01\",\"part02\",\"part03\",\"part04\")]) + theme(legend.position=\"bottom\")\n\n\n\n\n\n\n\n\ny luego en el índice de participación\n\nsummary(elsoc$partpol)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  4.000   4.000   4.000   5.473   6.000  20.000       8 \n\n\nPodemos notar que la mayoría de las respuestas se agrupan en la categoría “nunca” de las variables por separado y luego en el índice la mediana también corresponde al valor mínimo posible de “4” que es la suma de todas las personas que nunca han participado en ninguna de las opciones. Por lo tanto, tenemos dos criterios que nos permiten decidir que nuestra variable dependiente puede ser considera como dummy bajo los valores 0=nunca ha participado; y 1=si ha participado.\nUna forma de hacer esta agrupación de valores es con la función case_when del paquete dplyr (similar a ifelse)\n\nelsoc &lt;- elsoc %&gt;% rowwise() %&gt;%  mutate(partpol_dummy = case_when(partpol==4~0,\n                                                                   partpol&gt;4~1,\n                                                                   TRUE ~ NA))\ntable(elsoc$partpol_dummy)\n\n\n   0    1 \n2074 1666"
  },
  {
    "objectID": "resource/05-resource.html",
    "href": "resource/05-resource.html",
    "title": "Práctico 5. Documentos reproducibles",
    "section": "",
    "text": "La siguiente práctica tiene el objetivo de introducir en los supuestos y robustez del modelo de regresión. Por esta razón, volveremos a algunos de los contenidos previos relacionados con la estimación, análisis de residuos y ajuste. Para ello, utilizaremos la base de datos de la tercera ola del Estudio Longitudinal Social del Chile 2018 con el objetivo de analizar los determinantes de la Participación Ciudadana.\nLa versión original de este ejercicio proviene del curso de Estadística multivariada versión 2022."
  },
  {
    "objectID": "resource/05-resource.html#explorar-datos",
    "href": "resource/05-resource.html#explorar-datos",
    "title": "Práctico 5. Documentos reproducibles",
    "section": "Explorar datos",
    "text": "Explorar datos\nA partir de la siguiente tabla se obtienen estadísticos descriptivos que luego serán relevantes para realizar las transformaciones y análisis posteriores.\n\nview(dfSummary(elsoc, headings = FALSE, method = \"render\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo\nVariable\nLabel\nStats / Values\nFreqs (% of Valid)\nGraph\nValid\nMissing\n\n\n\n\n1\nsexo [numeric]\nSexo entrevistado\n\n\n\nMin : 0\n\n\nMean : 0.6\n\n\nMax : 1\n\n\n\n\n\n\n0\n:\n1446\n(\n38.6%\n)\n\n\n1\n:\n2302\n(\n61.4%\n)\n\n\n\n\n3748 (100.0%)\n0 (0.0%)\n\n\n2\nedad [numeric]\nEdad entrevistado\n\n\n\nMean (sd) : 47.1 (15.5)\n\n\nmin ≤ med ≤ max:\n\n\n18 ≤ 47 ≤ 90\n\n\nIQR (CV) : 25 (0.3)\n\n\n\n70 distinct values\n\n3748 (100.0%)\n0 (0.0%)\n\n\n3\neduc [factor]\nNivel educacional\n\n\n\n1. 1\n\n\n2. 2\n\n\n3. 3\n\n\n4. 4\n\n\n5. 5\n\n\n\n\n\n\n450\n(\n12.0%\n)\n\n\n370\n(\n9.9%\n)\n\n\n1600\n(\n42.7%\n)\n\n\n598\n(\n16.0%\n)\n\n\n725\n(\n19.4%\n)\n\n\n\n\n3743 (99.9%)\n5 (0.1%)\n\n\n4\npospol [factor]\nAutoubicacion escala izquierda-derecha\n\n\n\n1. 1\n\n\n2. 2\n\n\n3. 3\n\n\n4. 4\n\n\n\n\n\n\n807\n(\n22.0%\n)\n\n\n952\n(\n26.0%\n)\n\n\n734\n(\n20.0%\n)\n\n\n1171\n(\n32.0%\n)\n\n\n\n\n3664 (97.8%)\n84 (2.2%)\n\n\n5\npart01 [numeric]\nFrecuencia: Firma carta o peticion apoyando causa\n\n\n\nMean (sd) : 1.5 (0.9)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 1 ≤ 5\n\n\nIQR (CV) : 1 (0.6)\n\n\n\n\n\n\n1\n:\n2717\n(\n72.6%\n)\n\n\n2\n:\n476\n(\n12.7%\n)\n\n\n3\n:\n411\n(\n11.0%\n)\n\n\n4\n:\n117\n(\n3.1%\n)\n\n\n5\n:\n21\n(\n0.6%\n)\n\n\n\n\n3742 (99.8%)\n6 (0.2%)\n\n\n6\npart02 [numeric]\nFrecuencia: Asiste a marcha o manifestacion pacifica\n\n\n\nMean (sd) : 1.2 (0.6)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 1 ≤ 5\n\n\nIQR (CV) : 0 (0.5)\n\n\n\n\n\n\n1\n:\n3289\n(\n87.8%\n)\n\n\n2\n:\n195\n(\n5.2%\n)\n\n\n3\n:\n191\n(\n5.1%\n)\n\n\n4\n:\n51\n(\n1.4%\n)\n\n\n5\n:\n19\n(\n0.5%\n)\n\n\n\n\n3745 (99.9%)\n3 (0.1%)\n\n\n7\npart03 [numeric]\nFrecuencia: Participa en huelga\n\n\n\nMean (sd) : 1.2 (0.5)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 1 ≤ 5\n\n\nIQR (CV) : 0 (0.5)\n\n\n\n\n\n\n1\n:\n3407\n(\n91.0%\n)\n\n\n2\n:\n152\n(\n4.1%\n)\n\n\n3\n:\n146\n(\n3.9%\n)\n\n\n4\n:\n29\n(\n0.8%\n)\n\n\n5\n:\n11\n(\n0.3%\n)\n\n\n\n\n3745 (99.9%)\n3 (0.1%)\n\n\n8\npart04 [numeric]\nFrecuencia: Usa redes sociales para opinar en temas publicos\n\n\n\nMean (sd) : 1.6 (1.1)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 1 ≤ 5\n\n\nIQR (CV) : 1 (0.7)\n\n\n\n\n\n\n1\n:\n2598\n(\n69.4%\n)\n\n\n2\n:\n310\n(\n8.3%\n)\n\n\n3\n:\n514\n(\n13.7%\n)\n\n\n4\n:\n223\n(\n6.0%\n)\n\n\n5\n:\n98\n(\n2.6%\n)\n\n\n\n\n3743 (99.9%)\n5 (0.1%)\n\n\n9\nquintilemiss [factor]\n\n\n\n\n1. Quintil 1\n\n\n2. Quintil 2\n\n\n3. Quintil 3\n\n\n4. Quintil 4\n\n\n5. Quintil 5\n\n\n6. Missing\n\n\n\n\n\n\n711\n(\n19.0%\n)\n\n\n711\n(\n19.0%\n)\n\n\n710\n(\n18.9%\n)\n\n\n710\n(\n18.9%\n)\n\n\n710\n(\n18.9%\n)\n\n\n196\n(\n5.2%\n)\n\n\n\n\n3748 (100.0%)\n0 (0.0%)\n\n\n\n\nGenerated by summarytools 1.0.1 (R version 4.4.0)2024-07-08\n\n\n\n\n\nview_df(elsoc,max.len = 50)\n\n\n\nData frame: elsoc\n\n\n\n\n\n\n\n\n\nID\nName\nLabel\nValues\nValue Labels\n\n\n1\nsexo\nSexo entrevistado\n0\n1\nHombre\nMujer\n\n\n2\nedad\nEdad entrevistado\nrange: 18-90\n\n\n3\neduc\nNivel educacional\n1\n2\n3\n4\n5\nPrimaria incompleta menos\nPrimaria y secundaria baja\nSecundaria alta\nTerciaria ciclo corto\nTerciaria y Postgrado\n\n\n4\npospol\nAutoubicacion escala izquierda-derecha\n1\n2\n3\n4\nDerecha\nCentro\nIzquierda\nIndep./Ninguno\n\n\n5\npart01\nFrecuencia: Firma carta o peticion apoyando causa\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n6\npart02\nFrecuencia: Asiste a mbackground-color:#eeeeeeha o manifestacion\npacifica\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n7\npart03\nFrecuencia: Participa en huelga\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n8\npart04\nFrecuencia: Usa redes sociales para opinar en\ntemas publicos\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n9\nquintilemiss\n\n\nQuintil 1\nQuintil 2\nQuintil 3\nQuintil 4\nQuintil 5\nMissing\n\n\n\n\n\n\n\nelsoc &lt;- elsoc %&gt;% mutate(partpol=rowSums(select(., part01,part02,part03,part04)))"
  },
  {
    "objectID": "resource/05-resource.html#diágnosticos",
    "href": "resource/05-resource.html#diágnosticos",
    "title": "Práctico 5. Documentos reproducibles",
    "section": "Diágnosticos",
    "text": "Diágnosticos\n\nCasos influyentes\nPara determinar si un outlier es un caso influyente, es decir que su presencia/ausencia genera un cambio importante en la estimación de los coeficientes de regresión, calculamos la Distancia de Cook..\nPosteriormente, se establece un punto de corte de \\(4/(n-k-1)\\):\n\nn&lt;- nobs(fit04) #n de observaciones\nk&lt;- length(coef(fit04)) # n de parametros\ndcook&lt;- 4/(n-k-1) #punt de corte\n\nSi lo graficamos se ve de la siguiente manera:\n\nfinal &lt;- broom::augment_columns(fit04,data = elsoc)\nfinal$id &lt;- as.numeric(row.names(final))\n# identify obs with Cook's D above cutoff\nggplot(final, aes(id, .cooksd)) +\n  geom_bar(stat=\"identity\", position=\"identity\") +\n  xlab(\"Obs. Number\")+ # Modificación nombre eje x\n  ylab(\"Cook's distance\")+ # Modificación nombre eje y\n  geom_hline(yintercept=dcook)+ # Incluir una línea horizontal\n  geom_text(aes(label=ifelse((.cooksd&gt;dcook),id,\"\")), # geom text agrega nombre a los casos, en esta oportunidad solo a los valores mayores a dcook\n            vjust=-0.2, hjust=0.5)\n\n\n\n\n\n\n\n\nIdentificamos los casos influyentes y filtramos la base de datos:\n\nident&lt;- final %&gt;% filter(.cooksd&gt;dcook)\nelsoc02 &lt;- final %&gt;% filter(!(id %in% ident$id))\n\nEstimación sin casos influyentes:\n\nfit05&lt;- lm(partpol~sexo+edad+quintilemiss+pospol,data=elsoc02)\n\nlabs02 &lt;- c(\"Intercepto\",\"Sexo (mujer=1)\",\"Edad\",\n            \"Quintil 2\",\"Quintil 3\",\"Quintil 4\",\"Quintil 5\",\"Quintil perdido\",\n            \"Izquierda (ref. derecha)\",\"Centro\",\"Idep./Ninguno\")\n\nhtmlreg(list(fit04,fit05), \n        doctype = FALSE,\n        custom.model.names = c(\"Modelo 4\", \"Modelo 5\"),\n        custom.coef.names = labs02)\n\n\nStatistical models\n\n\n\n\n \n\n\nModelo 4\n\n\nModelo 5\n\n\n\n\n\n\nIntercepto\n\n\n7.97***\n\n\n7.05***\n\n\n\n\n \n\n\n(0.16)\n\n\n(0.11)\n\n\n\n\nSexo (mujer=1)\n\n\n0.12\n\n\n0.07\n\n\n\n\n \n\n\n(0.07)\n\n\n(0.05)\n\n\n\n\nEdad\n\n\n-0.04***\n\n\n-0.03***\n\n\n\n\n \n\n\n(0.00)\n\n\n(0.00)\n\n\n\n\nQuintil 2\n\n\n0.21\n\n\n0.11\n\n\n\n\n \n\n\n(0.11)\n\n\n(0.08)\n\n\n\n\nQuintil 3\n\n\n0.51***\n\n\n0.34***\n\n\n\n\n \n\n\n(0.11)\n\n\n(0.08)\n\n\n\n\nQuintil 4\n\n\n0.50***\n\n\n0.32***\n\n\n\n\n \n\n\n(0.11)\n\n\n(0.08)\n\n\n\n\nQuintil 5\n\n\n0.88***\n\n\n0.57***\n\n\n\n\n \n\n\n(0.12)\n\n\n(0.08)\n\n\n\n\nQuintil perdido\n\n\n0.59***\n\n\n0.31*\n\n\n\n\n \n\n\n(0.18)\n\n\n(0.13)\n\n\n\n\nIzquierda (ref. derecha)\n\n\n-1.04***\n\n\n-0.65***\n\n\n\n\n \n\n\n(0.10)\n\n\n(0.07)\n\n\n\n\nCentro\n\n\n-1.13***\n\n\n-0.71***\n\n\n\n\n \n\n\n(0.11)\n\n\n(0.08)\n\n\n\n\nIdep./Ninguno\n\n\n-1.60***\n\n\n-1.14***\n\n\n\n\n \n\n\n(0.10)\n\n\n(0.07)\n\n\n\n\nR2\n\n\n0.17\n\n\n0.18\n\n\n\n\nAdj. R2\n\n\n0.17\n\n\n0.18\n\n\n\n\nNum. obs.\n\n\n3656\n\n\n3460\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\nEn términos generales, el sentido y significación estadística de los coeficientes del Modelo 5 se mantiene respecto al Modelo 4. Adicionalmente, si observamos que el modelo sin casos influyentes presenta una mejora en ajuste. Por lo tanto, los análisis posteriores se realizaran en base a este modelo.\n\n\nLinealidad\nPara analizar la linealidad respecto de un modelo de regresión, debemos analizar la distribución de los residuos con respecto a la recta de regresión.\n\nLos residuos deben ser independientes de los valores predichos (fitted values).\nCualquier correlación entre residuo y valores predichos violarían este supuesto.\nLa presencia de un patrón no lineal, es señal de que el modelo está especificado incorrectamente.\n\n\nggplot(fit05, aes(.fitted, .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0) +\n  geom_smooth(se = TRUE)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nRelación entre residuos y valores predichos\n\n\n\n\nEl gráfico nos indica que existe un patrón en la distribución de los residuos. Para intentar mejorar la estimación podemos realizar una transformación de variables. A continuación presentaremos un ejemplo para la Edad y para los Ingresos.\n\nPolinomio: \\(\\text{Edad}^2\\)\n\n\nelsoc02$edad2 &lt;- elsoc02$edad^2\nfit06&lt;- lm(partpol~sexo+edad+edad2+quintilemiss+pospol,data=elsoc02)\n\n\nedad&lt;- fit06$model$edad\nfit&lt;- fit06$fitted.values\ndata01 &lt;- as.data.frame(cbind(edad,fit))\n\nggplot(data01, aes(x = edad, y = fit)) +\n  theme_bw() +\n  geom_point()+\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nEfecto cuadrático de la edad (Modelo 5)\n\n\n\n\n\nfit07 &lt;- lm(partpol~sexo+edad+edad2+quintilemiss+pospol,data=elsoc02)\n\nlabs03 &lt;- c(\"Intercepto\",\"Sexo (mujer=1)\",\"Edad\",\n            \"Quintil 2\",\"Quintil 3\",\"Quintil 4\",\"Quintil 5\",\"Quintil perdido\",\n            \"Izquierda (ref. derecha)\",\"Centro\",\"Idep./Ninguno\", \"Edad²\")\n\nhtmlreg(list(fit05, fit06, fit07), doctype = FALSE,\n        custom.model.names = c(\"Modelo 4\", \"Modelo 5\", \"Modelo 6\"), \n          custom.coef.names = labs03)\n\n\nStatistical models\n\n\n\n\n \n\n\nModelo 4\n\n\nModelo 5\n\n\nModelo 6\n\n\n\n\n\n\nIntercepto\n\n\n7.05***\n\n\n7.62***\n\n\n7.62***\n\n\n\n\n \n\n\n(0.11)\n\n\n(0.24)\n\n\n(0.24)\n\n\n\n\nSexo (mujer=1)\n\n\n0.07\n\n\n0.08\n\n\n0.08\n\n\n\n\n \n\n\n(0.05)\n\n\n(0.05)\n\n\n(0.05)\n\n\n\n\nEdad\n\n\n-0.03***\n\n\n-0.06***\n\n\n-0.06***\n\n\n\n\n \n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.01)\n\n\n\n\nQuintil 2\n\n\n0.11\n\n\n0.11\n\n\n0.11\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.08)\n\n\n(0.08)\n\n\n\n\nQuintil 3\n\n\n0.34***\n\n\n0.34***\n\n\n0.34***\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.08)\n\n\n(0.08)\n\n\n\n\nQuintil 4\n\n\n0.32***\n\n\n0.32***\n\n\n0.32***\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.08)\n\n\n(0.08)\n\n\n\n\nQuintil 5\n\n\n0.57***\n\n\n0.57***\n\n\n0.57***\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.08)\n\n\n(0.08)\n\n\n\n\nQuintil perdido\n\n\n0.31*\n\n\n0.31*\n\n\n0.31*\n\n\n\n\n \n\n\n(0.13)\n\n\n(0.13)\n\n\n(0.13)\n\n\n\n\nIzquierda (ref. derecha)\n\n\n-0.65***\n\n\n-0.65***\n\n\n-0.65***\n\n\n\n\n \n\n\n(0.07)\n\n\n(0.07)\n\n\n(0.07)\n\n\n\n\nCentro\n\n\n-0.71***\n\n\n-0.70***\n\n\n-0.70***\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.08)\n\n\n(0.08)\n\n\n\n\nIdep./Ninguno\n\n\n-1.14***\n\n\n-1.13***\n\n\n-1.13***\n\n\n\n\n \n\n\n(0.07)\n\n\n(0.07)\n\n\n(0.07)\n\n\n\n\nEdad²\n\n\n \n\n\n0.00**\n\n\n0.00**\n\n\n\n\n \n\n\n \n\n\n(0.00)\n\n\n(0.00)\n\n\n\n\nR2\n\n\n0.18\n\n\n0.19\n\n\n0.19\n\n\n\n\nAdj. R2\n\n\n0.18\n\n\n0.18\n\n\n0.18\n\n\n\n\nNum. obs.\n\n\n3460\n\n\n3460\n\n\n3460\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "resource/05-resource.html#referencias",
    "href": "resource/05-resource.html#referencias",
    "title": "Práctico 5. Documentos reproducibles",
    "section": "Referencias",
    "text": "Referencias\nDarlington & Hayes 2016 Cap16 Detecting and Managing Irregularities\nDarlington & Hayes 2016 Cap12 Nonlinear relationships"
  },
  {
    "objectID": "resource/03-resource.html",
    "href": "resource/03-resource.html",
    "title": "Práctico 3. Inferencia Estadística y Curva Normal",
    "section": "",
    "text": "El objetivo de esta guía práctica es introducirnos en la inferencia estadística, revisando los conceptos y aplicaciones de la curva normal y las probabilidades bajo esta con puntajes Z.\nEn detalle, aprenderemos:\n\nQué es la inferencia estadística.\nQué es una distribución muestral.\nQué es el error estándar.\nQué es la distribución normal y cómo interpretarla.\nCómo calcular probabilidades asociadas con valores Z en R.\nQué son y cómo calcular intervalos de conafianza.",
    "crumbs": [
      "Prácticos",
      "Práctico 3"
    ]
  },
  {
    "objectID": "resource/03-resource.html#qué-es-una-distribución",
    "href": "resource/03-resource.html#qué-es-una-distribución",
    "title": "Práctico 3. Inferencia Estadística y Curva Normal",
    "section": "2.1. ¿Qué es una distribución?",
    "text": "2.1. ¿Qué es una distribución?\nRecordemos que por distribución nos referimos al conjunto de todos los valores posibles de una variable y las frecuencias (o probabilidades) con las que se producen.\nExisten distribuciones empíricas y distribuciones teóricas, en donde:\n\nlas primeras reflejan la distribución de los valores que asume la variable en un grupo concreto a partir de una observación.\nlas segundas son una función matématica que expresan la distribución de un conjunto de números mediante su probabilidad de ocurencia.\n\nUna de las distribuciones teóricas más conocidas es la distribución normal estándar.",
    "crumbs": [
      "Prácticos",
      "Práctico 3"
    ]
  },
  {
    "objectID": "resource/03-resource.html#distribución-muestral-1",
    "href": "resource/03-resource.html#distribución-muestral-1",
    "title": "Práctico 3. Inferencia Estadística y Curva Normal",
    "section": "2.2. Distribución muestral",
    "text": "2.2. Distribución muestral\n\n\n\n\n\n\nNota\n\n\n\nVariabilidad muestral: el valor de un estadístico varía en un muestreo aleatorio repetido.\n\n\nLa distribución muestral es la distribución de las estimaciones, o estadísticos como la media o proporción, tomadas de múltiples muestras aleatorias de una población. Permite comprender cómo varían las estimaciones de una muestra a otra.\nEjemplo 1: Imaginemos que tenemos una población de niñ_s de 0 a 9 años, y tomamos múltiples muestras de 6 individu_s (n=6). Cada una de las muestras tendrá un promedio (estadístico muestral, en este caso \\(\\bar{x}\\)) diferente, que no necesariamente coincidirá con el promedio de la población (parámetro, en este caso \\(\\mu_{x}\\))\n\nEjemplo 2: Si usamos valores simulados, podemos ver que todas las medias obtenidas en cada muesta son distintas.\n\nset.seed(100)  # Establecer semilla \nmuestras &lt;- replicate(100, mean(rnorm(30, mean = 50, sd = 10))) # 100 muestras de tamaño 30\nmuestras\n\n  [1] 50.28864 50.92574 49.56125 49.25099 48.40735 51.67506 49.28325 48.90829\n  [9] 50.64635 51.94797 49.68514 49.00100 49.87287 47.58167 50.91109 47.61967\n [17] 47.94270 52.20491 51.38777 50.76559 49.02265 49.16394 50.59675 51.40631\n [25] 50.28247 51.94561 50.39929 51.95632 55.25584 51.26112 49.02810 46.46643\n [33] 52.21139 49.48146 50.43067 52.84081 47.97451 46.55278 49.87576 50.88025\n [41] 50.78748 49.42165 50.46664 51.06194 49.94867 48.39219 49.79581 49.82214\n [49] 49.93791 49.16883 52.24256 51.46923 46.51443 50.23611 49.87410 50.87291\n [57] 52.32911 46.21546 47.26934 51.29019 50.49509 49.63433 53.25368 50.81717\n [65] 49.45387 49.61571 50.33774 47.02089 47.94071 50.16296 51.12409 50.68963\n [73] 50.32398 52.22186 49.95585 50.74844 48.08507 52.90382 51.43478 46.74822\n [81] 49.21148 51.83738 49.66936 49.32308 50.31842 46.47797 50.64338 50.01220\n [89] 52.26135 47.49504 49.93140 53.04953 49.59253 48.83580 49.57802 49.23299\n [97] 50.32517 50.82952 48.92960 49.47553\n\n\nSi conocemos la desviación estándar de los promedios, podedmos construir un intervalo de probabilidad, basado en la curva normal.\n\n\n\n\n\n\nNota\n\n\n\nUna característica importante es que se asume que las muestras tomadas de la población son aleatorias y representativas, lo que es esencial para que la distribución muestral refleje adecuadamente la variabilidad de las estimaciones.\n\n\nLa importancia de la distribución muestral es que nos permitirá estimar parámetros poblacionales a partir de estadísticos muestrales, construir intervalos de confianza, y realizar pruebas de hipótesis.",
    "crumbs": [
      "Prácticos",
      "Práctico 3"
    ]
  },
  {
    "objectID": "resource/03-resource.html#ejercicio-de-aplicación",
    "href": "resource/03-resource.html#ejercicio-de-aplicación",
    "title": "Práctico 3. Inferencia Estadística y Curva Normal",
    "section": "Ejercicio de aplicación",
    "text": "Ejercicio de aplicación\nAhora que hemos generado distribuciones normales, echemos un vistazo a algunos datos y compárelos con la distribución normal. Utilizaremos un conjunto de datos desde internet, con mediciones de 247 hombres y 260 mujeres, la mayoría de los cuales eran considerados adultos jóvenes sanos. Puede encontrar una clave para los nombres de las variables aquí, pero nos centraremos en solo tres columnas: peso en kg (wgt), altura en cm (hgt) y sexo (1 = hombre; 0 = mujer).\n\nload(url(\"http://www.openintro.org/stat/data/bdims.RData\"))\n\nSeparemos estos datos en dos conjuntos, uno de hombres y otro de mujeres con la función subset\n\nmdims &lt;- subset(bdims, sex == 1)\nfdims &lt;- subset(bdims, sex == 0)\n\n\nEjercicio 1\nHaz un histograma de la altura de los hombres y un histograma de la altura de las mujeres. ¿Cómo compararía los diversos aspectos de las dos distribuciones?\n\nhist(mdims$hgt, xlim = c(150,200))\n\n\n\n\n\n\n\nhist(fdims$hgt, xlim = c(140,190))\n\n\n\n\n\n\n\n\n\n\nEjercicio 2\nscale es una función en R y se puede aplicar a cualquier vector numérico (lista de números en R). Genere los dos histogramas siguientes, esta vez graficando scale() de las estaturas y determine cómo la versión escalada de las alturas corresponde a las alturas originales. ¿Qué calcula la escala para cada punto?\n\nhist(scale(mdims$hgt))\n\n\n\n\n\n\n\nhist(scale(fdims$hgt))\n\n\n\n\n\n\n\n\n\n\nEjercicio 3\nNos gustaría comparar la distribución de estaturas en este conjunto de datos con la distribución normal. Para cada uno de los histogramas de alturas (sin escalar), trace una curva normal en la parte superior del histograma.\n\nCalcule la media y la desviación estándar para las alturas femeninas y guárdelas como variables, fhgtmean y fhgtsd, respectivamente.\nDetermine la lista de valores de x (el rango del eje X) y guarde este vector. Puede hacer fácilmente una lista de números usando la función seq() como lo hemos hecho antes, o teniendo el límite inferior:límite superior. Por ejemplo, para generar un vector (lista de números) del 1 al 10 y guardarlo como one_ten, usaría one_ten &lt;- 1:10.\nComo arriba, use dnorm() para tomar la lista de valores de x y encontrar el valor de y correspondiente si fuera una distribución normal perfecta. Guarde este vector como la variable y.\nVuelva a trazar su histograma y luego, en la siguiente línea, use lines(x = x, y = y, col = \"blue\") para dibujar una distribución normal encima.\n\n\nfhgtmean &lt;- mean(fdims$hgt)\nfhgtsd   &lt;- sd(fdims$hgt)\nhist(fdims$hgt, probability = TRUE, ylim = c(0, .07))\nx &lt;- 140:190\ny &lt;- dnorm(x = x, mean = fhgtmean, sd = fhgtsd)\nlines(x = x, y = y, col = \"blue\")\n\n\n\n\n\n\n\n\nSegún este gráfico, ¿parece que los datos siguen una distribución casi normal? Haz lo mismo con las estaturas masculinas.\n\nRespuesta: En general, sí, consideraría que estos valores siguen una distribución casi normal ya que el histograma se ajusta bastante bien a la curva.\n\nObserve que la forma del histograma es una forma de determinar si los datos parecen estar distribuidos casi normalmente, pero puede resultar frustrante decidir qué tan cerca está el histograma de la curva. Un enfoque alternativo implica construir una gráfica de probabilidad normal, también llamada gráfica Q-Q por “quantil-quantil”. Ejecute ambas líneas juntas.\n\nqqnorm(fdims$hgt)\nqqline(fdims$hgt)\n\n\n\n\n\n\n\n\nUn QQ plot nos muestra en el eje x los cuantiles teóricos de la distribución en términos de desviaciones estandar, y en el eje y los valores de la variable. La distribución de los puntos en una línea recta es una indicación de que los datos se distribuyen normalmente.\nVeamos otro ejemplo de otra variable de la base de datos:\n\nhist(fdims$che.de)\n\n\n\n\n\n\n\nqqnorm(fdims$che.de)\nqqline(fdims$che.de)\n\n\n\n\n\n\n\n\nUna vez que decidimos que una variable se distribuyte de forma normal, podemos responder todo tipo de preguntas sobre esa variable relacionadas con la probabilidad. Tomemos, por ejemplo, la pregunta: “¿Cuál es la probabilidad de que una mujer adulta joven elegida al azar mida más 182 cm?”\nSi suponemos que las alturas de las mujeres se distribuyen normalmente (una aproximación muy cercana también está bien), podemos encontrar esta probabilidad calculando una puntuación Z y consultando una tabla Z (también llamada tabla de probabilidad normal).\nEn R, esto se hace en un solo paso con la función pnorm (como hicimos anteriormente para la distribución normal estándar).\n\npnorm(q = 182, mean = fhgtmean, sd = fhgtsd)\n\n[1] 0.9955656\n\n\nObtenemos la proporción de mujeres que está bajo esa estatura, es decir 99,6%. Si queremos saber la proporción de mujeres que está sobre esa estatura:\n\n1 - pnorm(q = 182, mean = fhgtmean, sd = fhgtsd)\n\n[1] 0.004434387\n\n\nEn este caso, el 0,4% de las mujeres se encontraría sobre esa estatura.\nPodemos también hacer la operación inversa, es decir, a qué valor (estatura) corresponde un porcentaje o probabilidad basada en una distribución normal. Para ello utilizamos la función qnorm. Por ejemplo, para la probabilidad que calculamos más arriba para una altura de 182cm en las mujeres:\n\nqnorm(.9955656, fhgtmean, fhgtsd)\n\n[1] 182",
    "crumbs": [
      "Prácticos",
      "Práctico 3"
    ]
  },
  {
    "objectID": "resource/03-resource.html#cálculo-de-intervalos-de-confianza",
    "href": "resource/03-resource.html#cálculo-de-intervalos-de-confianza",
    "title": "Práctico 3. Inferencia Estadística y Curva Normal",
    "section": "Cálculo de intervalos de confianza",
    "text": "Cálculo de intervalos de confianza\nAhora ¡Manos a la obra!\nCalculemos intervalos de confianza. Primero, carguemos las librerías necesarias:\n\nlibrary(pacman)\npacman::p_load(tidyverse, # colección de paquetes para manipulación de datos\n               car,       # para recodificar\n               psych,     # para analizar datos\n               sjmisc,    # para analizar datos\n               srvyr,     # para estimación de IC y ponderadores\n               Publish)   # para IC\n\nInstalling package into 'C:/Users/kevin/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\n\n\nalso installing the dependencies 'mitools', 'RcppArmadillo', 'survey'\n\n\nWarning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.4:\n  no fue posible abrir la URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.4/PACKAGES'\n\n\n\n  There is a binary version available but the source version is later:\n                  binary   source needs_compilation\nRcppArmadillo 0.12.8.4.0 14.0.0-1              TRUE\n\n  Binaries will be installed\npackage 'mitools' successfully unpacked and MD5 sums checked\npackage 'RcppArmadillo' successfully unpacked and MD5 sums checked\npackage 'survey' successfully unpacked and MD5 sums checked\npackage 'srvyr' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\kevin\\AppData\\Local\\Temp\\RtmpMfb2TG\\downloaded_packages\n\n\n\nsrvyr installed\n\n\nWarning: package 'srvyr' was built under R version 4.4.1\n\n\nInstalling package into 'C:/Users/kevin/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\n\n\nalso installing the dependencies 'listenv', 'parallelly', 'shape', 'future', 'globals', 'zoo', 'diagram', 'future.apply', 'progressr', 'SQUAREM', 'mvtnorm', 'TH.data', 'sandwich', 'prodlim', 'lava', 'multcomp'\n\n\nWarning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.4:\n  no fue posible abrir la URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.4/PACKAGES'\n\n\npackage 'listenv' successfully unpacked and MD5 sums checked\npackage 'parallelly' successfully unpacked and MD5 sums checked\npackage 'shape' successfully unpacked and MD5 sums checked\npackage 'future' successfully unpacked and MD5 sums checked\npackage 'globals' successfully unpacked and MD5 sums checked\npackage 'zoo' successfully unpacked and MD5 sums checked\npackage 'diagram' successfully unpacked and MD5 sums checked\npackage 'future.apply' successfully unpacked and MD5 sums checked\npackage 'progressr' successfully unpacked and MD5 sums checked\npackage 'SQUAREM' successfully unpacked and MD5 sums checked\npackage 'mvtnorm' successfully unpacked and MD5 sums checked\npackage 'TH.data' successfully unpacked and MD5 sums checked\npackage 'sandwich' successfully unpacked and MD5 sums checked\npackage 'prodlim' successfully unpacked and MD5 sums checked\npackage 'lava' successfully unpacked and MD5 sums checked\npackage 'multcomp' successfully unpacked and MD5 sums checked\npackage 'Publish' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\kevin\\AppData\\Local\\Temp\\RtmpMfb2TG\\downloaded_packages\n\n\n\nPublish installed\n\n\nWarning: package 'Publish' was built under R version 4.4.1\n\n\nWarning: package 'prodlim' was built under R version 4.4.1\n\noptions(scipen = 999) # para desactivar notacion cientifica\nrm(list = ls()) # para limpiar el entorno de trabajo\n\ny también carguemos la base de datos que utilizaremos, que corresponde a un subset de la Encuesta Suplementaria de ingresos ESI para ocupados:\n\nload(url(\"https://github.com/cursos-metodos-facso/datos-ejemplos/raw/main/esi-2021-ocupados.rdata\"))\n\n\n\n\n\n\n\nNota\n\n\n\nRecordemos que podemos contar con bases de datos que tengan factor de expansión (ponderador) o no. Esta distinción se presenta cuando trabajamos con muestras simples o complejas. Al trabajar con muestras complejas debemos identificar cuál es la variable del ponderador e incorporarla en nuestro cálculo, como veremos a continuación.\n\n\n\nIntervalos de confianza sin ponderador\nPodemos calcular intervalos de confianza con muestras representativas sin ponderadores o factores de expansión. Supongamos que es el caso.\n\nIC para Medias\nCalculemos un intervalo de confianza para la media de ingresos de personas ocupadas:\n\npsych::describe(esi$ing_t_p)\n\n   vars     n     mean       sd   median  trimmed      mad min      max\nX1    1 37124 586360.4 697362.9 405347.7 474473.1 255411.6   0 38206253\n      range skew kurtosis      se\nX1 38206253   12   402.32 3619.36\n\n\n\nPublish::ci.mean(esi$ing_t_p, alpha = 0.05)\n\n mean      CI-95%               \n 586360.41 [579266.37;593454.45]\n\n\nAl no aplicar factores de expansión, contamos con una media de ingresos de $586.360 como estimación puntual. Pero también podemos decir que con un 95% de confianza el parámetro poblacional se encontrará entre $579.266 y $593.454.\n\n\nIC para Proporciones\nPara calcular un intervalo de confianza para la proporción por la variable sexo, usamos:\n\nsjmisc::frq(esi$sexo)\n\nx &lt;numeric&gt; \n# total N=37124 valid N=37124 mean=1.44 sd=0.50\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\n    1 | 20806 | 56.04 |   56.04 |  56.04\n    2 | 16318 | 43.96 |   43.96 | 100.00\n &lt;NA&gt; |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nprop.test(x = 20806, n = 37124, conf.level = 0.95)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  20806 out of 37124, null probability 0.5\nX-squared = 542.32, df = 1, p-value &lt; 0.00000000000000022\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5553777 0.5655019\nsample estimates:\n        p \n0.5604461 \n\n\nEn este caso, sabemos que el total de las personas ocupadas de la muestra son n=37.124, y que la cantidad de hombres son 20.806, correspondientes al 56% como estimación puntual. También podemos sostener con un 95% que la proporción de hombres en la población se encuentra entre 55.54% y 56.6%.\n\n\n\nIntervalos de confianza con ponderador\nPara muestras complejas que cuentan con ponderador (o factor de expansión) también podemos hacer este ejercicio.\nPrimero, es necesario identificar la variable de factor de expansión o ponderador:\n\nesi_pond &lt;- esi %&gt;% as_survey_design(ids = 1, # indica conglomerados de muestreo; ~0 o ~1 cuando no hay\n                                     strata = estrato, # indica efecto de diseño muestral\n                                     weights = fact_cal_esi) # indica el ponderador\n\noptions(survey.lonely.psu = \"certainty\") # seteamos para que ids no moleste\n\n\nIC para Medias\nAhora, teniendo en consideración el factor de expansión, podemos señalar que:\n\nesi_pond %&gt;% \n  summarise(media = survey_mean(ing_t_p, vartype = \"ci\", levels = 0.95, na.rm=TRUE)) # usamos funcion survey_mean\n\n# A tibble: 1 × 3\n    media media_low media_upp\n    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 681039.   666563.   695516.\n\n\nEl promedio de ingresos de personas ocupadas ponderado en la población corresponde a $681.039 como estimación puntual, pero que es posible afirmar con un 95% de confianza que el parámetro poblacional se encuentra entre $666.562 y $695.516.\n\n\nIC para Proporciones\nFinalmente, si calculamos la proporción de hombres ocupados en la población considerando el factor de expansión:\n\nsjmisc::frq(esi$sexo)\n\nx &lt;numeric&gt; \n# total N=37124 valid N=37124 mean=1.44 sd=0.50\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\n    1 | 20806 | 56.04 |   56.04 |  56.04\n    2 | 16318 | 43.96 |   43.96 | 100.00\n &lt;NA&gt; |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\nesi_pond %&gt;% \n  group_by(sexo) %&gt;% # agrupamos por sexo\n  summarise(prop = survey_prop(vartype = \"ci\", levels = 0.95, na.rm = TRUE))\n\n# A tibble: 2 × 4\n   sexo  prop prop_low prop_upp\n  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     1 0.582    0.575    0.590\n2     2 0.418    0.410    0.425\n\n\nTenemos que, con un 95% de conafianza, podemos afirmar que la proporción de hombre ocupados se encuentra entre el 57% y 58%.",
    "crumbs": [
      "Prácticos",
      "Práctico 3"
    ]
  },
  {
    "objectID": "resource/01-resource.html",
    "href": "resource/01-resource.html",
    "title": "Práctico 1. Cálculo y reporte de Correlación",
    "section": "",
    "text": "El objetivo de esta guía práctica es aprender a calcular y graficar la correlación entre dos variables utilizando R.\nEn detalle, aprenderemos:\n\nQué es una correlación\nCuál es la correlación de Pearson\nCómo calcular una correlación de Pearson y graficarla",
    "crumbs": [
      "Prácticos",
      "Práctico 1"
    ]
  },
  {
    "objectID": "resource/01-resource.html#diagrama-de-dispersión-nube-de-puntos-o-scatterplot",
    "href": "resource/01-resource.html#diagrama-de-dispersión-nube-de-puntos-o-scatterplot",
    "title": "Práctico 1. Cálculo y reporte de Correlación",
    "section": "Diagrama de dispersión (nube de puntos o scatterplot)",
    "text": "Diagrama de dispersión (nube de puntos o scatterplot)\nSiempre es recomendable acompañar el valor de la correlación con una exploración gráfica de la distribución bivariada de los datos. El gráfico o diagrama de dispersión es una buena herramienta, ya que muestra la forma, la dirección y la fuerza de la relación entre dos variables cuantitativas.\nEste tipo de gráfico lo podemos realizar usando la librería ggplot2.\n\npacman::p_load(ggplot2)\nplot1 &lt;- ggplot(data, \n                aes(x=educ, y=ing)) + \n                geom_point(colour = \"red\", \n                size = 5)\nplot1\n\n\n\n\n\n\n\n\nEn el gráfico podemos ver como se crea una nube de puntos en las intersecciones de los valores para ambas variables de cada caso.",
    "crumbs": [
      "Prácticos",
      "Práctico 1"
    ]
  },
  {
    "objectID": "resource/01-resource.html#el-cuarteto-de-anscombe-y-las-limitaciones-de-la-correlación-lineal",
    "href": "resource/01-resource.html#el-cuarteto-de-anscombe-y-las-limitaciones-de-la-correlación-lineal",
    "title": "Práctico 1. Cálculo y reporte de Correlación",
    "section": "El cuarteto de Anscombe y las limitaciones de la correlación lineal",
    "text": "El cuarteto de Anscombe y las limitaciones de la correlación lineal\nAhora, revisaremos un muy buen ejemplo de la importancia de la exploración gráfica de los datos mediante un ejemplo de Anscombe (1973), que permite visualizar las limitaciones del coeficiente de correlación.\nPrimero, crearemos la base de datos:\n\nanscombe &lt;- data.frame(\n  x1 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),\n  y1 = c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68),\n  x2 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),\n  y2 = c(9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74),\n  x3 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),\n  y3 = c(7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73),\n  x4 = c(8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8),\n  y4 = c(6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89)\n)\n\nCalculamos la correlación pares de datos\n\ncor(anscombe$x1, anscombe$y1)\n\n[1] 0.8164205\n\ncor(anscombe$x2, anscombe$y2)\n\n[1] 0.8162365\n\ncor(anscombe$x3, anscombe$y3)\n\n[1] 0.8162867\n\ncor(anscombe$x4, anscombe$y4)\n\n[1] 0.8165214\n\n\nPodemos observar que los valores de las correlaciones son equivalentes, por lo tanto podríamos pensar que todos los pares de columnas se encuentran correlacionados de manera similar.\nPero, ¿será suficiente con esa información? Pasemos a revisar los gráficos de dispersión de cada par de variables.\n\nggplot(anscombe, aes(x = x1, y = y1)) +\n  geom_point(colour = \"red\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso I\")\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x2, y = y2)) +\n  geom_point(colour = \"green\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso II\")\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x3, y = y3)) +\n  geom_point(colour = \"yellow\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso III\")\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x4, y = y4)) +\n  geom_point(colour = \"orange\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso IV\")\n\n\n\n\n\n\n\n\nComo vemos, con distintas distribuciones las correlaciones pueden ser las mismas, principalmente porque Pearson es una medida que solo captura relaciones lineales (rectas), además de verse influido fuertemente por valores extremos. Por lo mismo, es relevante siempre una buena visualización de la distribución bivariada de los datos como complemento al cálculo del coeficiente de correlación.",
    "crumbs": [
      "Prácticos",
      "Práctico 1"
    ]
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Presentaciones, lecturas y actividades",
    "section": "",
    "text": "En esta sección se encuentran los documentos de presentación correspondientes a cada clase, lecturas y también actividades prácticas.",
    "crumbs": [
      "Clases",
      "Información general",
      "Presentaciones, lecturas y actividades"
    ]
  },
  {
    "objectID": "content/10-content.html#lecturas",
    "href": "content/10-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas",
    "crumbs": [
      "Clases",
      "Sesiones",
      "Regresión logística I"
    ]
  },
  {
    "objectID": "content/08-content.html#lecturas",
    "href": "content/08-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas",
    "crumbs": [
      "Clases",
      "Sesiones",
      "Regresión lineal I"
    ]
  },
  {
    "objectID": "content/06-content.html#lecturas",
    "href": "content/06-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas\nR for Data Science"
  },
  {
    "objectID": "content/04-content.html#lecturas",
    "href": "content/04-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas\nR for Data Science"
  },
  {
    "objectID": "content/02-content.html#lecturas",
    "href": "content/02-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas\nR for Data Science"
  },
  {
    "objectID": "assignment/index.html",
    "href": "assignment/index.html",
    "title": "Assignments",
    "section": "",
    "text": "The main goals of this class are to help you design, critique, code, and run rigorous, valid, and feasible evaluations of public sector programs. Each type of assignment in this class is designed to help you achieve one or more of these goals."
  },
  {
    "objectID": "assignment/index.html#weekly-check-in",
    "href": "assignment/index.html#weekly-check-in",
    "title": "Assignments",
    "section": "Weekly check-in",
    "text": "Weekly check-in\nEvery week, after you finish working through the content, I want to hear about what you learned and what questions you still have. Because the content in thsi course is flipped, these questions are crucial for our weekly in-class discussions.\nTo encourage engagement with the course content—and to allow me to collect the class’s questions each week—you’ll need to fill out a short response on iCollege. This should be ≈150 words. That’s fairly short: there are ≈250 words on a typical double-spaced page in Microsoft Word (500 when single-spaced).\nThese check-ins are due by noon on the days we have class. This is so I can look through the responses and start structuring the discussion for the evening’s class.\nYou should answer these two questions each week:\n\nWhat were the three (3) most interesting or exciting things you learned from the session? Why?\nWhat were the three (3) muddiest or unclear things from the session this week? What are you still wondering about?\n\nYou can include more than three interesting or muddiest things, but you must include at least three. There should be six easily identifiable things in each check-in: three exciting things and three questions.\nI will grade these check-ins using a check system:\n\n✔+: (11.5 points (115%) in gradebook) Response shows phenomenal thought and engagement with the course content. I will not assign these often.\n✔: (10 points (100%) in gradebook) Response is thoughtful, well-written, and shows engagement with the course content. This is the expected level of performance.\n✔−: (5 points (50%) in gradebook) Response is hastily composed, too short, and/or only cursorily engages with the course content. This grade signals that you need to improve next time. I will hopefully not assign these often.\n\nNotice that is essentially a pass/fail or completion-based system. I’m not grading your writing ability, I’m not counting the exact number of words you’re writing, and I’m not looking for encyclopedic citations of every single reading to prove that you did indeed read everything. I’m looking for thoughtful engagement, three interesting things, and three questions. That’s all. Do good work and you’ll get a ✓.\nYou will submit these check-ins via iCollege."
  },
  {
    "objectID": "assignment/index.html#problem-sets",
    "href": "assignment/index.html#problem-sets",
    "title": "Assignments",
    "section": "Problem sets",
    "text": "Problem sets\nTo practice writing R code, running inferential models, and thinking about causation, you will complete a series of problem sets.\nYou need to show that you made a good faith effort to work each question. I will not grade these in detail. The problem sets will be graded using a check system:\n\n✔+: (33 points (110%) in gradebook) Assignment is 100% completed. Every question was attempted and answered, and most answers are correct. Document is clean and easy to follow. Work is exceptional. I will not assign these often.\n✔: (30 points (100%) in gradebook) Assignment is 70–99% complete and most answers are correct. This is the expected level of performance.\n✔−: (15 points (50%) in gradebook) Assignment is less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not asisgn these often.\n\nYou may (and should!) work together on the problem sets, but you must turn in your own answers. You cannot work in groups of more than four people, and you must note who participated in the group in your assignment."
  },
  {
    "objectID": "assignment/index.html#evaluation-assignments",
    "href": "assignment/index.html#evaluation-assignments",
    "title": "Assignments",
    "section": "Evaluation assignments",
    "text": "Evaluation assignments\nFor your final project, you will conduct a pre-registered evaluation of a social program using synthetic data. To (1) give you practice with the principles of program evaluation, research design, measurement, and causal diagrams, and (2) help you with the foundation of your final project, you will complete a set of four evaluation-related assignments.\nIdeally these will become major sections of your final project. However, there is no requirement that the programs you use in these assignments must be the same as the final project. If, through these assignments, you discover that your initially chosen program is too simple, too complex, too boring, etc., you can change at any time.\nThese assignments will be graded using a check system:\n\n✔+: (33 points (110%) in gradebook) Assignment is 100% completed. Every question was attempted and answered, and most answers are correct. Document is clean and easy to follow. Work is exceptional. I will not assign these often.\n✔: (30 points (100%) in gradebook) Assignment is 70–99% complete and most answers are correct. This is the expected level of performance.\n✔−: (15 points (50%) in gradebook) Assignment is less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not asisgn these often."
  },
  {
    "objectID": "assignment/index.html#exams",
    "href": "assignment/index.html#exams",
    "title": "Assignments",
    "section": "Exams",
    "text": "Exams\nThere will be two exams covering (1) program evaluation, design, and causation, and (2) the core statistical tools of program evaluation and causal inference.\nYou will take these exams online through iCollege. The exams will have a time limit, but you can use notes and readings and the Google. You must take the exams on your own though, and not talk to anyone about them."
  },
  {
    "objectID": "assignment/index.html#final-project",
    "href": "assignment/index.html#final-project",
    "title": "Assignments",
    "section": "Final project",
    "text": "Final project\nAt the end of the course, you will demonstrate your knowledge of program evaluation and causal inference by completing a final project.\nComplete details for the final project are here.\nThere is no final exam. This project is your final exam."
  },
  {
    "objectID": "content/01-content.html#lecturas",
    "href": "content/01-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas\nR for Data Science"
  },
  {
    "objectID": "content/03-content.html#lecturas",
    "href": "content/03-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas\nR for Data Science"
  },
  {
    "objectID": "content/05-content.html#lecturas",
    "href": "content/05-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas\nR for Data Science"
  },
  {
    "objectID": "content/07-content.html#lecturas",
    "href": "content/07-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas\nBrown (2015)The Common Factor Model and Exploratory Factor Analysis"
  },
  {
    "objectID": "content/09-content.html#lecturas",
    "href": "content/09-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas",
    "crumbs": [
      "Clases",
      "Sesiones",
      "Regresión lineal II"
    ]
  },
  {
    "objectID": "content/11-content.html#lecturas",
    "href": "content/11-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas",
    "crumbs": [
      "Clases",
      "Sesiones",
      "Regresión logística II"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Metodología Cuantitativa Avanzada\n        ",
    "section": "",
    "text": "Metodología Cuantitativa Avanzada\n        \n        \n            Magister en Ciencias Sociales, mención Sociología de la Modernización\n        \n        \n            MCS7113 • Primer Semestre 2024Departamento de Sociología FACSOUniversidad de Chile\n        \n    \n    \n        \n    \n\n\n\n\n\n\nProfesores\n\n   Pablo Perez Ahumada\n   Departamento de Sociología FACSO - sala 319\n   &lt;a href=“mailto:pabloperez@uchile.cl”&gt;pabloperez@uchile.cl\n   pablo_perez_a\n\n\n\n   Kevin Carrasco Quintanilla\n   Departamento de Sociología FACSO - sala 328\n   &lt;a href=“mailto:pabloperez@uchile.cl”&gt;kevin.carrasco@ug.uchile.cl\n   kevincarrascoq1\n\n\n\n   Daniela Olivares Collío\n   Departamento de Sociología FACSO - sala 328\n   &lt;a href=“mailto:danielaolivarescollio@gmail.com”&gt;danielaolivarescollio@gmail.com\n   \n\n\n\nInformación del curso\n\n   Viernes\n   Marzo – Julio, 2024\n   09:00-11:45 AM\n   Sala 35. FACSO \n\n\n\nContacto\nA través de correo"
  },
  {
    "objectID": "resource/02-resource.html",
    "href": "resource/02-resource.html",
    "title": "Práctico 2. Matrices de correlación y tamaños de efecto",
    "section": "",
    "text": "El objetivo de esta guía práctica es conocer maneras de reportar coeficientes de correlación y cómo interpretar sus tamaños de efecto en ciencias sociales. Además, nos introduciremos en el tratamiento de valores perdidos y otras medidas de correlación entre variables.\nEn detalle, aprenderemos:\n\nCómo reportar y presentar matrices de correlación.\nInterpretar el tamaño de efecto de una correlación.\nTratamiento de casos perdidos.\nQué es y cómo calcular la correlación de Spearman.\nQué es el coeficiente de determinación \\(R^2\\).\n\n\n\n\n\n\n\nNota\n\n\n\n¿Qué era la correlación?\nLa correlación es una medida de asociación entre variables, que describe el sentido (dirección) y fuerza de la asociación.\nEn otras palabras, nos permite conocer cómo y cuánto se relaciona la variación de una variable, con la variación de otra variable.\n\n\n\n\nEn esta práctica trabajaremos con un subconjunto de datos previamente procesados del Estudio Longitudinal Social de Chile (ELSOC) del año 2016, elaborado por COES. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  ELSOC 2016. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos ELSOC 2016.",
    "crumbs": [
      "Prácticos",
      "Práctico 2"
    ]
  },
  {
    "objectID": "resource/02-resource.html#recursos-de-la-práctica",
    "href": "resource/02-resource.html#recursos-de-la-práctica",
    "title": "Práctico 2. Matrices de correlación y tamaños de efecto",
    "section": "",
    "text": "En esta práctica trabajaremos con un subconjunto de datos previamente procesados del Estudio Longitudinal Social de Chile (ELSOC) del año 2016, elaborado por COES. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  ELSOC 2016. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos ELSOC 2016.",
    "crumbs": [
      "Prácticos",
      "Práctico 2"
    ]
  },
  {
    "objectID": "resource/02-resource.html#tratamiento-de-casos-perdidos",
    "href": "resource/02-resource.html#tratamiento-de-casos-perdidos",
    "title": "Práctico 2. Matrices de correlación y tamaños de efecto",
    "section": "Tratamiento de casos perdidos",
    "text": "Tratamiento de casos perdidos\nTrabajar con datos a menudo implica enfrentar valores perdidos (NA), lo que puede ser un gran desafío. Estos valores indican la ausencia de un valor en una base de datos. Los valores perdidos pueden originarse por diversas razones, como el sesgo de no respuesta en encuestas, errores en la entrada de datos o simplemente la falta de información para ciertas variables.\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\nNA\n4\n1\nHola\n\n\n7\n1\n4\nNo soy un NA\n\n\n8\nNA\n2\nNA\n\n\n9\nNA\n9\nAmo R\n\n\n3\n3\n6\nNA\n\n\n\n\n\n\n\n\n\nLa presencia de valores perdidos puede tener un impacto considerable en la precisión y confiabilidad de los análisis estadísticos, lo que a su vez puede conducir a resultados sesgados y conclusiones incorrectas.\nExisten varias formas de tratar valores perdidos, que van desde enfoques simples hasta métodos más complejos, como la imputación. En esta ocasión, nos centraremos en las dos estrategias más comunes:\n\ntrabajar exclusivamente con casos completos (listwise) o\nretener los casos con valores perdidos, pero excluyéndolos al calcular estadísticas (pairwise).\n\n\na) Analísis con casos completos: listwise deletion\nEste enfoque es uno de los más conocidos: implica remover completamente las observaciones que tienen valores perdidos en cualquier variable de interés. En otras palabras, si una fila/caso en un conjunto de datos tiene al menos un valor faltante en alguna de las variables que estás considerando, se eliminará por completo.\nEn R, esto podemos hacerlo con la función na.omit. Para hacer esto, sigamos estos pasos:\n\nrespaldar la base de datos original en el espacio de trabajo (por si queremos en adelante realizar algún análisis referido a casos perdidos)-\ncontamos el número de casos con el comando dim.\ncontamos cuántos y en dónde tenemos casos perdidos.\nborramos los casos perdidos con na.omit.\ncontamos nuevamente con dim para asegurarnos que se borraron.\n\n\nproc_elsoc_original &lt;- proc_elsoc\ndim(proc_elsoc)\n\n[1] 2927    7\n\n\n\nsum(is.na(proc_elsoc))\n\n[1] 81\n\n\n\ncolSums(is.na(proc_elsoc))\n\nmesfuerzo  mtalento       ess    edcine      sexo      edad    pmerit \n       18        20        12         2         0         0        29 \n\n\n\nproc_elsoc &lt;- na.omit(proc_elsoc)\ndim(proc_elsoc)\n\n[1] 2887    7\n\n\nAhora nos quedamos con 2887 observaciones sin casos perdidos.\nAunque es simple de implementar, con este enfoque podemos perder información importante, especialmente si los valores perdidos no se distribuyen aleatoriamente.\n\nSiempre hay que intentar rescatar la mayor cantidad de casos posibles. Por lo tanto, si un listwise genera más de un 10% de casos perdidos se debe detectar qué variables esta produciendo esta pérdida e intentar recuperar datos. Puedes revisar un ejemplo aquí.\n\n\n\nb) Retener pero excluir: pairwise deletion\nA diferencia del anterior, este es un enfoque en el que las observaciones se utilizan para el análisis siempre que tengan datos disponibles para las variables específicas que se están analizando. En lugar de eliminar toda una fila si falta un valor, se eliminan solo los valores faltantes en las variables que se están analizando en ese momento.\nPara hacer esto en R debemos siempre verificar e indicar en nuestro código si queremos (o no) remover los NA para realizar los análisis.\n\nmean(proc_elsoc_original$pmerit); mean(proc_elsoc$edad); mean(proc_elsoc$ess)\n\n[1] NA\n\n\n[1] 45.98337\n\n\n[1] 4.333564\n\nmean(proc_elsoc_original$pmerit, na.rm = TRUE); mean(proc_elsoc$edad, na.rm = TRUE); mean(proc_elsoc$ess, na.rm = TRUE)\n\n[1] 2.653899\n\n\n[1] 45.98337\n\n\n[1] 4.333564\n\n\nCon el primer código no obtuvimos información sustantiva en ciertas variables, pero con el segundo sí al remover los NA solo de dicha variable para un cálculo determinado.",
    "crumbs": [
      "Prácticos",
      "Práctico 2"
    ]
  },
  {
    "objectID": "resource/04-resource.html",
    "href": "resource/04-resource.html",
    "title": "Práctico 4. Inferencia Estadística y Curva Normal",
    "section": "",
    "text": "El objetivo de esta guía práctica es continuar profundizando en la inferencia estadística, en particular en .\n\nAplicar pruebas de hipótesis de diferencia de medias.\nAplicar pruebas de hipótesis direccionales.\nAplicar inferencia estadística a proporciones.\nEmplear la correlación en contexto de inferencia.\n\n\n\nEn esta práctica trabajaremos con un subconjunto de datos previamente procesados de la Encuesta de Caracterización Socioeconómica (CASEN) del año 2022, elaborada por el Ministerio de Desarrollo Social y Familia. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  CASEN 20222. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos CASEN 2022.",
    "crumbs": [
      "Prácticos",
      "Práctica 4"
    ]
  },
  {
    "objectID": "resource/04-resource.html#recursos-de-la-práctica",
    "href": "resource/04-resource.html#recursos-de-la-práctica",
    "title": "Práctico 4. Inferencia Estadística y Curva Normal",
    "section": "",
    "text": "En esta práctica trabajaremos con un subconjunto de datos previamente procesados de la Encuesta de Caracterización Socioeconómica (CASEN) del año 2022, elaborada por el Ministerio de Desarrollo Social y Familia. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  CASEN 20222. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos CASEN 2022.",
    "crumbs": [
      "Prácticos",
      "Práctica 4"
    ]
  },
  {
    "objectID": "resource/04-resource.html#pruebas-de-dos-colas-o-una-cola",
    "href": "resource/04-resource.html#pruebas-de-dos-colas-o-una-cola",
    "title": "Práctico 4. Inferencia Estadística y Curva Normal",
    "section": "Pruebas de dos colas o una cola",
    "text": "Pruebas de dos colas o una cola\nEn estadística, la formulación de hipótesis que implica dos variables (o la comparación de grupos) busca determinar si existen diferencias en una variable entre grupos y, de ser el caso, evaluar si esta diferencia es estadísticamente significativa.\nExisten los contrastes de hipótesis sobre diferencias entre grupos, también se les llama hipótesis de dos colas.\n\n\n\n\n\n\nPrueba de dos colas\n\n\n\nContrastamos la hipótesis nula (o de trabajo) de no diferencias entre grupos: \\[  H_{0}: \\mu_{1} - \\mu_{2} = 0 \\] En relación a una hipótesis alternativa sobre diferencias entre grupos: \\[  H_{A}: \\mu_{1} - \\mu_{2} \\neq 0 \\]\n\n\nSin embargo, también podemos plantear hipótesis respecto a que el valor de cierto parámetro para un grupo puede ser mayor o menor al de otro grupo. A esto se le conoce como hipótesis de una cola.\n\n\n\n\n\n\nPrueba de una cola\n\n\n\n\\[  H_{0}: \\mu_{0} ≥ \\mu_{1} ; \\mu_{0} ≤ \\mu_{1}\\]\n\\[  H_{A}: \\mu_{0} &gt; \\mu_{1} \\]\n\\[  H_{A}: \\mu_{0} &lt; \\mu_{1} \\]",
    "crumbs": [
      "Prácticos",
      "Práctica 4"
    ]
  },
  {
    "objectID": "resource/04-resource.html#cálculo-paso-a-paso-de-estadístico-t",
    "href": "resource/04-resource.html#cálculo-paso-a-paso-de-estadístico-t",
    "title": "Práctico 4. Inferencia Estadística y Curva Normal",
    "section": "Cálculo paso a paso de estadístico t",
    "text": "Cálculo paso a paso de estadístico t\nEn esta sección se realizará el cálculo paso a paso del estadístico \\(t\\) del ejemplo anterior para demostrar cómo se origina la información que aparece en el output de R.\nLa fórmula de t:\n\\(t=\\frac{(\\bar{x}_1-\\bar{x}_2)}{\\sqrt{\\frac{s_1²}{\\sqrt{n_1}}+\\frac{s_2²}{\\sqrt{n_2}} }}\\)\nDonde en la parte superior se encuentra la diferencia de medias entre dos grupos, y en la inferior el error estándar de t.\nPasos:\n\nSe calcula la diferencia de medias\nSe calcula el error estándar de la diferencia de medias\nCálculo del valor t\nSe fija un \\(\\alpha\\) (usualmente 0.05) para rechazar \\(H_0\\), y se busca el valor crítico asociado a este \\(\\alpha\\) (en una tabla de valores t, o en R)\nSi nuestro t es superior al valor crítico, se rechaza \\(H_0\\)\n\nPaso 1: Calculamos la diferencia de medias \\((\\bar{x}_1-\\bar{x}_2)\\)\n\nmuestra %&gt;%\n  dplyr::group_by(sexo=sjlabelled::as_label(sexo)) %&gt;% # se agrupan por la variable categórica y se usan sus etiquetas con as_label\n  dplyr::summarise(Obs.=n(),Promedio=mean(edad, na.rm=TRUE),SD=sd(edad, na.rm=TRUE)) %&gt;% # se agregan las operaciones a presentar en la tabla\n  kable(format = \"markdown\")\n\n\n\n\nsexo\nObs.\nPromedio\nSD\n\n\n\n\n1\n6\n32.33333\n1.505545\n\n\n2\n4\n25.25000\n2.629956\n\n\n\n\ndif_medias &lt;- 32.333 - 25.250\ndif_medias\n\n[1] 7.083\n\n\nPaso 2: Calculamos el error estándar de la diferencia de medias: \\(\\sqrt{\\frac{s_1²}{\\sqrt{n_1}}+\\frac{s_2²}{\\sqrt{n_2}}}\\)\n\nmuestra_h &lt;- muestra %&gt;% filter(sexo==1)\nmuestra_m &lt;- muestra %&gt;% filter(sexo==2)\n  \ns_h &lt;- sd(muestra_h$edad)\nn_h &lt;- length(muestra_h$edad)\ns_m &lt;- sd(muestra_m$edad)\nn_m &lt;- length(muestra_m$edad)\n\nee &lt;- sqrt((s_h^2)/n_h + (s_m^2)/n_m)\nee\n\n[1] 1.451532\n\n\nPaso 3: Cálculo del valor t\n\nte &lt;- dif_medias/ee\nte\n\n[1] 4.879673\n\n\nPaso 4: Fijamos un \\(\\alpha\\) y se busca el valor crítico de t asociado al \\(\\alpha\\). En este caso utilizaremos el valor usual de \\(\\alpha = 0.05\\).\n\ntt &lt;- qt(0.05/2,df=9,lower.tail=F)\ntt\n\n[1] 2.262157\n\n\nPaso 5: test de hipótesis\nSegún la distribución t, el valor crítico para poder rechazar \\(H_0\\) con un 95% de confianza es 2.26. El t calculado con información de la muestra (o t empírico) es 4.87. Este valor es superior al t crítico, por lo tanto se rechaza \\(H_0\\) con un 95% de confianza, o una probabilidad de error p&lt;0.05.",
    "crumbs": [
      "Prácticos",
      "Práctica 4"
    ]
  },
  {
    "objectID": "resource/04-resource.html#ejemplo-casos-reales",
    "href": "resource/04-resource.html#ejemplo-casos-reales",
    "title": "Práctico 4. Inferencia Estadística y Curva Normal",
    "section": "Ejemplo casos reales",
    "text": "Ejemplo casos reales\nComencemos por preparar nuestros datos. Iniciamos cargando las librerías necesarias.\n\npacman::p_load(tidyverse, # Manipulacion datos\n               sjPlot, #tablas\n               confintr, # IC\n               gginference, # Visualizacion \n               rempsyc, # Reporte\n               broom) # Varios\npackage 'confintr' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\kevin\\AppData\\Local\\Temp\\RtmpS2iUq4\\downloaded_packages\npackage 'gginference' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\kevin\\AppData\\Local\\Temp\\RtmpS2iUq4\\downloaded_packages\npackage 'rempsyc' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\kevin\\AppData\\Local\\Temp\\RtmpS2iUq4\\downloaded_packages\n\noptions(scipen = 999) # para desactivar notacion cientifica\nrm(list = ls()) # para limpiar el entorno de trabajo\n\nCargamos los datos directamente desde internet.\n\nload(url(\"https://github.com/cursos-metodos-facso/datos-ejemplos/raw/main/proc_casen.RData\")) #Cargar base de datos\n\nA continuación, exploramos la base de datos proc_casen.\n\nnames(proc_casen) # Nombre de columnas\n\n [1] \"id_vivienda\"      \"folio\"            \"id_persona\"       \"hogar\"           \n [5] \"nucleo\"           \"varunit\"          \"varstrat\"         \"expr\"            \n [9] \"edad\"             \"sexo\"             \"educ\"             \"activ\"           \n[13] \"y1\"               \"ytrabajocor\"      \"pobreza_multi_5d\" \"o15\"             \n[17] \"qaut\"             \"fdt\"              \"ocupado\"          \"desocupado\"      \n[21] \"inact\"            \"hijo\"             \"n_educ\"           \"universitaria\"   \n[25] \"tipo_ocup\"        \"ss_salud\"         \"ayuda_moverse\"    \"ayuda_thogar\"    \n[29] \"disc_fisica\"     \n\ndim(proc_casen) # Dimensiones\n\n[1] 202111     29\n\n\nContamos con 24 variables (columnas) y 202.111 observaciones (filas).\nEvaluemos si el promedio de ingresos del trabajo de las mujeres es distinto al de los hombres en Chile en el 2022.\nApliquemos nuestros cinco pasos para inferencia.\n\nFormulamos nuestras hipótesis y dirección de la prueba:\n\n\n\\(H_{0}\\): \\(\\mu_{hombres}\\) \\(-\\) \\(\\mu_{mujeres}\\) \\(=\\) \\(0\\)\n\\(H_{A}\\): \\(\\mu_{hombres}\\) \\(-\\) \\(\\mu_{mujeres}\\) \\(\\neq\\) \\(0\\)\n\n\nCalcula el error estándar (SE) para diferencia de medias:\n\n\nocupados &lt;- proc_casen %&gt;%\n  filter(ocupado == 1) %&gt;% \n  na.omit() #  subset de datos solo con personas ocupadas\n\ndatos_t &lt;- ocupados %&gt;% \n  group_by(sexo) %&gt;% \n  summarise(media = mean(ytrabajocor, na.rm = T),\n            ds = sd(ytrabajocor, na.rm = T),\n            n = n())\n\ndatos_t\n\n# A tibble: 2 × 4\n   sexo   media      ds     n\n  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n1     1 578107. 388352.    64\n2     2 516235. 405302.   109\n\n\nObtenemos la diferencia de medias (\\(\\bar{x_1}\\) - \\(\\bar{x_2}\\))\n\ndif_medias &lt;- 817688.2 - 674428.3\ndif_medias\n\n[1] 143259.9\n\n\nAhora, calculamos el error estándar.\n\ns_h &lt;- 837710.9 \ns_m &lt;- 638044.1\n\nn_h &lt;- 32019 \nn_m &lt;- 26313    \n\nse_dif &lt;- sqrt((s_h^2)/n_h + (s_m^2)/n_m)\nse_dif\n\n[1] 6114.607\n\n\n\nCalcula el valor estimado de la prueba (t para diferencia de medias):\n\n\nt_stat &lt;- dif_medias/se_dif\nt_stat\n\n[1] 23.42912\n\n\n\nEspecifica el valor crítico:\n\n\ndf &lt;- n_h + n_m - 2 # definimos grados de libertad\n\nt_critico &lt;- qt(p = 0.05/2, df, lower.tail = FALSE)\nt_critico\n\n[1] 1.960005\n\n\n\nContrasta el valor estimado con el crítico e interpreta los resultados:\n\n\nt_stat &gt; t_critico\n\n[1] TRUE\n\n\nComparamos el valor estimado con el valor crítico para dos colas. Por tanto, nuestro valor estimado queda dentro de la zona de rechazo de \\(H_0\\). En consecuencia, podemos decir que:\n\nLa prueba T que evalúa la diferencia de medias entre el ingreso del trabajo según sexo sugiere que el efecto es positivo y estadísticamente signficativo (diferencia = 143.260, t(58004.33) = 23.43, p &lt; .001). Por tanto, rechazamos la \\(H_{0}\\) sobre igualdad de medias con un 95% de confianza, existiendo evidencia a favor de nuestra \\(H_{A}\\) ya que hay diferencias salariales significativas entre hombres y mujeres.\n\n\nY el cálculo directo en R:\n\n\nt_results &lt;- t.test(ocupados$ytrabajocor ~ ocupados$sexo, \n       alternative = \"two.sided\")\n\n# stats.table &lt;- tidy(t_results, conf_int = T)\n# nice_table(stats.table, broom = \"t.test\")\n\nVisualicemos la distribución de esta prueba y su zona de rechazo.\n\nggttest(t_results)\n\n\n\n\n\n\n\n\nAdemás, podemos calcular un intervalo de confianza que acompaña nuestra estimación. En este caso, vemos que el IC para la diferencia de medias oscila entre [131.275 - 155.245] y no contiene el cero, por lo que podemos rechazar la hipótesis nula.",
    "crumbs": [
      "Prácticos",
      "Práctica 4"
    ]
  },
  {
    "objectID": "resource/06-resource.html",
    "href": "resource/06-resource.html",
    "title": "Práctico 6. Asociación con variables categóricas",
    "section": "",
    "text": "El objetivo de esta guía práctica es introducirnos en técnicas de asociación entre variables categóricas, aplicando lo apredendido hasta ahora sobre inferencia estadística.\nEn detalle, aprenderemos:\n\nGenerar y analizar tablas de contingencia (o cruzadas)\nEstimar e interpretar la prueba de Chi-cuadrado\nAplicar coeficientes de correlación entre variables categóricas\n\n\n\nEn esta práctica trabajaremos con un subconjunto de datos previamente procesados de la Encuesta de Caracterización Socioeconómica (CASEN) del año 2022, elaborada por el Ministerio de Desarrollo Social y Familia. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienen la opción de acceder a la misma información a través del siguiente enlace:  CASEN 20222. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos CASEN 2022.",
    "crumbs": [
      "Prácticos",
      "Práctica 6"
    ]
  },
  {
    "objectID": "resource/06-resource.html#recursos-de-la-práctica",
    "href": "resource/06-resource.html#recursos-de-la-práctica",
    "title": "Práctico 6. Asociación con variables categóricas",
    "section": "",
    "text": "En esta práctica trabajaremos con un subconjunto de datos previamente procesados de la Encuesta de Caracterización Socioeconómica (CASEN) del año 2022, elaborada por el Ministerio de Desarrollo Social y Familia. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienen la opción de acceder a la misma información a través del siguiente enlace:  CASEN 20222. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos CASEN 2022.",
    "crumbs": [
      "Prácticos",
      "Práctica 6"
    ]
  },
  {
    "objectID": "resource/index.html",
    "href": "resource/index.html",
    "title": "Instrucciones generales",
    "section": "",
    "text": "Los prácticos consisten en el desarrollo de una guía práctica (por lo general cada semana de clases) donde se aplican y profundizan los contenidos de las clases, y donde también se abordan otras temáticas relacionadas al manejo y repote de datos.",
    "crumbs": [
      "Prácticos",
      "Descripción"
    ]
  },
  {
    "objectID": "resource/index.html#descripción",
    "href": "resource/index.html#descripción",
    "title": "Instrucciones generales",
    "section": "",
    "text": "Los prácticos consisten en el desarrollo de una guía práctica (por lo general cada semana de clases) donde se aplican y profundizan los contenidos de las clases, y donde también se abordan otras temáticas relacionadas al manejo y repote de datos.",
    "crumbs": [
      "Prácticos",
      "Descripción"
    ]
  },
  {
    "objectID": "resource/index.html#trabajo-con-software-r",
    "href": "resource/index.html#trabajo-con-software-r",
    "title": "Instrucciones generales",
    "section": "Trabajo con software R",
    "text": "Trabajo con software R\nPara los análisis estadísticos de este curso usamos el programa R, en parte porque es gratuito, pero la principal razón es que es de código abierto. Esto quiere decir que cualquier persona puede revisar cómo está hecho y aportar con modificaciones y procedimientos nuevos, como son las librerías que realizan funciones específicas.\nEl carácter de apertura de R posee muchas ventajas, pero también conlleva complicaciones. Se actualiza permanentemente, así como también las librerías, y esto puede generar problemas de compatibilidad y de fallas en ejecución del código de análisis.\nPara minimizar estos posibles problemas en este curso, vamos a:\n\ntrabajar con la misma y última versión de R, que es la 4.3 (Chequear con sessionInfo())\nevitar uso de tilde, ñ, espacios y mayúsculas tanto en carpetas y archivos, así como también en los nombres de las variables",
    "crumbs": [
      "Prácticos",
      "Descripción"
    ]
  },
  {
    "objectID": "resource/index.html#sobre-errores-y-consultas-respecto-a-problemas-con-r-y-ejecución-de-código",
    "href": "resource/index.html#sobre-errores-y-consultas-respecto-a-problemas-con-r-y-ejecución-de-código",
    "title": "Instrucciones generales",
    "section": "Sobre errores y consultas respecto a problemas con R y ejecución de código",
    "text": "Sobre errores y consultas respecto a problemas con R y ejecución de código\nEn caso de problemas con ejecución de código, se sugiere intentar solucionarlo autónomamente por no más de 10 minutos, si los problemas siguen entonces consultar.\nSe sugiere que las consultas sobre problemas en la ejecución del código y otros se realicen en los foros al final de los prácticos correspondientes, para lo cual se requiere solo habilitar una cuenta en Github. Al hacer la consulta, adjuntar la siguiente información:\n\nCódigo completo hasta que se produce el problema\nIndicar línea del código donde se produce el problema\nAdjuntar el resultado del output de la información de la sesión (sessionInfo())\n\n\nSobre el trabajo en hojas de código en RStudio\n\nEl trabajo de análisis en RStudio se efectua en una hoja de código (o R script o sintaxis, o para los usuarios de Stata la do-file), que es donde se anotan los comandos y funciones. Para abrir una hoja, en RStudio ir a File &gt; New File &gt; R Script (o ctrl+shift+N),y aparecerá un panel con una pestaña “Untitled” (sin título). Esta es la hoja de código donde se anotan los comandos.\nLos contenidos de las hojas de código son básicamente 2:\n\ncomandos o funciones: se escriben en la hoja, y para ejecutarlos se debe posicionar el cursor en la línea respectiva y ctrl+enter, el resultado aparecerá en el panel de resultados o Consola.\ntexto: para escribir títulos, comentarios, y todo lo que permita entender qué se está haciendo, al principio de la línea respectiva escribir el signo #\n\nPara grabar nuestra hoja de código y así respaldar nuestros análisis, File &gt; Save (o ctrl+s), y dar un nombre al archivo. Recordar: breve, sin espacios ni tildes ni eñes. Por defecto, la extensión de estos archivos es .R",
    "crumbs": [
      "Prácticos",
      "Descripción"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Programa",
    "section": "",
    "text": "Pablo Perez Ahumada\n   Departamento de Sociología FACSO - sala 319\n   &lt;a href=“mailto:pabloperez@uchile.cl”&gt;pabloperez@uchile.cl\n   pablo_perez_a\n   Schedule an appointment\n\n\n\n\n\n   Viernes\n   Marzo – Julio, 2024\n   09:00-11:45 AM\n   Sala 35. FACSO\n   Slack"
  },
  {
    "objectID": "syllabus.html#resumen",
    "href": "syllabus.html#resumen",
    "title": "Programa",
    "section": "Resumen",
    "text": "Resumen\nEste curso busca dar un primer acercamiento a la investigación social cuantitativa, abarcando desde aspectos iniciales básicos de estadística descriptiva y visualización de datos, hasta análisis e interpretación de modelos explicativos de investigación social. Asimismo, se busca que los y las estudiantes logren familiarizarse con el uso de Rstudio para el análisis de datos sociales.\nLa metodología incluye clases lectivas y trabajo práctico en R."
  },
  {
    "objectID": "syllabus.html#objetivo-general",
    "href": "syllabus.html#objetivo-general",
    "title": "Programa",
    "section": "Objetivo general",
    "text": "Objetivo general\nAl finalizar el curso, el/la estudiante podrá elaborar y analizar diseños de investigación social de carácter cuantitativo, así como describir cuantitativamente un conjunto de datos utilizando el lenguaje R."
  },
  {
    "objectID": "syllabus.html#objetivos-específicos",
    "href": "syllabus.html#objetivos-específicos",
    "title": "Programa",
    "section": "Objetivos específicos",
    "text": "Objetivos específicos\nAl concluir el curso lo/as estudiantes deberán haber alcanzado los siguientes resultados de aprendizaje:\n\nConocer las etapas de un diseño de investigación social cuantitativa y sus principales elementos\nFormular diseños de investigación social cuantitativa\nConocer y aplicar instrumentos de medición y tipos de estudios cuantitativos\nInterpretar y analizar los elementos centrales de una base de datos con información social\nAplicar e interpretar técnicas de estadística descriptiva según las distintas características de los datos\nAplicar e interpretar técnicas de estadística correlacional e inferencia estadística para variables con distinta unidad de medida\nAplicar e interpretar técnicas de regresión lineal y logística para variables numéricas y variables categóricas"
  },
  {
    "objectID": "syllabus.html#saberes-contenidos",
    "href": "syllabus.html#saberes-contenidos",
    "title": "Programa",
    "section": "Saberes / contenidos",
    "text": "Saberes / contenidos\n\nMódulo 1: Estadística descriptiva\n1.1 Elementos básicos de la investigación social\n\nEtapas de la investigación Social\nTipos de diseños\nDiseño de instrumentos de medición\nBases de datos: datos de corte transversal, series de tiempo, cohortes, panel o longitudinal\n\n1.2 Operacionalización y análisis de datos\n\nOperacionalización y niveles de medición\nTidy data: unir, dividir, filtrar y ordenar datos en R\nRecodificación de variables: descriptivos básicos, casos perdidos, etiquetamiento de variables\nAgrupación de datos y construcción de variables a partir de datos existentes\nConstrucción de índices y validez de escalas\n\n1.3 Visualización de datos en R\n\nTablas descriptivas y tablas de contingencia\nggplot2: gráficos de barra, de caja, dispersión e histograma\n\n\n\nMódulo 2: Inferencia y estadística correlacional\n2.1 Inferencia estadística\n2.2 Pruebas de hipótesis\n2.3 Correlación\n\n\nMódulo 3: Regresión lineal y regresión logística\n3.1 Regresión lineal de mínimos cuadrados\n\nAspectos centrales y supuestos de la regresión MCO\nInterpretación de coeficientes (variables cuantitativas y cualitativas) y efectos de interacción\nRepresentación gráfica de coeficientes de regresión lineal\n\n3.2 Regresión logística binaria\n\nAspectos básicos de la regresión logística\nTipos de coeficientes e interpretación\nRepresentación gráfica (cálculo de probabilidades predichas)"
  },
  {
    "objectID": "syllabus.html#bibliografía",
    "href": "syllabus.html#bibliografía",
    "title": "Programa",
    "section": "Bibliografía",
    "text": "Bibliografía\nWickham, Hadley & Grolemund, Garrett (2017). R for Data Science. Visualize, model, transform, tidy and import data. / Versión en español disponible acá\nMoore, D. S., & Comas, J. (2010). Estadística aplicada básica. Barcelona: Antoni Bosch.\nWooldridge, J. M. (2008). Introducción a la econometría: un enfoque moderno. Paraninfo Cengage Learning.\nCamarero, et al (2017) Regresión Logística: Fundamentos y aplicación a la investigación sociológica.\nHair, Joseph F., et al. (2004). Análisis multivariante. 5ta ed. Madrid: Prentice Hall.\nCharte, Francisco (2014). Análisis exploratorio y Visualización de datos con R."
  },
  {
    "objectID": "syllabus.html#metodología",
    "href": "syllabus.html#metodología",
    "title": "Programa",
    "section": "Metodología",
    "text": "Metodología\nEl curso se organiza en sesiones semanales, con una parte lectiva seguida de una práctica. En la parte lectiva se transmiten y discuten los conceptos centrales de la investigación cuantitativa. En la parte práctica se aplicarán los conceptos transmitidos en la parte lectiva, además de resolver dudas en el avance de los trabajos de investigación"
  },
  {
    "objectID": "syllabus.html#evaluación",
    "href": "syllabus.html#evaluación",
    "title": "Programa",
    "section": "Evaluación",
    "text": "Evaluación\nLa evaluación consistirá en"
  },
  {
    "objectID": "syllabus.html#requisitos-de-aprobación",
    "href": "syllabus.html#requisitos-de-aprobación",
    "title": "Programa",
    "section": "Requisitos de aprobación",
    "text": "Requisitos de aprobación\n\nNota mínima de aprobación: 4,0 (en escala de 1 a 7)."
  },
  {
    "objectID": "syllabus.html#palabras-clave",
    "href": "syllabus.html#palabras-clave",
    "title": "Programa",
    "section": "Palabras Clave",
    "text": "Palabras Clave\n\nEstadística, investigación cuantitativa, manipulación de datos, visualización de datos, interpretación de coeficientes"
  }
]
[
  {
    "objectID": "assignment/04-practico.html",
    "href": "assignment/04-practico.html",
    "title": "Práctico 4. Inferencia",
    "section": "",
    "text": "El objetivo de esta guía práctica es, primero, introducirnos en la inferencia estadística, revisando los conceptos y aplicaciones de la curva normal y las probabilidades bajo esta con puntajes Z. Segundo, revisar algunos de sus conceptos clave como distribución muestral, error estándar, y la definición y aplicaciones de los intervalos de confianza. Y tercero, contraste de hipótesis de diferencia de medias y proporciones.\nEn detalle, aprenderemos:\n\nQué es la inferencia estadística.\nQué es la distribución normal y cómo interpretarla.\nCómo calcular probabilidades asociadas con valores Z en R.\nQué es una distribución muestral.\nQué es el error estándar.\nQué son y cómo calcular intervalos de conafianza.\nTest de hipótesis para diferencia de medias\nTest de hipótesis para diferencia de proporciones",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 04. Inferencia"
    ]
  },
  {
    "objectID": "assignment/04-practico.html#objetivo-de-la-práctica",
    "href": "assignment/04-practico.html#objetivo-de-la-práctica",
    "title": "Práctico 4. Inferencia",
    "section": "",
    "text": "El objetivo de esta guía práctica es, primero, introducirnos en la inferencia estadística, revisando los conceptos y aplicaciones de la curva normal y las probabilidades bajo esta con puntajes Z. Segundo, revisar algunos de sus conceptos clave como distribución muestral, error estándar, y la definición y aplicaciones de los intervalos de confianza. Y tercero, contraste de hipótesis de diferencia de medias y proporciones.\nEn detalle, aprenderemos:\n\nQué es la inferencia estadística.\nQué es la distribución normal y cómo interpretarla.\nCómo calcular probabilidades asociadas con valores Z en R.\nQué es una distribución muestral.\nQué es el error estándar.\nQué son y cómo calcular intervalos de conafianza.\nTest de hipótesis para diferencia de medias\nTest de hipótesis para diferencia de proporciones",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 04. Inferencia"
    ]
  },
  {
    "objectID": "assignment/04-practico.html#inferencia-estadística",
    "href": "assignment/04-practico.html#inferencia-estadística",
    "title": "Práctico 4. Inferencia",
    "section": "1. Inferencia estadística",
    "text": "1. Inferencia estadística\nEn estadística, llamamos inferencia al ejercicio de extrapolar determinadas estimaciones (estadístico) de una muestra a una población más grande (parámetro). En concreto, es el proceso de realizar conclusiones o predicciones sobre una población a partir de una muestra o subconjunto de esa población.\n\nUn concepto central en todo esto es la probabilidad de error, es decir, en qué medida nos estamos equivocando (o estamos dispuestos a estar equivocados) en tratar de extrapolar una estimación muestral a la población.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 04. Inferencia"
    ]
  },
  {
    "objectID": "assignment/04-practico.html#distribución-normal",
    "href": "assignment/04-practico.html#distribución-normal",
    "title": "Práctico 4. Inferencia",
    "section": "2. Distribución normal",
    "text": "2. Distribución normal\n\n\n\n\n\n\nDistribución de una variable\n\n\n\nRecordemos que por distribución nos referimos al conjunto de todos los valores posibles de una variable y las frecuencias (o probabilidades) con las que se producen.\n\n\nExisten distribuciones empíricas y distribuciones teóricas, en donde:\n\nlas primeras reflejan la distribución de los valores que asume la variable en un grupo concreto a partir de una observación.\nlas segundas son una función matématica que expresan la distribución de un conjunto de números mediante su probabilidad de ocurencia.\n\nPara empezar, veamos una de las distribuciones teóricas más conocidas: la distribución normal estándar. La distribución normal estándar:\n\nes una distribución normal con una media de 0 y una desviación estándar de 1.\nsimétricas y con un solo punto de elevación\nla media se sitúa al centro, y la desviación estandar expresa su dispersión\nla pendiente es más fuerte cerca del centro, y se suaviza hacia los extremos\nlos puntos en los que tiene lugar este cambio de curvatura se hallan a una distancia σ, a ambos lados de la media µ.\n\nCon R es posible generar un conjunto de datos simulados con una distribución normal.\n\nx.values &lt;- seq(-4,4, length = 1000)\ny.values &lt;- dnorm(x.values)\nplot(x.values, y.values, type=\"l\", xlab=\"Z value\", ylab=\"Probability\", main=\"Normal Distribution\")\n\n\n\n\n\n\n\n\n¿Qué estamos haciendo en cada una de las 3 líneas de código? ¿Qué variables se crearon y cómo nos aseguramos de que los datos generados siguieran una distribución normal? Pensemos un poco…\nAhora podemos preguntar qué parte de la curva cae por debajo de un valor particular. Por ejemplo, preguntaremos sobre el valor 0 antes de ejecutar el código. Piense ¿cuál debería ser la respuesta?\n\npnorm(q = 0)\n\n[1] 0.5\n\n\nTenemos que la probabilidad (en una curva normal estándar) de obtener un valor igual o menor a 0 es de 0.5, es decir, del 50%, pero ¿por qué?\nPorque como la distribución normal es simétrica alrededor de cero, la probabilidad de que sea menor o igual a cero es 0.5, es decir, el 50% de la distribución está por debajo de cero y el otro 50% está por encima de cero.\n\nEsto es posible mediante la relación entre las áreas bajo la curva normal y las probabilidades.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 04. Inferencia"
    ]
  },
  {
    "objectID": "assignment/04-practico.html#probabilidades-asociadas-con-valores-z",
    "href": "assignment/04-practico.html#probabilidades-asociadas-con-valores-z",
    "title": "Práctico 4. Inferencia",
    "section": "3. Probabilidades asociadas con valores z",
    "text": "3. Probabilidades asociadas con valores z\nLa puntuación Z es una medida que se utiliza para expresar la posición relativa de un valor con respecto a la media en una distribución normal. La puntuación Z mide cuántas desviaciones estándar está un valor por encima o por debajo de la media.\nEn los ejemplos siguientes, usaremos valores Z de + 1,96 y -1,96 porque sabemos que estos valores aproximados marcan el 2,5% superior e inferior de la distribución normal estándar. Esto corresponde a un \\(alpha\\) típico = 0,05 para una prueba de hipótesis de dos colas (sobre la cual aprenderemos más en las próximas semanas).\n\npnorm(q = 1.96, lower.tail=TRUE)\n\n[1] 0.9750021\n\n\nLa respuesta nos dice lo que ya sabemos: el 97,5% de la distribución normal ocurre por debajo del valor z de 1,96.\nPodemos agregar una línea al gráfico para mostrar dónde se usaría abline.\nEl 97,5% de la distribución queda por debajo de esta línea.\n\nplot(x.values, y.values, type=\"l\", lty=1, xlab=\"Z value\", ylab=\"Probability\", main=\"Normal Distribution\") +\nabline(v = 1.96)\n\n\n\n\n\n\n\n\ninteger(0)\n\n\n¿Y si lo hacemos hacia la cola izquierda o inferior de la distribución?\n\npnorm(q = -1.96, lower.tail = TRUE)\n\n[1] 0.0249979\n\n\nTenemos que, hacia el extremo inferior de la distribución, el valor z -1,96 marca el 2,5% inferior de la distribución normal estándar.\n\nplot(x.values, y.values, type=\"l\", lty=1, xlab=\"Z value\", ylab=\"Probability\", main=\"Normal Distribution\") +\nabline(v = -1.96)\n\n\n\n\n\n\n\n\ninteger(0)\n\n\n\nEjercicio 1\nUtilice la función abline() para agregar líneas en el puntaje z apropiado para demostrar el clásico 68-95-99.7 de esta curva normal estándar.\n\nplot(x.values, y.values, type=\"l\", lty=1, xlab=\"Z value\", ylab=\"Probability\", main=\"Normal Distribution\") +\nabline(v = 1) +\nabline(v = -1) +\nabline(v = 2) +\nabline(v = -2) +\nabline(v = 3) +\nabline(v = -3)\n\n\n\n\n\n\n\n\ninteger(0)\n\n\nComo se discutió en clases, también podemos hacer lo contrario: decidir primero cuánta probabilidad queremos (percentil) y luego calcular qué valores críticos están asociados con esas probabilidades. Esto utiliza la función qnorm. Si queremos saber qué valor z marca la probabilidad p del 2,5% inferior de una distribución normal estándar, usaríamos:\n\nqnorm(p = 0.025)\n\n[1] -1.959964\n\n\nEsto nos dice que el valor z de -1,96 marca el 2,5% inferior de la distribución normal estándar. Para determinar el valor z que marca el 2,5% superior de la distribución, escribo:\n\nqnorm(p = 0.975)\n\n[1] 1.959964\n\n\n\n\nEjercicio 2\nHasta ahora hemos demostrado todo con una distribución normal estándar. Pero la mayoría de las curvas normales no son normales estándar.\nGenere una curva (como hicimos anteriormente para la distribución normal estándar) y trácela con una media de 20 y una desviación estándar de 1,65.\n\nx.values &lt;- seq(10,30, length = 1000)\ny.values &lt;- dnorm(x.values, mean = 20, sd = 1.65) # indico media y sd\nplot(x.values, y.values, type=\"l\", lty=1, xlab=\"Z value\", ylab=\"Probability\", main=\"Normal Distribution\")\n\n\n\n\n\n\n\n\nAhora, identifique el valor en el que el 97,5% de la distribución cae por debajo de este valor. Esto lo hicimos antes con qnorm.\n\nqnorm(p = .975, mean = 20, sd = 1.65)\n\n[1] 23.23394\n\n\nTenemos que el 97,5% de los valores estarán por debajo de 23,2.\n\n\nEjercicio de aplicación\nAhora que hemos generado distribuciones normales, echemos un vistazo a algunos datos y compárelos con la distribución normal. Utilizaremos un conjunto de datos desde internet, con mediciones de 247 hombres y 260 mujeres, la mayoría de los cuales eran considerados adultos jóvenes sanos.P uede encontrar una clave para los nombres de las variables aquí, pero nos centraremos en solo tres columnas: peso en kg (wgt), altura en cm (hgt) y sexo (1 = hombre; 0 = mujer).\n\nload(url(\"http://www.openintro.org/stat/data/bdims.RData\"))\n\nSeparemos estos datos en dos conjuntos, uno de hombres y otro de mujeres con la función subset\n\nmdims &lt;- subset(bdims, sex == 1)\nfdims &lt;- subset(bdims, sex == 0)\n\n\n\nEjercicio 1\nHaz un histograma de la altura de los hombres y un histograma de la altura de las mujeres. ¿Cómo compararía los diversos aspectos de las dos distribuciones?\n\nhist(mdims$hgt, xlim = c(150,200))\n\n\n\n\n\n\n\nhist(fdims$hgt, xlim = c(140,190))\n\n\n\n\n\n\n\n\n\n\nEjercicio 2\nscale es una función en R y se puede aplicar a cualquier vector numérico (lista de números en R). Genere los dos histogramas siguientes, esta vez graficando scale() de las estaturas y determine cómo la versión escalada de las alturas corresponde a las alturas originales. ¿Qué calcula la escala para cada punto?\n\nhist(scale(mdims$hgt))\n\n\n\n\n\n\n\nhist(scale(fdims$hgt))\n\n\n\n\n\n\n\n\n\n\nEjercicio 3\nNos gustaría comparar la distribución de estaturas en este conjunto de datos con la distribución normal. Para cada uno de los histogramas de alturas (sin escalar), trace una curva normal en la parte superior del histograma.\n\nCalcule la media y la desviación estándar para las alturas femeninas y guárdelas como variables, fhgtmean y fhgtsd, respectivamente.\nDetermine la lista de valores de x (el rango del eje X) y guarde este vector. Puede hacer fácilmente una lista de números usando la función seq() como lo hemos hecho antes, o teniendo el límite inferior:límite superior. Por ejemplo, para generar un vector (lista de números) del 1 al 10 y guardarlo como one_ten, usaría one_ten &lt;- 1:10.\nComo arriba, use dnorm() para tomar la lista de valores de x y encontrar el valor de y correspondiente si fuera una distribución normal perfecta. Guarde este vector como la variable y.\nVuelva a trazar su histograma y luego, en la siguiente línea, use lines(x = x, y = y, col = \"blue\") para dibujar una distribución normal encima.\n\n\nfhgtmean &lt;- mean(fdims$hgt)\nfhgtsd   &lt;- sd(fdims$hgt)\nhist(fdims$hgt, probability = TRUE, ylim = c(0, .07))\nx &lt;- 140:190\ny &lt;- dnorm(x = x, mean = fhgtmean, sd = fhgtsd)\nlines(x = x, y = y, col = \"blue\")\n\n\n\n\n\n\n\n\nSegún este gráfico, ¿parece que los datos siguen una distribución casi normal? Haz lo mismo con las estaturas masculinas.\n\nRespuesta: En general, sí, consideraría que estos valores siguen una distribución casi normal ya que el histograma se ajusta bastante bien a la curva.\n\nObserve que la forma del histograma es una forma de determinar si los datos parecen estar distribuidos casi normalmente, pero puede resultar frustrante decidir qué tan cerca está el histograma de la curva. Un enfoque alternativo implica construir una gráfica de probabilidad normal, también llamada gráfica Q-Q por “quantil-quantil”. Ejecute ambas líneas juntas.\n\nqqnorm(fdims$hgt)\nqqline(fdims$hgt)\n\n\n\n\n\n\n\n\nUn QQ plot nos muestra en el eje x los cuantiles teóricos de la distribución en términos de desviaciones estandar, y en el eje y los valores de la variable. La distribución de los puntos en una línea recta es una indicación de que los datos se distribuyen normalmente.\nVeamos otro ejemplo de otra variable de la base de datos:\n\nhist(fdims$che.de)\n\n\n\n\n\n\n\nqqnorm(fdims$che.de)\nqqline(fdims$che.de)\n\n\n\n\n\n\n\n\nUna vez que decidimos que una variable se distribuyte de forma normal, podemos responder todo tipo de preguntas sobre esa variable relacionadas con la probabilidad. Tomemos, por ejemplo, la pregunta: “¿Cuál es la probabilidad de que una mujer adulta joven elegida al azar mida más 182 cm?”\nSi suponemos que las alturas de las mujeres se distribuyen normalmente (una aproximación muy cercana también está bien), podemos encontrar esta probabilidad calculando una puntuación Z y consultando una tabla Z (también llamada tabla de probabilidad normal).\nEn R, esto se hace en un solo paso con la función pnorm (como hicimos anteriormente para la distribución normal estándar).\n\npnorm(q = 182, mean = fhgtmean, sd = fhgtsd)\n\n[1] 0.9955656\n\n\nObtenemos la proporción de mujeres que está bajo esa estatura, es decir 99,6%. Si queremos saber la proporción de mujeres que está sobre esa estatura:\n\n1 - pnorm(q = 182, mean = fhgtmean, sd = fhgtsd)\n\n[1] 0.004434387\n\n\nEn este caso, el 0,4% de las mujeres se encontraría sobre esa estatura.\nPodemos también hacer la operación inversa, es decir, a qué valor (estatura) corresponde un porcentaje o probabilidad basada en una distribución normal. Para ello utilizamos la función qnorm. Por ejemplo, para la probabilidad que calculamos más arriba para una altura de 182cm en las mujeres:\n\nqnorm(.9955656, fhgtmean, fhgtsd)\n\n[1] 182",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 04. Inferencia"
    ]
  },
  {
    "objectID": "assignment/04-practico.html#distribución-muestral",
    "href": "assignment/04-practico.html#distribución-muestral",
    "title": "Práctico 4. Inferencia",
    "section": "4. Distribución muestral",
    "text": "4. Distribución muestral\n\n\n\n\n\n\nNota\n\n\n\nVariabilidad muestral: el valor de un estadístico varía en un muestreo aleatorio repetido.\n\n\nLa distribución muestral es la distribución de las estimaciones, o estadísticos como la media o proporción, tomadas de múltiples muestras aleatorias de una población. Permite comprender cómo varían las estimaciones de una muestra a otra.\nEjemplo 1: Imaginemos que tenemos una población de niñ_s de 0 a 9 años, y tomamos múltiples muestras de 6 individu_s (n=6). Cada una de las muestras tendrá un promedio (estadístico muestral, en este caso \\(\\bar{x}\\)) diferente, que no necesariamente coincidirn con el promedio de la población (parámetro, en este caso \\(\\mu_{x}\\))\n\n\nEjemplo 2\nSi usamos valores simulados, podemos ver que todas las medias obtenidas en cada muesta son distintas.\n\nset.seed(100)  # Establecer semilla \nmuestras &lt;- replicate(100, mean(rnorm(30, mean = 50, sd = 10))) # 100 muestras de tamaño 30\nmuestras\n\n  [1] 50.28864 50.92574 49.56125 49.25099 48.40735 51.67506 49.28325 48.90829\n  [9] 50.64635 51.94797 49.68514 49.00100 49.87287 47.58167 50.91109 47.61967\n [17] 47.94270 52.20491 51.38777 50.76559 49.02265 49.16394 50.59675 51.40631\n [25] 50.28247 51.94561 50.39929 51.95632 55.25584 51.26112 49.02810 46.46643\n [33] 52.21139 49.48146 50.43067 52.84081 47.97451 46.55278 49.87576 50.88025\n [41] 50.78748 49.42165 50.46664 51.06194 49.94867 48.39219 49.79581 49.82214\n [49] 49.93791 49.16883 52.24256 51.46923 46.51443 50.23611 49.87410 50.87291\n [57] 52.32911 46.21546 47.26934 51.29019 50.49509 49.63433 53.25368 50.81717\n [65] 49.45387 49.61571 50.33774 47.02089 47.94071 50.16296 51.12409 50.68963\n [73] 50.32398 52.22186 49.95585 50.74844 48.08507 52.90382 51.43478 46.74822\n [81] 49.21148 51.83738 49.66936 49.32308 50.31842 46.47797 50.64338 50.01220\n [89] 52.26135 47.49504 49.93140 53.04953 49.59253 48.83580 49.57802 49.23299\n [97] 50.32517 50.82952 48.92960 49.47553\n\n\nSi tengo la desviación estándar de los promedios, puedo construir un intervalo de probabilidad, basado en la curva normal.\n\n\n\n\n\n\nNota\n\n\n\nUna característica importante es que se asume que las muestras tomadas de la población son aleatorias y representativas, lo que es esencial para que la distribución muestral refleje adecuadamente la variabilidad de las estimaciones.\n\n\nLa importancia de la distribución muestral es que nos permitirá estimar parámetros poblacionales a partir de estadísticos muestrales, construir intervalos de confianza, y realizar pruebas de hipótesis.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 04. Inferencia"
    ]
  },
  {
    "objectID": "assignment/04-practico.html#error-estándar",
    "href": "assignment/04-practico.html#error-estándar",
    "title": "Práctico 4. Inferencia",
    "section": "5. Error estándar",
    "text": "5. Error estándar\nEl error estándar es una medida de la variabilidad de una estimación, proporciona una medida de cuán precisas son nuestras estimaciones de la población. El error estándar del promedio corresponde a una estimación de la desviación estándar de los promedios extraídos de distintas muestras.\nConsideremos a cada punto una observación para la muestra roja, con la línea vertical como la media y la línea horizontal como la desviación estándar.\n\nLuego, lo mismo para cada una de las muestras verde, lila, naranja y negra.\n\nLo que tendremos es que cada promedio de cada muestra es un valor que podemos promediar, y tienen su propia desviación estándar a su propio promedio. Luego, a esa desviación estándar de los promedios de cada muestra al promedio general, la llamaremos error estándar.\n\n\n\n\n\n\n\nNota\n\n\n\nTeorema del límite central la distribución de los promedios de distintas muestras - o distribución muestral del promedio - se aproxima a una distribución normal.\n\n\nBasados en el teorema del límite central, es posible calcular la desviación estándar del promedio (error estándar) con una sola muestra:\n\\[\\sigma_{\\bar{X}}=SE(error estándar)=\\frac{s}{\\sqrt{N}}\\]\n\n# Generar una muestra de datos\nset.seed(123)  # Establecer semilla \nmuestra &lt;- rnorm(100, mean = 50, sd = 10)  # Muestra de tamaño 100, media 50, desviación estándar 10\n\n# Calcular el error estándar\nerror_estandar &lt;- sd(muestra) / sqrt(length(muestra))\nerror_estandar\n\n[1] 0.9128159\n\n\nLa importancia del error estándar es que nos permitirá construir intervalos de confianza, y realizar pruebas de hipótesis.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 04. Inferencia"
    ]
  },
  {
    "objectID": "assignment/04-practico.html#intervalos-de-confianza",
    "href": "assignment/04-practico.html#intervalos-de-confianza",
    "title": "Práctico 4. Inferencia",
    "section": "6. Intervalos de confianza",
    "text": "6. Intervalos de confianza\nUn intervalo de confianza es un rango dentro del cual es probable que se encuentre un parámetro poblacional con un nivel de confianza específico. Además, proporciona información sobre la precisión de nuestras estimaciones.\nPor ejemplo,\n\n# Calcular un intervalo de confianza para la media\nintervalo_confianza &lt;- t.test(muestra)$conf.int  # Intervalo de confianza del 95% para la media\nintervalo_confianza\n\n[1] 49.09283 52.71528\nattr(,\"conf.level\")\n[1] 0.95\n\n\nEl promedio de la muestra \\(\\bar{x}\\) se distruye normal, con un error estándar \\(SE\\), lo que nos permite estimar probabilidades basándonos en la curva normal.\nPor ejemplo, si nos desviamos \\(\\pm1.96SE\\) desde la media abarcaremos aproximadamente el 95% de los valores probables.\n\n\n\n\n\n\n\nNota\n\n\n\nNivel de confianza: Corresponde a la probabilidad de que la muestra elegida produzca un intervalo que incluya el parámetro que se está estimando (\\(1-\\alpha\\)).\nNivel de significacncia: Corresponde a la probabilidad de que el intervalo calculado a partir de la muestra no comprenda el parámetro de la población (\\(\\alpha\\)).\n\n\n\n6.1. Cálculo de intervalos de confianza\nAhora ¡Manos a la obra!\nCalculemos intervalos de confianza. Primero, carguemos las librerías necesarias:\n\nlibrary(pacman)\n\nWarning: package 'pacman' was built under R version 4.3.3\n\npacman::p_load(tidyverse, # colección de paquetes para manipulación de datos\n               car,       # para recodificar\n               psych,     # para analizar datos\n               sjmisc,    # para analizar datos\n               srvyr,     # para estimación de IC y ponderadores\n               Publish)   # para IC\n\noptions(scipen = 999) # para desactivar notacion cientifica\nrm(list = ls()) # para limpiar el entorno de trabajo\n\ny también carguemos la base de datos que utilizaremos, que corresponde a un subset de la Encuesta Suplementaria de ingresos ESI para ocupados:\n\nload(url(\"https://github.com/cursos-metodos-facso/datos-ejemplos/raw/main/esi-2021-ocupados.rdata\"))\n\n\n\n\n\n\n\nNota\n\n\n\nRecordemos que podemos contar con bases de datos que tengan factor de expansión (ponderador) o no. Esta distinción se presenta cuando trabajamos con muestras simples o complejas. Al trabajar con muestras complejas debemos identificar cuál es la variable del ponderador e incorporarla en nuestro cálculo, como veremos a continuación.\n\n\n\n\n6.2. Intervalos de confianza sin ponderador\nPodemos calcular intervalos de confianza con muestras representativas sin ponderadores o factores de expansión. Supongamos que es el caso.\n\nIC para Medias\nCalculemos un intervalo de confianza para la media de ingresos de personas ocupadas:\n\npsych::describe(esi$ing_t_p)\n\n   vars     n     mean       sd   median  trimmed      mad min      max\nX1    1 37124 586360.4 697362.9 405347.7 474473.1 255411.6   0 38206253\n      range skew kurtosis      se\nX1 38206253   12   402.32 3619.36\n\n\n\nPublish::ci.mean(esi$ing_t_p, alpha = 0.05)\n\n mean      CI-95%               \n 586360.41 [579266.37;593454.45]\n\n\nAl no aplicar factores de expansión, contamos con una media de ingresos de $586.360 como estimación puntual. Pero también podemos decir que con un 95% de confianza el parámetro poblacional se encontrará entre $579.266 y $593.454.\n\n\nIC para Proporciones\nPara calcular un intervalo de confianza para la proporción por la variable sexo, usamos:\n\nsjmisc::frq(esi$sexo)\n\nx &lt;numeric&gt; \n# total N=37124 valid N=37124 mean=1.44 sd=0.50\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\n    1 | 20806 | 56.04 |   56.04 |  56.04\n    2 | 16318 | 43.96 |   43.96 | 100.00\n &lt;NA&gt; |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nprop.test(x = 20806, n = 37124, conf.level = 0.95)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  20806 out of 37124, null probability 0.5\nX-squared = 542.32, df = 1, p-value &lt; 0.00000000000000022\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5553777 0.5655019\nsample estimates:\n        p \n0.5604461 \n\n\nEn este caso, sabemos que el total de las personas ocupadas de la muestra son n=37.124, y que la cantidad de hombres son 20.806, correspondientes al 56% como estimación puntual. También podemos sostener con un 95% que la proporción de hombres en la población se encuentra entre 55.54% y 56.6%.\n\n\n\n6.3. Intervalos de confianza con ponderador\nPara muestras complejas que cuentan con ponderador (o factor de expansión) también podemos hacer este ejercicio.\nPrimero, es necesario identificar la variable de factor de expansión o ponderador:\n\nesi_pond &lt;- esi %&gt;% as_survey_design(ids = 1, # indica conglomerados de muestreo; ~0 o ~1 cuando no hay\n                                     strata = estrato, # indica efecto de diseño muestral\n                                     weights = fact_cal_esi) # indica el ponderador\n\noptions(survey.lonely.psu = \"certainty\") # seteamos para que ids no moleste\n\n\nIC para Medias\nAhora, teniendo en consideración el factor de expansión, podemos señalar que:\n\nesi_pond %&gt;% \n  summarise(media = survey_mean(ing_t_p, vartype = \"ci\", levels = 0.95, na.rm=TRUE)) # usamos funcion survey_mean\n\n# A tibble: 1 × 3\n    media media_low media_upp\n    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 681039.   666563.   695516.\n\n\nEl promedio de ingresos de personas ocupadas ponderado en la población corresponde a $681.039 como estimación puntual, pero que es posible afirmar con un 95% de confianza que el parámetro poblacional se encuentra entre $666.562 y $695.516.\n\n\nIC para Proporciones\nFinalmente, si calculamos la proporción de hombres ocupados en la población considerando el factor de expansión:\n\nsjmisc::frq(esi$sexo)\n\nx &lt;numeric&gt; \n# total N=37124 valid N=37124 mean=1.44 sd=0.50\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\n    1 | 20806 | 56.04 |   56.04 |  56.04\n    2 | 16318 | 43.96 |   43.96 | 100.00\n &lt;NA&gt; |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\nesi_pond %&gt;% \n  group_by(sexo) %&gt;% # agrupamos por sexo\n  summarise(prop = survey_prop(vartype = \"ci\", levels = 0.95, na.rm = TRUE))\n\n# A tibble: 2 × 4\n   sexo  prop prop_low prop_upp\n  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     1 0.582    0.575    0.590\n2     2 0.418    0.410    0.425\n\n\nTenemos que, con un 95% de conafianza, podemos afirmar que la proporción de hombre ocupados se encuentra entre el 57% y 58%.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 04. Inferencia"
    ]
  },
  {
    "objectID": "assignment/04-practico.html#test-de-hipótesis-para-diferencia-de-medias",
    "href": "assignment/04-practico.html#test-de-hipótesis-para-diferencia-de-medias",
    "title": "Práctico 4. Inferencia",
    "section": "7. Test de hipótesis para diferencia de medias",
    "text": "7. Test de hipótesis para diferencia de medias\nPrimero, carguemos las librerías necesarias:\n\nlibrary(pacman)\npacman::p_load(tidyverse,  # colección de paquetes para manipulación de datos\n               car,        # para recodificar\n               psych,      # para analizar datos\n               sjmisc,     # para analizar datos\n              # srvyr,      # para estimación de IC y ponderadores\n              # Publish,    # para IC\n               kableExtra) # para presentación de tablas\n\noptions(scipen = 999) # para desactivar notacion cientifica\nrm(list = ls()) # para limpiar el entorno de trabajo\n\nVamos a testear la siguiente hipótesis:\n\n\\(H_a\\): existen diferencias de edad entre hombres y mujeres\n\nY su correspondiente hipótesis nula:\n\n\\(H_0\\): no existen diferencias de edad entre hombres y mujeres\n\nGeneración de datos (muestra_3) y descriptivos:\n\nmuestra_3 &lt;- data.frame(edad=c(33,35,23,32,24,25,29,31,32,31),\n                        sexo=c(1,1,2,1,2,2,2,1,1,1))\n\nmean(muestra_3$edad)\n\n[1] 29.5\n\nmuestra_3$sexo &lt;- as.factor(muestra_3$sexo)\n\nmuestra_3 %&gt;% \n  group_by(sexo) %&gt;% \n  summarise(media=mean(edad)) \n\n# A tibble: 2 × 2\n  sexo  media\n  &lt;fct&gt; &lt;dbl&gt;\n1 1      32.3\n2 2      25.2\n\n\nGráfico descriptivo:\n\nmuestra_3 %&gt;% \n  group_by(sexo) %&gt;% \n  summarise(media=mean(edad)) %&gt;% \n  ggplot(aes(x=sexo, y=media)) +\n  geom_point() +\n  ylim(25,35) +\n  labs(title = \"Medias de edad para hombres y mujeres, muestra 3\",\n                   x = \"Sexo\",\n                   y = \"Media edad\")\n\n\n\n\n\n\n\n\nPrueba t de diferencia de medias:\n\nt.test(edad ~ sexo,data=muestra_3)\n\n\n    Welch Two Sample t-test\n\ndata:  edad by sexo\nt = 4.8799, df = 4.33, p-value = 0.006658\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n  3.171534 10.995133\nsample estimates:\nmean in group 1 mean in group 2 \n       32.33333        25.25000 \n\n\nLo principal en este output es el valor p, que es la probabilidad de error de rechazar la hipótesis nula. En este caso, \\(p = 0.006658\\), que es menor a un nivel de confianza convencional como \\(\\alpha = 0.05\\), incluso es menor que un nivel más exigente como el \\(\\alpha = 0.01\\). Por lo tanto, rechazamos la hipótesis nula de igualdad de medias con un 99% de confianza, hay suficiente evidencia estadística para sostener que el promedio de edad de hombres y mujeres es diferente.\n\n7.1. Cálculo paso a paso de estadístico t\nEn esta última sección se realizará el cálculo paso a paso del estadístico \\(t\\) del ejemplo anterior para demostrar cómo se origina la información que aparece en el output de R.\nRecordemos la fórmula de t:\n\\(t=\\frac{(\\bar{x}_1-\\bar{x}_2)}{\\sqrt{\\frac{s_1²}{\\sqrt{n_1}}+\\frac{s_2²}{\\sqrt{n_2}} }}\\)\nDonde en la parte superior se encuentra la diferencia de medias entre dos grupos, y en la inferior el error estándar de t.\nPasos:\n\nSe calcula la diferencia de medias\nSe calcula el error estándar de la diferencia de medias\nCálculo del valor t\nSe fija un \\(\\alpha\\) (usualmente 0.05) para rechazar \\(H_0\\), y se busca el valor crítico asociado a este \\(\\alpha\\) (en una tabla de valores t, o en R)\nSi nuestro t es superior al valor crítico, se rechaza \\(H_0\\)\n\nPaso 1: Calculamos la diferencia de medias \\((\\bar{x}_1-\\bar{x}_2)\\)\n\nmuestra_3 %&gt;%\n   dplyr::group_by(sexo=sjlabelled::as_label(sexo)) %&gt;% # se agrupan por la variable categórica y se usan sus etiquetas con as_label\n  dplyr::summarise(Obs.=n(),Promedio=mean(edad, na.rm=TRUE),SD=sd(edad, na.rm=TRUE)) %&gt;% # se agregan las operaciones a presentar en la tabla\n  kable(format = \"markdown\")\n\n\n\n\nsexo\nObs.\nPromedio\nSD\n\n\n\n\n1\n6\n32.33333\n1.505545\n\n\n2\n4\n25.25000\n2.629956\n\n\n\n\ndif_medias &lt;- 32.333 - 25.250\ndif_medias\n\n[1] 7.083\n\n\nPaso 2: Calculamos el error estándar de la diferencia de medias: \\(\\sqrt{\\frac{s_1²}{\\sqrt{n_1}}+\\frac{s_2²}{\\sqrt{n_2}}}\\)\n\nmuestra_3h &lt;- muestra_3 %&gt;% filter(sexo==1)\nmuestra_3m &lt;- muestra_3 %&gt;% filter(sexo==2)\n  \ns_h &lt;- sd(muestra_3h$edad)\nn_h &lt;- length(muestra_3h$edad)\ns_m &lt;- sd(muestra_3m$edad)\nn_m &lt;- length(muestra_3m$edad)\n\nee &lt;- sqrt((s_h^2)/n_h + (s_m^2)/n_m)\nee\n\n[1] 1.451532\n\n\nPaso 3: Cálculo del valor t\n\nte &lt;- dif_medias/ee\nte\n\n[1] 4.879673\n\n\nPaso 4: Fijamos un \\(\\alpha\\) y se busca el valor crítico de t asociado al \\(\\alpha\\). En este caso utilizaremos el valor usual de \\(\\alpha = 0.05\\).\n\ntt &lt;- qt(0.05/2,df=9,lower.tail = F)\ntt\n\n[1] 2.262157\n\n\nPaso 5: test de hipótesis\nSegún la distribución t, el valor crítico para poder rechazar \\(H_0\\) con un 95% de confianza es 2.26. El t calculado con información de la muestra (o t empírico) es 4.87. Este valor es superior al t crítico, por lo tanto se rechaza \\(H_0\\) con un 95% de confianza, o una probabilidad de error p&lt;0.05.\n\n\n7.2. Aplicación práctica\nEn esta práctica trabajaremos con un subconjunto de datos previamente procesados de la Encuesta de Caracterización Socioeconómica (CASEN) del año 2022, elaborada por el Ministerio de Desarrollo Social y Familia. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  CASEN 20222. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos CASEN 2022.\n\n\nCinco pasos para la inferencia estadística\nEn inferencia, las pruebas de hipótesis nos ayudan a determinar si el resultado que obtenemos en nuestra muestra es un efecto real/extensible a la población o un error de muestreo. Aquí recomendamos una lista de cinco pasos lógicos para enfrentarnos a la inferencia estadística:\n\n\n\n\nPaso\n\n\nDetalle\n\n\n\n\n1\n\n\nFormula \\(H_0\\) y \\(H_A\\) y estipula la dirección de la prueba\n\n\n\n\n2\n\n\nCalcula el error estándar (SE)\n\n\n\n\n3\n\n\nCalcula el valor estimado de la prueba (ej: Z o t)\n\n\n\n\n4\n\n\nEspecifica el valor crítico de la prueba\n\n\n\n\n5\n\n\nContrasta el valor estimado con el valor crítico e intrepreta los resultados\n\n\n\nAdemás de estos 5 pasos también existe la posibilidad de calcular un intervalo de confianza, que acompañe la precisión de nuestra estimación.\n\nPreparación datos\nComencemos por preparar nuestros datos. Iniciamos cargando las librerías necesarias.\n\npacman::p_load(tidyverse, # Manipulacion datos\n               sjPlot, #tablas\n               confintr, # IC\n               gginference, # Visualizacion \n               rempsyc, # Reporte\n               broom) # Varios\n\noptions(scipen = 999) # para desactivar notacion cientifica\nrm(list = ls()) # para limpiar el entorno de trabajo\n\nCargamos los datos directamente desde internet.\n\nload(url(\"https://github.com/cursos-metodos-facso/datos-ejemplos/raw/main/proc_casen.RData\")) #Cargar base de datos\n\nA continuación, exploramos la base de datos proc_casen.\n\nnames(proc_casen) # Nombre de columnas\n\n [1] \"id_vivienda\"      \"folio\"            \"id_persona\"       \"hogar\"           \n [5] \"nucleo\"           \"varunit\"          \"varstrat\"         \"expr\"            \n [9] \"edad\"             \"sexo\"             \"educ\"             \"activ\"           \n[13] \"y1\"               \"ytrabajocor\"      \"pobreza_multi_5d\" \"o15\"             \n[17] \"qaut\"             \"fdt\"              \"ocupado\"          \"desocupado\"      \n[21] \"inact\"            \"hijo\"             \"n_educ\"           \"universitaria\"   \n[25] \"tipo_ocup\"        \"ss_salud\"         \"ayuda_moverse\"    \"ayuda_thogar\"    \n[29] \"disc_fisica\"     \n\ndim(proc_casen) # Dimensiones\n\n[1] 202111     29\n\n\nContamos con 24 variables (columnas) y 202.111 observaciones (filas).\n\n\n\n\n\n\n\n\n\nRecordemos…\nEn estadística, la formulación de hipótesis que implica dos variables (o la comparación de grupos) busca determinar si existen diferencias en una variable entre grupos y, de ser el caso, evaluar si esta diferencia es estadísticamente significativa.\nhemos aprendido a contrastar hipótesis sobre diferencias entre grupos. A esto también se le llama hipótesis de dos colas.\n\n\n\n\n\n\nPrueba de dos colas\n\n\n\nContrastamos la hipótesis nula (o de trabajo) de no diferencias entre grupos: \\[  H_{0}: \\mu_{1} - \\mu_{2} = 0 \\] En relación a una hipótesis alternativa sobre diferencias entre grupos: \\[  H_{A}: \\mu_{1} - \\mu_{2} \\neq 0 \\]\n\n\nVeamos un ejemplo con nuestros datos. Evaluemos si el promedio de ingresos del trabajo de las mujeres es distinto al de los hombres en Chile en el 2022.\nApliquemos nuestros cinco pasos para inferencia.\n\nFormulamos nuestras hipótesis y dirección de la prueba:\n\n\n\\(H_{0}\\): \\(\\mu_{hombres}\\) \\(-\\) \\(\\mu_{mujeres}\\) \\(=\\) \\(0\\)\n\\(H_{A}\\): \\(\\mu_{hombres}\\) \\(-\\) \\(\\mu_{mujeres}\\) \\(\\neq\\) \\(0\\)\n\n\nCalcula el error estándar (SE) para diferencia de medias:\n\n\nocupados &lt;- proc_casen %&gt;%\n  filter(ocupado == 1) %&gt;% \n  na.omit() #  subset de datos solo con personas ocupadas\n\ndatos_t &lt;- ocupados %&gt;% \n  group_by(sexo) %&gt;% \n  summarise(media = mean(ytrabajocor, na.rm = T),\n            ds = sd(ytrabajocor, na.rm = T),\n            n = n())\n\ndatos_t\n\n# A tibble: 2 × 4\n   sexo   media      ds     n\n  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n1     1 578107. 388352.    64\n2     2 516235. 405302.   109\n\n\nObtenemos la diferencia de medias (\\(\\bar{x_1}\\) - \\(\\bar{x_2}\\))\n\ndif_medias &lt;- 817688.2 - 674428.3\ndif_medias\n\n[1] 143259.9\n\n\nAhora, calculamos el error estándar.\n\ns_h &lt;- 837710.9 \ns_m &lt;- 638044.1\n\nn_h &lt;- 32019 \nn_m &lt;- 26313    \n\nse_dif &lt;- sqrt((s_h^2)/n_h + (s_m^2)/n_m)\nse_dif\n\n[1] 6114.607\n\n\n\nCalcula el valor estimado de la prueba (t para diferencia de medias):\n\n\nt_stat &lt;- dif_medias/se_dif\nt_stat\n\n[1] 23.42912\n\n\n\nEspecifica el valor crítico:\n\n\ndf &lt;- n_h + n_m - 2 # definimos grados de libertad\n\nt_critico &lt;- qt(p = 0.05/2, df, lower.tail = FALSE)\nt_critico\n\n[1] 1.960005\n\n\n\nContrasta el valor estimado con el crítico e interpreta los resultados:\n\n\nt_stat &gt; t_critico\n\n[1] TRUE\n\n\nComparamos el valor estimado con el valor crítico para dos colas. Por tanto, nuestro valor estimado queda dentro de la zona de rechazo de \\(H_0\\). En consecuencia, podemos decir que:\n\nRespuesta: La prueba T que evalúa la diferencia de medias entre el ingreso del trabajo según sexo sugiere que el efecto es positivo y estadísticamente signficativo (diferencia = 143.260, t(58004.33) = 23.43, p &lt; .001). Por tanto, rechazamos la \\(H_{0}\\) sobre igualdad de medias con un 95% de confianza, existiendo evidencia a favor de nuestra \\(H_{A}\\) ya que hay diferencias salariales significativas entre hombres y mujeres.\n\n\nY el cálculo directo en R:\n\n\n\nt_results &lt;- t.test(ocupados$ytrabajocor ~ ocupados$sexo, \n       alternative = \"two.sided\")\n\nstats.table &lt;- tidy(t_results, conf_int = T)\nnice_table(stats.table, broom = \"t.test\")\n\nMethodAlternativeMean 1Mean 2M1 - M2tdfp95% CIWelch Two Sample t-testtwo.sided578,106.61516,234.7161,871.901.00136.73.321[-61042.85, 184786.65]\n\n\nVisualicemos la distribución de esta prueba y su zona de rechazo.\n\nggttest(t_results)\n\n\n\n\n\n\n\n\nAdemás, podemos calcular un intervalo de confianza que acompaña nuestra estimación. En este caso, vemos que el IC para la diferencia de medias oscila entre [131.275 - 155.245] y no contiene el cero, por lo que podemos rechazar la hipótesis nula.\n\n\n\nPruebas de hipótesis direccionales para la media\nSin embargo, también podemos plantear hipótesis respecto a que el valor de cierto parámetro para un grupo puede ser mayor o menor al de otro grupo. A esto se le conoce como hipótesis de una cola.\n\n\n\n\n\n\nPrueba de una cola\n\n\n\n\\[  H_{0}: \\mu_{0} ≥ \\mu_{1} ; \\mu_{0} ≤ \\mu_{1}\\]\n\\[  H_{A}: \\mu_{0} &gt; \\mu_{1} \\]\n\\[  H_{A}: \\mu_{0} &lt; \\mu_{1} \\]\n\n\nTomando como ejemplo el trabajo de la ganadora del Premio Nobel de Economía 2023 Claudia Goldin, comprobemos si los ingresos de las mujeres trabajadoras que no tienen hijos es mayor al ingreso de las mujeres trabajadoras que sí tienen hijos. Por tanto, usaremos prueba \\(t\\) para diferencia de medias.\nApliquemos nuestros 5 pasos.\n\nFormulamos nuestras hipótesis y dirección de la prueba:\n\nEn donde 0 = sin hijos y 1 = con hijos;\n\n\\(H_{0}\\): \\(\\mu_{0}\\) \\(≤\\) \\(\\mu_{1}\\)\n\\(H_{A}\\): \\(\\mu_{0}\\) \\(&gt;\\) \\(\\mu_{1}\\)\n\n\nCalcula el error estándar (SE) para diferencia de medias:\n\n\ngoldin_data &lt;- proc_casen %&gt;% \n  filter(ocupado == 1 & sexo == 2) %&gt;% \n  na.omit()# creamos subset con solo mujeres ocupadas\n\ndatos_t &lt;- goldin_data %&gt;% \n  group_by(hijo) %&gt;% \n  summarise(media = mean(ytrabajocor, na.rm = T),\n            ds = sd(ytrabajocor, na.rm = T),\n            n = n()) \n\ndatos_t\n\n# A tibble: 2 × 4\n   hijo   media      ds     n\n  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n1     0 586741. 441974.     9\n2     1 509889. 403640.   100\n\n\nEn este caso, obtenemos la diferencia de medias (\\(\\bar{x}_0 - \\bar{x}_1\\) ).\n\ndif_medias &lt;- 729850.1 - 655087.8\ndif_medias\n\n[1] 74762.3\n\n\nY luego su error estándar.\n\ns_0 &lt;- 659400.5 \ns_1 &lt;- 629290.5\n\nn_0 &lt;- 6807 \nn_1 &lt;- 19506    \n\nse_dif &lt;- sqrt((s_0^2)/n_0 + (s_1^2)/n_1)\nse_dif\n\n[1] 9174.886\n\n\n\nCalcula el valor estimado de la prueba (t para diferencia de medias):\n\n\nt_stat &lt;- dif_medias / se_dif\n\nt_stat\n\n[1] 8.148581\n\n\n\nEspecifica el valor crítico:\n\n\ndf &lt;- n_0 + n_1 - 2 # definimos grados de libertad\n\nt_critico &lt;- qt(p = 0.05, df, lower.tail = FALSE)\n\nt_critico\n\n[1] 1.644912\n\n\n\nContrasta el valor estimado con el crítico e interpreta los resultados:\n\n\nt_stat &gt; t_critico\n\n[1] TRUE\n\n\n\nRespuesta: La prueba T que evalúa la diferencia de medias entre el ingreso del trabajo y tener hijos en las mujeres ocupadas sugiere que el efecto es positivo y estadísticamente signficativo (diferencia = 74.762, t(11417.34) = 8.15, p &lt; .05). Por tanto, con un 95% de confianza, rechazamos la \\(H_{0}\\) ya que existe evidencia a favor de nuestra \\(H_{A}\\) respecto a que el ingreso de las mujeres sin hijos es mayor al ingreso de las mujeres con hijos.\n\n\nY el cálculo en R.\n\n\nt_results_goldin &lt;- t.test(goldin_data$ytrabajocor ~ goldin_data$hijo, \n                           alternative = \"greater\") # indicamos la direccion de la prueba\n\nstats.table &lt;- tidy(t_results_goldin, conf.int = T)\nnice_table(stats.table, broom = \"t.test\") \n\nMethodAlternativeMean 1Mean 2M1 - M2tdfp95% CIWelch Two Sample t-testgreater586,740.67509,889.1776,851.500.509.24.313[-202331.58, Inf]\n\n\nVisualicemos la distribución de esta prueba y su zona de rechazo.\n\nggttest(t_results_goldin)\n\n\n\n\n\n\n\n\nEn este caso, el IC del 95% es (59.670, Inf), donde “Inf” representa infinito. Esto significa que podemos estar 95% seguros de que la diferencia real entre las medias está por encima de 59.670. En otras palabras, con alta confianza, podemos afirmar que la media del “grupo 0” es significativamente mayor que la del “grupo 1”.\n\n\n7.3. Inferencia para proporciones\nCuando queremos realizar inferencia para variables categóricas, generlamente utilizamos pruebas que comparan proporciones de una variable de interés entre grupos.\nEl test de proporciones es una prueba estadística utilizada para determinar si hay una diferencia significativa entre dos proporciones (tasas) en dos grupos. Se basa en datos categóricos, en donde se cuentan los eventos “exitosos” o “positivos” en cada grupo de comparación, por sobre el total de eventos.\nEvaluamos si existen diferencias entre hombres y mujeres en la proporción de encontrarse en situación de pobreza multidimensional.\n\npobreza &lt;- proc_casen[1:300,] # creamos un subsample solo como ejemplo\n\nsjPlot::sjt.xtab(pobreza$pobreza_multi_5d, pobreza$sexo, show.col.prc = T)\n\n\n \n situación de pobrezamultidimensional conentorno y redes (5dimensiones)\n Sexo\n Total\n \n \n\n 1. Hombre\n 2. Mujer\n \n \n \nNo pobreza\n9167.9 %\n11571 %\n20669.6 % \n\n \n \nPobreza\n4332.1 %\n4729 %\n9030.4 % \n\n \n \nTotal\n134100 %\n162100 %\n296100 % \n\nχ2=0.199 · df=1 · φ=0.033 · p=0.656 \n\n \n\n\n\nFormulemos nuestras hipótesis:\n\n\\(H_0\\): \\(p_{hombres}\\) \\(=\\) \\(p_{mujeres}\\)\n\\(H_A\\): \\(p_{hombres}\\) \\(\\neq\\) \\(p_{mujeres}\\)\n\nAhora, creemos un objeto llamado xtab que contendrá la frecuencia de casos exitosos (“pobreza”) sobre el total de casos para cada grupo.\n\nxtab &lt;- as.table(rbind(c(43, 47),c(91, 115)))\n\ndimnames(xtab) &lt;- list(\n  pobreza = c(\"si\", \"no\"),\n  sexo = c(\"hombre\", \"mujer\")\n)\n\nLuego, aplicamos la función prop.test() para evaluar nuestra hipótesis.\n\nprop_results &lt;- prop.test(xtab, alternative = \"two.sided\")\n\nstats.table &lt;- tidy(prop_results, conf.int = T)\nnice_table(stats.table, broom = \"prop.test\")\n\nestimate1estimate2tpdfMethodAlternative95% CI0.480.440.20.65612-sample test for equality of proportions with continuity correctiontwo.sided[-0.10, 0.17]\n\n\n¿Y cómo reportamos esto?:\n\nRespuesta: Esta prueba que evalúa la diferencia de proporciones entre encontrarse en situación de pobreza según sexo sugiere que el efecto es positivo y estadísticamente no significativo (\\(X^2\\)= 0.19887, p = 0.6556). Por tanto, no se rechaza la \\(H_{0}\\) sobre igualdad de proporciones ya que no existe suficiente evidencia para concluir que las proporciones en los dos grupos son significativamente diferentes.\n\nAdemás de lo anterior, vemos que en este caso el IC de diferencia de proporciones sí contiene el cero, por lo que no podemos rechazar la hipótesis nula.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 04. Inferencia"
    ]
  },
  {
    "objectID": "assignment/04-practico.html#inferencia-en-correlación",
    "href": "assignment/04-practico.html#inferencia-en-correlación",
    "title": "Práctico 4. Inferencia",
    "section": "8. Inferencia en correlación",
    "text": "8. Inferencia en correlación\nEn el contexto de la inferencia, la correlación nos permite determinar si existe (o no) una asociación estadísticamente significativa entre dos variables. En ese sentido, la lógica del contraste de hipótesis usando correlación es:\n\n\n\n\n\n\nHipótesis en correlación\n\n\n\nContrastamos la hipótesis nula (o de trabajo) de no asociación entre variables: \\[  H_{0}: \\rho = 0 \\]\nEn relación a una hipótesis alternativa sobre la existencia una asociación significativa entre variables:\n\\[  H_{A}: \\rho \\neq 0 \\]\n\n\nTomemos por ejemplo la siguiente pregunta de investigación: ¿en qué medida el nivel educacional alcanzado por las personas se relaciona con sus ingresos en Chile en el 2022?\nFormulemos nuestra hipótesis:\n\n\\(H_{0}\\): \\(cor(educ,ingreso)\\) \\(=\\) \\(0\\)\n\\(H_{A}\\): \\(cor(educ,ingreso)\\) \\(\\neq\\) \\(0\\)\n\nObtengamos el coeficiente de correlación \\(r\\) de Pearson entre el nivel educacional alcanzado y los ingresos de las personas en Chile en 2022. Para esto usaremos solamente observaciones completas (listwise).\n\ncor_results &lt;- cor.test(proc_casen$educ, proc_casen$ytrabajocor, \n         method = \"pearson\", \n         use = \"complete.obs\")\n\nstats.table &lt;- tidy(cor_results)\nnice_table(stats.table, broom = \"cor\")\n\nestimatetpdfMethodAlternative95% CI0.38123.36&lt; .001***88,290Pearson's product-moment correlationtwo.sided[0.38, 0.39]\n\n\n\nRespuesta: El coeficiente de correlación de Pearson entre nivel educativo e ingresos es positivo, estadísticamente significativo (r = 0.38, p &lt; .001) y moderado de acuerdo con las recomendaciones de Cohen (1988). Por tanto, con un 95% de confianza se puede rechazar la \\(H_{0}\\) de no asociación entre variables, existiendo evidencia a favor de la \\(H_{A}\\) sobre una asociación significativa entre educación e ingresos.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 04. Inferencia"
    ]
  },
  {
    "objectID": "assignment/05-practico.html",
    "href": "assignment/05-practico.html",
    "title": "Práctico 05. Correlación",
    "section": "",
    "text": "El objetivo de esta guía práctica es aprender a calcular y graficar la correlación entre dos variables utilizando R. Particularmente, conocer maneras de reportar coeficientes de correlación y cómo interpretar sus tamaños de efecto en ciencias sociales. Además, nos introduciremos en el tratamiento de valores perdidos y otras medidas de correlación entre variables.\nEn detalle, aprenderemos:\n\nCómo calcular una correlación de Pearson y graficarla\nCómo reportar y presentar matrices de correlación.\nInterpretar el tamaño de efecto de una correlación.\nTratamiento de casos perdidos.\nQué es y cómo calcular la correlación de Spearman.\nQué es el coeficiente de determinación \\(R^2\\).",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 05: Correlación"
    ]
  },
  {
    "objectID": "assignment/05-practico.html#objetivo-de-la-práctica",
    "href": "assignment/05-practico.html#objetivo-de-la-práctica",
    "title": "Práctico 05. Correlación",
    "section": "",
    "text": "El objetivo de esta guía práctica es aprender a calcular y graficar la correlación entre dos variables utilizando R. Particularmente, conocer maneras de reportar coeficientes de correlación y cómo interpretar sus tamaños de efecto en ciencias sociales. Además, nos introduciremos en el tratamiento de valores perdidos y otras medidas de correlación entre variables.\nEn detalle, aprenderemos:\n\nCómo calcular una correlación de Pearson y graficarla\nCómo reportar y presentar matrices de correlación.\nInterpretar el tamaño de efecto de una correlación.\nTratamiento de casos perdidos.\nQué es y cómo calcular la correlación de Spearman.\nQué es el coeficiente de determinación \\(R^2\\).",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 05: Correlación"
    ]
  },
  {
    "objectID": "assignment/05-practico.html#correlación",
    "href": "assignment/05-practico.html#correlación",
    "title": "Práctico 05. Correlación",
    "section": "1. Correlación",
    "text": "1. Correlación\nLa correlación es una medida estadística que describe la asociación entre dos variables. Cuando existe correlación entre dos variables, el cambio en una de ellas tiende a estar asociado con un cambio en la otra variable.\nEn términos concretos, lo que observamos es cómo se comportan los valores de dos (o más) variables para cada observación, y si podemos suponer que ese comportamiento conjunto tiene algún patrón.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 05: Correlación"
    ]
  },
  {
    "objectID": "assignment/05-practico.html#correlación-de-pearson",
    "href": "assignment/05-practico.html#correlación-de-pearson",
    "title": "Práctico 05. Correlación",
    "section": "2. Correlación de Pearson",
    "text": "2. Correlación de Pearson\nLa correlación de Pearson (o coeficiente de correlación de pearson) es una medida estadística que cuantifica la relación lineal entre dos variables continuas. Esta medida va desde -1 hasta 1, donde:\n\nr=1: Correlación positiva perfecta. Cuando una variable aumenta, la otra también aumenta en proporción constante.\nr=−1: Correlación negativa perfecta. Cuando una variable aumenta, la otra disminuye en proporción constante.\nr=0: No hay correlación lineal entre las variables. No hay una relación lineal discernible entre los cambios en las variables.\n\nCuanto más cercano esté el valor de r a 1 o -1, más fuerte será la correlación. Cuanto más cercano esté a 0, más débil será la correlación.\n\nEn el siguiente enlace pueden visualizar la correlación para dos variables cambiando la fuerza y el sentido de esta, al mismo tiempo que les permite observar la varianza compartida entre ambas variables.\n\n2.1. Estimación de la correlación de Pearson\nUtilizaremos un set mínimo simulado de datos para 8 casos:\n\ncon 8 niveles de educación (ej: desde basica incompleta = 1, hasta postgrado = 8), y\n12 niveles de rangos de ingreso (ej: desde menos de 100.000 = 1 hasta más de 10.000.000 = 12)\n\n\nid &lt;-seq(1,8) # identificador de cada caso\neduc &lt;- c(2,3,4,4,5,7,8,8)\ning  &lt;- c(1,3,3,5,4,7,9,11)\ndata &lt;- data.frame(id,educ,ing)\n\n\nkableExtra::kbl(data, escape=F, full_width = F)  %&gt;%\n   kable_paper(\"hover\")\n\n\n\n\nid\neduc\ning\n\n\n\n\n1\n2\n1\n\n\n2\n3\n3\n\n\n3\n4\n3\n\n\n4\n4\n5\n\n\n5\n5\n4\n\n\n6\n7\n7\n\n\n7\n8\n9\n\n\n8\n8\n11\n\n\n\n\n\n\n\nLa correlación de Pearson se basa en la covarianza, que es una medida de asociación entre variables basada en la varianza de cada una de ellas:\n\\[\\begin{align*}\nCovarianza = cov(x,y) &= \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})} {n-1}\\\\\n\\\\\nCorrelación=r &= \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})} {(n-1)\\sigma_x \\sigma_y }\\\\ \\\\\n&= \\frac{\\sum(x-\\bar{x})(y-\\bar{y})}{\\sqrt{\\sum(x-\\bar{x})^{2} \\sum(y-\\bar{y})^{2}}}\n\\end{align*}\\]\nPara obtener el coeficiente de correlación directamente en R se utiliza la función cor() :\n\ncor(data$educ, data$ing)\n\n[1] 0.9512367\n\n\nVamos por paso:\n\n\\((x_i - \\bar{x})\\) es la diferencia de un valor de una variable respecto de su promedio. Por ejemplo, si x=educación, y un caso tiene educación 6 y el promedio de educación es 2, entonces el valor de \\((x_i - \\bar{x})=4\\)\n\\((y_i - \\bar{y})\\) : lo mismo pero para la otra varible, en el caso de nuestro ejemplo es ingreso.\n\\((x-\\bar{x})(y-\\bar{y})\\) es la multiplicación de los dos pasos anteriores para cada caso.\n\\(\\sum(x-\\bar{x})(y-\\bar{y})\\) es la suma de estos valores para el total de los casos\n\nEn nuestra base de datos de ejemplo simulamos columnas adicionales con esta información:\n\n\\((x_i - \\bar{x})\\) : dif_m_educ\n\\((y_i - \\bar{y})\\) : dif_m_ing\n\\((x-\\bar{x})(y-\\bar{y})\\) : dif_xy\n\nY además las diferencias de promedio de cada variable al cuadrado:\n\n\\((x_i - \\bar{x})²\\) : dif_m_educ2\n\\((y_i - \\bar{y})²\\) : dif_m_ing2\n\n\ndata$dif_m_educ &lt;- data$educ-mean(data$educ)\ndata$dif_m_ing &lt;- data$ing-mean(data$ing)\ndata$dif_xy &lt;-  data$dif_m_educ*data$dif_m_ing\ndata$dif_m_educ2 &lt;- (data$dif_m_educ)^2\ndata$dif_m_ing2 &lt;- (data$dif_m_ing)^2\n\nEn nuestra base de datos:\n\nkableExtra::kbl(data, digits = 3,full_width = F)  %&gt;%\n   kable_paper(\"hover\")\n\n\n\n\nid\neduc\ning\ndif_m_educ\ndif_m_ing\ndif_xy\ndif_m_educ2\ndif_m_ing2\n\n\n\n\n1\n2\n1\n-3.125\n-4.375\n13.672\n9.766\n19.141\n\n\n2\n3\n3\n-2.125\n-2.375\n5.047\n4.516\n5.641\n\n\n3\n4\n3\n-1.125\n-2.375\n2.672\n1.266\n5.641\n\n\n4\n4\n5\n-1.125\n-0.375\n0.422\n1.266\n0.141\n\n\n5\n5\n4\n-0.125\n-1.375\n0.172\n0.016\n1.891\n\n\n6\n7\n7\n1.875\n1.625\n3.047\n3.516\n2.641\n\n\n7\n8\n9\n2.875\n3.625\n10.422\n8.266\n13.141\n\n\n8\n8\n11\n2.875\n5.625\n16.172\n8.266\n31.641\n\n\n\n\n\n\n\nY obtenemos la suma de cada una de las tres últimas columnas, que es lo que se necesita para reemplazar en la fórmula de la correlación:\n\nsum(data$dif_xy); sum(data$dif_m_educ2);sum(data$dif_m_ing2)\n\n[1] 51.625\n\n\n[1] 36.875\n\n\n[1] 79.875\n\n\nReemplazando,\n\\[\\begin{align*}\nr &= \\frac{\\sum(x-\\bar{x})(y-\\bar{y})}{\\sqrt{\\sum(x-\\bar{x})^{2} \\sum(y-\\bar{y})^{2}}} \\\\ \\\\\n&= \\frac{51.625}{ \\sqrt{36.875*79.875}} \\\\ \\\\\n&= \\frac{51.625}{54.271} \\\\ \\\\\n&= 0.951\n\\end{align*}\\]\nY comprobamos con la función cor() de R, que nos entrega la correlación de Pearson:\n\ncor(data$educ, data$ing)\n\n[1] 0.9512367\n\n\nComo vimos en clases, el valor de la correlación va entre -1 y 1, donde el valor 0 significa que las dos variables no se encuentran correlacionadas, y el valor -1 y 1 significa que se encuentran completamente correlacionadas, positiva y negativamente, respectivamente.\nEn este caso, un valor de 0.95 es muy alto, y significa que las variables ingreso y educación, en nuestra base de datos simulada, se encuentran fuertemente correlacionadas.\n\n\n2.2. Diagrama de dispersión (nube de puntos o scatterplot)\nSiempre es recomendable acompañar el valor de la correlación con una exploración gráfica de la distribución bivariada de los datos. El gráfico o diagrama de dispersión es una buena herramienta, ya que muestra la forma, la dirección y la fuerza de la relación entre dos variables cuantitativas.\nEste tipo de gráfico lo podemos realizar usando la librería ggplot2.\n\npacman::p_load(ggplot2)\n\nplot1 &lt;- ggplot(data, \n                aes(x=educ, y=ing)) + \n  geom_point(colour = \"red\", size = 5) +\n  labs(x = \"Educación\", y = \"Ingresos\", title = \"Relación entre Educación e Ingresos\") +  \n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 20, face = \"bold\", hjust = 0.5), \n    axis.title.x = element_text(size = 16),    \n    axis.title.y = element_text(size = 16),     \n    axis.text.x = element_text(size = 14),      \n    axis.text.y = element_text(size = 14)       \n  )\n\nplot1\n\n\n\n\n\n\n\n\nEn el gráfico podemos ver como se crea una nube de puntos en las intersecciones de los valores para ambas variables de cada caso.\n\n\n2.3. El cuarteto de Anscombe\nAhora, revisaremos un muy buen ejemplo de la importancia de la exploración gráfica de los datos mediante un ejemplo de Anscombe (1973), que permite visualizar las limitaciones del coeficiente de correlación.\nPrimero, crearemos la base de datos:\n\nanscombe &lt;- data.frame(\n  x1 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),\n  y1 = c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68),\n  x2 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),\n  y2 = c(9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74),\n  x3 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),\n  y3 = c(7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73),\n  x4 = c(8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8),\n  y4 = c(6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89)\n)\n\nCalculamos la correlación para cada par de datos:\n\ncor(anscombe$x1, anscombe$y1); cor(anscombe$x2, anscombe$y2); cor(anscombe$x3, anscombe$y3); cor(anscombe$x4, anscombe$y4)\n\n[1] 0.8164205\n\n\n[1] 0.8162365\n\n\n[1] 0.8162867\n\n\n[1] 0.8165214\n\n\nPodemos observar que los valores de las correlaciones son equivalentes, por lo tanto podríamos pensar que todos los pares de columnas se encuentran correlacionados de manera similar.\nPero, ¿será suficiente con esa información? Pasemos a revisar los gráficos de dispersión de cada par de variables.\n\nggplot(anscombe, aes(x = x1, y = y1)) +\n  geom_point(colour = \"red\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso I\")\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x2, y = y2)) +\n  geom_point(colour = \"green\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso II\")\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x3, y = y3)) +\n  geom_point(colour = \"yellow\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso III\")\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x4, y = y4)) +\n  geom_point(colour = \"orange\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso IV\")\n\n\n\n\n\n\n\n\nComo vemos, con distintas distribuciones las correlaciones pueden ser las mismas, principalmente porque Pearson es una medida que solo captura relaciones lineales (rectas), además de verse influido fuertemente por valores extremos. Por lo mismo, es relevante siempre una buena visualización de la distribución bivariada de los datos como complemento al cálculo del coeficiente de correlación.\n\n\n2.4. Aplicación práctica\nEn esta práctica trabajaremos con un subconjunto de datos previamente procesados del Estudio Longitudinal Social de Chile (ELSOC) del año 2016, elaborado por COES. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  ELSOC 2016. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos ELSOC 2016.\n\nPreparación datos\nComencemos por preparar nuestros datos. Iniciamos cargando las librerías necesarias.\n\npacman::p_load(tidyverse, # Manipulacion datos\n               sjmisc, # Descriptivos\n               sjPlot, # Tablas\n               kableExtra, #Tablas\n               GGally, # Correlaciones\n               corrplot) # Correlaciones\n\noptions(scipen = 999) # para desactivar notacion cientifica\nrm(list = ls()) # para limpiar el entorno de trabajo\n\nCargamos los datos directamente desde internet (por esta vez).\n\nload(url(\"https://multivariada.netlify.app/assignment/data/proc/ELSOC_ess_merit2016.RData\")) \n\nA continuación, exploramos la base de datos proc_elsoc.\n\nnames(proc_elsoc) # Nombre de columnas\n\n[1] \"mesfuerzo\" \"mtalento\"  \"ess\"       \"edcine\"    \"sexo\"      \"edad\"     \n[7] \"pmerit\"   \n\ndim(proc_elsoc) # Dimensiones\n\n[1] 2927    7\n\n\nContamos con 7 variables (columnas) y 2927 observaciones (filas).\nAhora, profundicemos un poco más y observemos algunos estadísticos descriptivos de resumen de nuestra base de datos. Utilizaremos la función descr del paquete sjmisc.\n\nsjmisc::descr(proc_elsoc,\n      show = c(\"label\",\"range\", \"mean\", \"sd\", \"NA.prc\", \"n\")) %&gt;%\n      kable(.,\"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar\nlabel\nn\nNA.prc\nmean\nsd\nrange\n\n\n\n\n4\nmesfuerzo\nRecompensa: esfuerzo\n2909\n0.6149641\n2.5727054\n1.0466874\n4 (1-5)\n\n\n5\nmtalento\nRecompensa: talento\n2907\n0.6832935\n2.7389061\n1.0596182\n4 (1-5)\n\n\n3\ness\nEstatus Social Subjetivo\n2915\n0.4099761\n4.3300172\n1.5666965\n10 (0-10)\n\n\n2\nedcine\nEducación\n2925\n0.0683293\n3.1839316\n1.2066058\n4 (1-5)\n\n\n7\nsexo\nSexo\n2927\n0.0000000\n0.6026648\n0.4894300\n1 (0-1)\n\n\n1\nedad\nEdad\n2927\n0.0000000\n46.0908780\n15.2867983\n70 (18-88)\n\n\n6\npmerit\nMeritocracia promedio\n2898\n0.9907755\n2.6538992\n0.9694792\n4 (1-5)\n\n\n\n\n\nTenemos algunos valores o casos perdidos en ciertas variables. ¿Cómo lidiar con los casos perdidos?\n\n\nTratamiento de casos perdidos\nTrabajar con datos a menudo implica enfrentar valores perdidos (NA), lo que puede ser un gran desafío. Estos valores indican la ausencia de un valor en una base de datos. Los valores perdidos pueden originarse por diversas razones, como el sesgo de no respuesta en encuestas, errores en la entrada de datos o simplemente la falta de información para ciertas variables.\n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\nNA\n4\n1\nHola\n\n\n7\n1\n4\nNo soy un NA\n\n\n8\nNA\n2\nNA\n\n\n9\nNA\n9\nAmo R\n\n\n3\n3\n6\nNA\n\n\n\n\n\n\n\n\nLa presencia de valores perdidos puede tener un impacto considerable en la precisión y confiabilidad de los análisis estadísticos, lo que a su vez puede conducir a resultados sesgados y conclusiones incorrectas.\nExisten varias formas de tratar valores perdidos, que van desde enfoques simples hasta métodos más complejos, como la imputación. En esta ocasión, nos centraremos en las dos estrategias más comunes:\n\ntrabajar exclusivamente con casos completos (listwise) o\nretener los casos con valores perdidos, pero excluyéndolos al calcular estadísticas (pairwise).\n\n\n\na) Analísis con casos completos: listwise deletion\nEste enfoque es uno de los más conocidos: implica remover completamente las observaciones que tienen valores perdidos en cualquier variable de interés. En otras palabras, si una fila/caso en un conjunto de datos tiene al menos un valor faltante en alguna de las variables que estás considerando, se eliminará por completo.\nEn R, esto podemos hacerlo con la función na.omit. Para hacer esto, sigamos estos pasos:\n\nrespaldar la base de datos original en el espacio de trabajo (por si queremos en adelante realizar algún análisis referido a casos perdidos)\ncontamos el número de casos con el comando dim.\ncontamos cuántos y dónde tenemos casos perdidos.\nborramos los casos perdidos con na.omit.\ncontamos nuevamente con dim para asegurarnos que se borraron.\n\n\nproc_elsoc_original &lt;- proc_elsoc\ndim(proc_elsoc)\n\n[1] 2927    7\n\n\n\nsum(is.na(proc_elsoc))\n\n[1] 81\n\n\n\ncolSums(is.na(proc_elsoc))\n\nmesfuerzo  mtalento       ess    edcine      sexo      edad    pmerit \n       18        20        12         2         0         0        29 \n\n\n\nproc_elsoc &lt;- na.omit(proc_elsoc)\ndim(proc_elsoc)\n\n[1] 2887    7\n\n\nAhora nos quedamos con 2887 observaciones sin casos perdidos.\nAunque simple de implementar, con este enfoque podemos perder información importante, especialmente si los valores perdidos no se distribuyen aleatoriamente.\n\n\n\n\n\n\nNota\n\n\n\nSiempre hay que intentar rescatar la mayor cantidad de casos posibles. Por lo tanto, si un listwise genera más de un 10% de casos perdidos se debe detectar qué variables esta produciendo esta pérdida e intentar recuperar datos. Puedes revisar un ejemplo aquí.\n\n\n\n\nb) Retener pero excluir: pairwise deletion\nA diferencia del anterior, este es un enfoque en el que las observaciones se utilizan para el análisis siempre que tengan datos disponibles para las variables específicas que se están analizando. En lugar de eliminar toda una fila si falta un valor, se eliminan solo los valores faltantes en las variables que se están analizando en ese momento.\nPara hacer esto en R debemos siempre verificar e indicar en nuestro código si queremos (o no) remover los NA para realizar los análisis.\n\nmean(proc_elsoc_original$pmerit); mean(proc_elsoc$edad); mean(proc_elsoc$ess)\n\n[1] NA\n\n\n[1] 45.98337\n\n\n[1] 4.333564\n\nmean(proc_elsoc_original$pmerit, na.rm = TRUE); mean(proc_elsoc$edad, na.rm = TRUE); mean(proc_elsoc$ess, na.rm = TRUE)\n\n[1] 2.653899\n\n\n[1] 45.98337\n\n\n[1] 4.333564\n\n\nCon el primer código no obtuvimos información sustantiva en ciertas variables, pero con el segundo sí al remover los NA solo de dicha variable para un cálculo determinado.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 05: Correlación"
    ]
  },
  {
    "objectID": "assignment/05-practico.html#matrices-de-correlación",
    "href": "assignment/05-practico.html#matrices-de-correlación",
    "title": "Práctico 05. Correlación",
    "section": "3. Matrices de correlación",
    "text": "3. Matrices de correlación\nLa correlación es una estimación de asociación de dos variables. Sin embargo, en los análisis de bases de datos usualmente se exploran asociaciones entre múltiples pares de variables, lo que genera una matriz de correlación. En una matriz, las variables se presentan en las filas y las columnas, y en las celdas donde se cruzan los pares de variables se muestra su coeficiente de correlación.\nEn su forma simple en R se aplica la función cor a la base de datos, y la guardamos en un objeto que le damos el nombre M para futuras operaciones:\n\nM &lt;- cor(proc_elsoc_original, use = \"complete.obs\") \nM\n\n             mesfuerzo    mtalento          ess      edcine        sexo\nmesfuerzo  1.000000000  0.69768811 -0.004312135 -0.12167659 -0.04480502\nmtalento   0.697688106  1.00000000  0.018447696 -0.10582754 -0.03759340\ness       -0.004312135  0.01844770  1.000000000  0.28959248 -0.03745546\nedcine    -0.121676591 -0.10582754  0.289592479  1.00000000 -0.08682644\nsexo      -0.044805024 -0.03759340 -0.037455462 -0.08682644  1.00000000\nedad       0.096495547  0.07383771 -0.066031873 -0.37660283  0.06121699\npmerit     0.920404032  0.92224547  0.007740598 -0.12341680 -0.04469515\n                 edad       pmerit\nmesfuerzo  0.09649555  0.920404032\nmtalento   0.07383771  0.922245465\ness       -0.06603187  0.007740598\nedcine    -0.37660283 -0.123416804\nsexo       0.06121699 -0.044695146\nedad       1.00000000  0.092369792\npmerit     0.09236979  1.000000000\n\n\nEste es el reporte simple, pero no muy amigable a la vista. Para una versión más reportable, utilizamos la funcion tab_corr.\n\nsjPlot::tab_corr(proc_elsoc_original, \n                 triangle = \"lower\")\n\n\n\n\n \nRecompensa: esfuerzo\nRecompensa: talento\nEstatus Social Subjetivo\nEducación\nSexo\nEdad\nMeritocracia promedio\n\n\nRecompensa: esfuerzo\n \n \n \n \n \n \n \n\n\nRecompensa: talento\n0.698***\n \n \n \n \n \n \n\n\nEstatus Social Subjetivo\n-0.004\n0.018\n \n \n \n \n \n\n\nEducación\n-0.122***\n-0.106***\n0.290***\n \n \n \n \n\n\nSexo\n-0.045*\n-0.038*\n-0.037*\n-0.087***\n \n \n \n\n\nEdad\n0.096***\n0.074***\n-0.066***\n-0.377***\n0.061**\n \n \n\n\nMeritocracia promedio\n0.920***\n0.922***\n0.008\n-0.123***\n-0.045*\n0.092***\n \n\n\nComputed correlation used pearson-method with listwise-deletion.\n\n\n\n\n\nLa distinción entre listwise y pairwise es relevante al momento de estimar matricies de correlación, donde esta decisión debe estar claramente explicitada y fundamentada. En ejemplo de tabla anterior usamos listwise que es el argumento por defecto (y nos lo indica al final de la tabla).\nVeamos como hacerlo con pairwise:\n\nsjPlot::tab_corr(proc_elsoc_original, \n                 na.deletion = \"pairwise\", # espeficicamos tratamiento NA\n                 triangle = \"lower\")\n\n\n\n\n \nRecompensa: esfuerzo\nRecompensa: talento\nEstatus Social Subjetivo\nEducación\nSexo\nEdad\nMeritocracia promedio\n\n\nRecompensa: esfuerzo\n \n \n \n \n \n \n \n\n\nRecompensa: talento\n0.696***\n \n \n \n \n \n \n\n\nEstatus Social Subjetivo\n-0.006\n0.016\n \n \n \n \n \n\n\nEducación\n-0.124***\n-0.109***\n0.287***\n \n \n \n \n\n\nSexo\n-0.044*\n-0.036\n-0.035\n-0.090***\n \n \n \n\n\nEdad\n0.099***\n0.075***\n-0.068***\n-0.379***\n0.062***\n \n \n\n\nMeritocracia promedio\n0.920***\n0.922***\n0.008\n-0.125***\n-0.043*\n0.092***\n \n\n\nComputed correlation used pearson-method with pairwise-deletion.\n\n\n\n\n\nCon esta mejor visualización, algunas observaciones sobre la matriz de correlaciones:\n\nEn esta matriz las variables están representadas en las filas y en las columnas.\nCada coeficiente expresa la correlación de una variable con otra. Por ejemplo, la correlación entre la variable de mesfuerzo y mtalento es 0.69.\nLa información de cada coeficiente se repite sobre y bajo la diagonal, ya que es el mismo par de variables pero en el orden alterno. Por convención en general se omiten las correlaciones redundantes sobre la diagonal, por eso aparece en blanco.\nEn la diagonal corresponde que todos los coeficientes sean 1, ya que la correlación de una variable consigo misma es perfectamente positiva.\n\nOtra manera de presentar matrices de correlación es mediante gráficos. Veamos un ejemplo con la función corrplot.mixed de la librería corrplot sobre nuestra matriz M ya creada.\n\ncorrplot.mixed(M)\n\n\n\n\n\n\n\n\nEste gráfico/matriz representa el grado de asociación entre variables mediante el tamaño de los círculos e intensidad de colores, y el signo de la asociación se representa con una gradiente de colores que va del azul (positivo) al rojo (negativo).\nOtra manera de graficar la matriz es con la función ggpairs del paquete GGally, que nos entrega no solo el valor del coeficiente y su significancia (***), si no que también un scatter del cruce entre variables.\n\nggpairs(proc_elsoc) \n\n\n\n\n\n\n\n\nFinalmente, también se puede representar la correlación entre dos variables en un gráfico de nube de puntos o scatterplot.\n\nsjPlot::plot_scatter(proc_elsoc, edad, ess)\n\n\n\n\n\n\n\n\nDonde:\n\ncada punto representa un caso\nla forma de la nube indica si la asociación es positiva, negativa o neutra:\n\n\nEn el caso de nuestra nube de puntos entre edad y estatus social subjetivo, observamos que no hay asociación (lo que ya era indicado por su correlación de -0.07 observada en la matriz de correlaciones).",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 05: Correlación"
    ]
  },
  {
    "objectID": "assignment/05-practico.html#tamaños-de-efecto",
    "href": "assignment/05-practico.html#tamaños-de-efecto",
    "title": "Práctico 05. Correlación",
    "section": "4. Tamaños de efecto",
    "text": "4. Tamaños de efecto\n¿Y cómo puedo saber si el valor de la correlación es alto, medio o bajo? Si bien la correlación no nos indica causalidad, si nos permite conocer la dirección y fuerza de asociación entre dos variables. Un estándar para determinar qué tan fuerte es dicha asociación en las ciencias sociales es el propuesto por Cohen (1998).\n\n\n\n\nr\n\n\nSignificado aproximado (Cohen 1988)\n\n\n\n\n&lt; ±0.1 \n\n\nMuy pequeño\n\n\n\n\n±0.1–0.3\n\n\nPequeño\n\n\n\n\n±0.3–0.5\n\n\nModerado\n\n\n\n\n&gt;±0.5\n\n\nGrande\n\n\n\nCon estos criterios podemos interpretar de mejor manera nuestros resultados de correlación. Como se observa, mientras más alto (sea en + o -) el coeficiente, más juntos estarán los datos (puntos), mostrando un patrón.\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretación\nRecordemos nuestra matriz del comienzo:\nTenemos que la correlación entre la variable de estatus social subjetivo y años de educación es 0.3. ¿Cómo interpreto esto?\nUna manera recomendable es la siguiente:\nEl coeficiente de correlación de Pearson entre estatus social subjetivo y años de educación es positivo y moderado (r = 0.3) según Cohen (1988).",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 05: Correlación"
    ]
  },
  {
    "objectID": "assignment/05-practico.html#correlación-spearman",
    "href": "assignment/05-practico.html#correlación-spearman",
    "title": "Práctico 05. Correlación",
    "section": "5. Correlación Spearman",
    "text": "5. Correlación Spearman\nCuando queremos conocer la asociación entre variables que son ordinales y/o cuando nuestras variables no cumplen con los supuestos de distribución normal, podemos utilizar la correlación de Spearman.\n\nEl coeficiente de correlación de Spearman es una medida estadística que evalúa la relación entre variables al considerar no solo la relación lineal entre ellas, sino también su relación de orden.\nEmplea rangos en lugar de valores numéricos para evaluar la relación.\nSus valores están entre -1 y 1.\nEs alta cuando las observaciones tienen un ranking similar.\n\nEn R calcularlo es sencillo, pero debemos tener en cuenta que las variables que relacionemos tengan un orden de rango similar: por ejemplo, que el valor más bajo sea el rango más bajo y que el valor más alto sea el rango más alto.\nTomemos por ejemplo las variables mesfuerzo y mtalento.\n\nsjmisc::frq(proc_elsoc$mesfuerzo)\n\nx &lt;numeric&gt; \n# total N=2887 valid N=2887 mean=2.57 sd=1.05\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\n    1 |  355 | 12.30 |   12.30 |  12.30\n    2 | 1324 | 45.86 |   45.86 |  58.16\n    3 |  489 | 16.94 |   16.94 |  75.10\n    4 |  641 | 22.20 |   22.20 |  97.30\n    5 |   78 |  2.70 |    2.70 | 100.00\n &lt;NA&gt; |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nsjmisc::frq(proc_elsoc$mtalento)\n\nx &lt;numeric&gt; \n# total N=2887 valid N=2887 mean=2.74 sd=1.06\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\n    1 |  288 |  9.98 |    9.98 |   9.98\n    2 | 1155 | 40.01 |   40.01 |  49.98\n    3 |  557 | 19.29 |   19.29 |  69.28\n    4 |  805 | 27.88 |   27.88 |  97.16\n    5 |   82 |  2.84 |    2.84 | 100.00\n &lt;NA&gt; |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\nAhora, calculemos el coeficiente de correlación de Spearman con cor.test.\n\ncor.test(proc_elsoc$mesfuerzo, proc_elsoc$mtalento, method = \"spearman\") #especificamos metodo spearman\n\n\n    Spearman's rank correlation rho\n\ndata:  proc_elsoc$mesfuerzo and proc_elsoc$mtalento\nS = 1200288294, p-value &lt; 0.00000000000000022\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n     rho \n0.700707 \n\n\nAhora conocemos el valor del coeficiente mediante al argumento rho, que es igual a 0.7, siendo positivo y grande según los criterios de Cohen (1988).",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 05: Correlación"
    ]
  },
  {
    "objectID": "assignment/05-practico.html#coeficiente-de-determinación",
    "href": "assignment/05-practico.html#coeficiente-de-determinación",
    "title": "Práctico 05. Correlación",
    "section": "6. Coeficiente de determinación",
    "text": "6. Coeficiente de determinación\nEl coeficiente de determinación \\(R^2\\) es una medida estadística que indica la proporción de la varianza total de una variable que es explicada por otra(s) variable(s). En pocas palabras,\n\nse utiliza para evaluar cuánta de la variabilidad de una variable se debe a otra variable.\nsus valores van desde 0 a 1, en donde 0 indica que ambas variables comparten el 0% de su varianza, y 1 que comparten el 100% de su varianza.\n\nEn el contexto de la correlación entre solo dos variables, el \\(R^2\\) es igual a elevar al cuadrado el coeficiente de correlación = (r)^2. Esto nos permite conocer qué tanto la variabilidad de una variable X estaría asociado a la variabilidad de otra variable Y.\nEn nuestro ejemplo anterior entre estatus social subjetivo y años de educación, teníamos que su coeficiente de correlación era r = 0.3.\n\ncoef_r &lt;- M[4,3] # seleccionamos el coef de nuestra matriz\n\ncoef_r\n\n[1] 0.2895925\n\n\nCalculemos el \\(R^2\\) de esta asociación.\n\ncoef_r^2\n\n[1] 0.0838638\n\n\nCon esto, podemos decir que el 9% de la variabilidad del estatus social subjetivo es compartido con la variabilidad en los años de educación.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 05: Correlación"
    ]
  },
  {
    "objectID": "resource/03-resource.html",
    "href": "resource/03-resource.html",
    "title": "Práctico 3. Inferencia Estadística y Curva Normal",
    "section": "",
    "text": "El objetivo de esta guía práctica es introducirnos en la inferencia estadística, revisando los conceptos y aplicaciones de la curva normal y las probabilidades bajo esta con puntajes Z.\nEn detalle, aprenderemos:\n\nQué es la inferencia estadística.\nQué es una distribución muestral.\nQué es el error estándar.\nQué es la distribución normal y cómo interpretarla.\nCómo calcular probabilidades asociadas con valores Z en R.\nQué son y cómo calcular intervalos de conafianza."
  },
  {
    "objectID": "resource/03-resource.html#qué-es-una-distribución",
    "href": "resource/03-resource.html#qué-es-una-distribución",
    "title": "Práctico 3. Inferencia Estadística y Curva Normal",
    "section": "2.1. ¿Qué es una distribución?",
    "text": "2.1. ¿Qué es una distribución?\nRecordemos que por distribución nos referimos al conjunto de todos los valores posibles de una variable y las frecuencias (o probabilidades) con las que se producen.\nExisten distribuciones empíricas y distribuciones teóricas, en donde:\n\nlas primeras reflejan la distribución de los valores que asume la variable en un grupo concreto a partir de una observación.\nlas segundas son una función matématica que expresan la distribución de un conjunto de números mediante su probabilidad de ocurencia.\n\nUna de las distribuciones teóricas más conocidas es la distribución normal estándar."
  },
  {
    "objectID": "resource/03-resource.html#distribución-muestral-1",
    "href": "resource/03-resource.html#distribución-muestral-1",
    "title": "Práctico 3. Inferencia Estadística y Curva Normal",
    "section": "2.2. Distribución muestral",
    "text": "2.2. Distribución muestral\n\n\n\n\n\n\nNota\n\n\n\nVariabilidad muestral: el valor de un estadístico varía en un muestreo aleatorio repetido.\n\n\nLa distribución muestral es la distribución de las estimaciones, o estadísticos como la media o proporción, tomadas de múltiples muestras aleatorias de una población. Permite comprender cómo varían las estimaciones de una muestra a otra.\nEjemplo 1: Imaginemos que tenemos una población de niñ_s de 0 a 9 años, y tomamos múltiples muestras de 6 individu_s (n=6). Cada una de las muestras tendrá un promedio (estadístico muestral, en este caso \\(\\bar{x}\\)) diferente, que no necesariamente coincidirá con el promedio de la población (parámetro, en este caso \\(\\mu_{x}\\))\n\nEjemplo 2: Si usamos valores simulados, podemos ver que todas las medias obtenidas en cada muesta son distintas.\n\nset.seed(100)  # Establecer semilla \nmuestras &lt;- replicate(100, mean(rnorm(30, mean = 50, sd = 10))) # 100 muestras de tamaño 30\nmuestras\n\n  [1] 50.28864 50.92574 49.56125 49.25099 48.40735 51.67506 49.28325 48.90829\n  [9] 50.64635 51.94797 49.68514 49.00100 49.87287 47.58167 50.91109 47.61967\n [17] 47.94270 52.20491 51.38777 50.76559 49.02265 49.16394 50.59675 51.40631\n [25] 50.28247 51.94561 50.39929 51.95632 55.25584 51.26112 49.02810 46.46643\n [33] 52.21139 49.48146 50.43067 52.84081 47.97451 46.55278 49.87576 50.88025\n [41] 50.78748 49.42165 50.46664 51.06194 49.94867 48.39219 49.79581 49.82214\n [49] 49.93791 49.16883 52.24256 51.46923 46.51443 50.23611 49.87410 50.87291\n [57] 52.32911 46.21546 47.26934 51.29019 50.49509 49.63433 53.25368 50.81717\n [65] 49.45387 49.61571 50.33774 47.02089 47.94071 50.16296 51.12409 50.68963\n [73] 50.32398 52.22186 49.95585 50.74844 48.08507 52.90382 51.43478 46.74822\n [81] 49.21148 51.83738 49.66936 49.32308 50.31842 46.47797 50.64338 50.01220\n [89] 52.26135 47.49504 49.93140 53.04953 49.59253 48.83580 49.57802 49.23299\n [97] 50.32517 50.82952 48.92960 49.47553\n\n\nSi conocemos la desviación estándar de los promedios, podedmos construir un intervalo de probabilidad, basado en la curva normal.\n\n\n\n\n\n\nNota\n\n\n\nUna característica importante es que se asume que las muestras tomadas de la población son aleatorias y representativas, lo que es esencial para que la distribución muestral refleje adecuadamente la variabilidad de las estimaciones.\n\n\nLa importancia de la distribución muestral es que nos permitirá estimar parámetros poblacionales a partir de estadísticos muestrales, construir intervalos de confianza, y realizar pruebas de hipótesis."
  },
  {
    "objectID": "resource/03-resource.html#ejercicio-de-aplicación",
    "href": "resource/03-resource.html#ejercicio-de-aplicación",
    "title": "Práctico 3. Inferencia Estadística y Curva Normal",
    "section": "Ejercicio de aplicación",
    "text": "Ejercicio de aplicación\nAhora que hemos generado distribuciones normales, echemos un vistazo a algunos datos y compárelos con la distribución normal. Utilizaremos un conjunto de datos desde internet, con mediciones de 247 hombres y 260 mujeres, la mayoría de los cuales eran considerados adultos jóvenes sanos. Puede encontrar una clave para los nombres de las variables aquí, pero nos centraremos en solo tres columnas: peso en kg (wgt), altura en cm (hgt) y sexo (1 = hombre; 0 = mujer).\n\nload(url(\"http://www.openintro.org/stat/data/bdims.RData\"))\n\nSeparemos estos datos en dos conjuntos, uno de hombres y otro de mujeres con la función subset\n\nmdims &lt;- subset(bdims, sex == 1)\nfdims &lt;- subset(bdims, sex == 0)\n\n\nEjercicio 1\nHaz un histograma de la altura de los hombres y un histograma de la altura de las mujeres. ¿Cómo compararía los diversos aspectos de las dos distribuciones?\n\nhist(mdims$hgt, xlim = c(150,200))\n\n\n\n\n\n\n\nhist(fdims$hgt, xlim = c(140,190))\n\n\n\n\n\n\n\n\n\n\nEjercicio 2\nscale es una función en R y se puede aplicar a cualquier vector numérico (lista de números en R). Genere los dos histogramas siguientes, esta vez graficando scale() de las estaturas y determine cómo la versión escalada de las alturas corresponde a las alturas originales. ¿Qué calcula la escala para cada punto?\n\nhist(scale(mdims$hgt))\n\n\n\n\n\n\n\nhist(scale(fdims$hgt))\n\n\n\n\n\n\n\n\n\n\nEjercicio 3\nNos gustaría comparar la distribución de estaturas en este conjunto de datos con la distribución normal. Para cada uno de los histogramas de alturas (sin escalar), trace una curva normal en la parte superior del histograma.\n\nCalcule la media y la desviación estándar para las alturas femeninas y guárdelas como variables, fhgtmean y fhgtsd, respectivamente.\nDetermine la lista de valores de x (el rango del eje X) y guarde este vector. Puede hacer fácilmente una lista de números usando la función seq() como lo hemos hecho antes, o teniendo el límite inferior:límite superior. Por ejemplo, para generar un vector (lista de números) del 1 al 10 y guardarlo como one_ten, usaría one_ten &lt;- 1:10.\nComo arriba, use dnorm() para tomar la lista de valores de x y encontrar el valor de y correspondiente si fuera una distribución normal perfecta. Guarde este vector como la variable y.\nVuelva a trazar su histograma y luego, en la siguiente línea, use lines(x = x, y = y, col = \"blue\") para dibujar una distribución normal encima.\n\n\nfhgtmean &lt;- mean(fdims$hgt)\nfhgtsd   &lt;- sd(fdims$hgt)\nhist(fdims$hgt, probability = TRUE, ylim = c(0, .07))\nx &lt;- 140:190\ny &lt;- dnorm(x = x, mean = fhgtmean, sd = fhgtsd)\nlines(x = x, y = y, col = \"blue\")\n\n\n\n\n\n\n\n\nSegún este gráfico, ¿parece que los datos siguen una distribución casi normal? Haz lo mismo con las estaturas masculinas.\n\nRespuesta: En general, sí, consideraría que estos valores siguen una distribución casi normal ya que el histograma se ajusta bastante bien a la curva.\n\nObserve que la forma del histograma es una forma de determinar si los datos parecen estar distribuidos casi normalmente, pero puede resultar frustrante decidir qué tan cerca está el histograma de la curva. Un enfoque alternativo implica construir una gráfica de probabilidad normal, también llamada gráfica Q-Q por “quantil-quantil”. Ejecute ambas líneas juntas.\n\nqqnorm(fdims$hgt)\nqqline(fdims$hgt)\n\n\n\n\n\n\n\n\nUn QQ plot nos muestra en el eje x los cuantiles teóricos de la distribución en términos de desviaciones estandar, y en el eje y los valores de la variable. La distribución de los puntos en una línea recta es una indicación de que los datos se distribuyen normalmente.\nVeamos otro ejemplo de otra variable de la base de datos:\n\nhist(fdims$che.de)\n\n\n\n\n\n\n\nqqnorm(fdims$che.de)\nqqline(fdims$che.de)\n\n\n\n\n\n\n\n\nUna vez que decidimos que una variable se distribuyte de forma normal, podemos responder todo tipo de preguntas sobre esa variable relacionadas con la probabilidad. Tomemos, por ejemplo, la pregunta: “¿Cuál es la probabilidad de que una mujer adulta joven elegida al azar mida más 182 cm?”\nSi suponemos que las alturas de las mujeres se distribuyen normalmente (una aproximación muy cercana también está bien), podemos encontrar esta probabilidad calculando una puntuación Z y consultando una tabla Z (también llamada tabla de probabilidad normal).\nEn R, esto se hace en un solo paso con la función pnorm (como hicimos anteriormente para la distribución normal estándar).\n\npnorm(q = 182, mean = fhgtmean, sd = fhgtsd)\n\n[1] 0.9955656\n\n\nObtenemos la proporción de mujeres que está bajo esa estatura, es decir 99,6%. Si queremos saber la proporción de mujeres que está sobre esa estatura:\n\n1 - pnorm(q = 182, mean = fhgtmean, sd = fhgtsd)\n\n[1] 0.004434387\n\n\nEn este caso, el 0,4% de las mujeres se encontraría sobre esa estatura.\nPodemos también hacer la operación inversa, es decir, a qué valor (estatura) corresponde un porcentaje o probabilidad basada en una distribución normal. Para ello utilizamos la función qnorm. Por ejemplo, para la probabilidad que calculamos más arriba para una altura de 182cm en las mujeres:\n\nqnorm(.9955656, fhgtmean, fhgtsd)\n\n[1] 182"
  },
  {
    "objectID": "resource/03-resource.html#cálculo-de-intervalos-de-confianza",
    "href": "resource/03-resource.html#cálculo-de-intervalos-de-confianza",
    "title": "Práctico 3. Inferencia Estadística y Curva Normal",
    "section": "Cálculo de intervalos de confianza",
    "text": "Cálculo de intervalos de confianza\nAhora ¡Manos a la obra!\nCalculemos intervalos de confianza. Primero, carguemos las librerías necesarias:\n\nlibrary(pacman)\n\nWarning: package 'pacman' was built under R version 4.3.3\n\npacman::p_load(tidyverse, # colección de paquetes para manipulación de datos\n               car,       # para recodificar\n               psych,     # para analizar datos\n               sjmisc,    # para analizar datos\n               srvyr,     # para estimación de IC y ponderadores\n               Publish)   # para IC\n\nInstalling package into 'C:/Users/kevin/AppData/Local/R/win-library/4.3'\n(as 'lib' is unspecified)\n\n\nalso installing the dependencies 'TH.data', 'multcomp'\n\n\nWarning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3:\n  no fue posible abrir la URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3/PACKAGES'\n\n\npackage 'TH.data' successfully unpacked and MD5 sums checked\npackage 'multcomp' successfully unpacked and MD5 sums checked\npackage 'Publish' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\kevin\\AppData\\Local\\Temp\\Rtmp0aXAJf\\downloaded_packages\n\n\n\nPublish installed\n\n\nWarning: package 'Publish' was built under R version 4.3.3\n\noptions(scipen = 999) # para desactivar notacion cientifica\nrm(list = ls()) # para limpiar el entorno de trabajo\n\ny también carguemos la base de datos que utilizaremos, que corresponde a un subset de la Encuesta Suplementaria de ingresos ESI para ocupados:\n\nload(url(\"https://github.com/cursos-metodos-facso/datos-ejemplos/raw/main/esi-2021-ocupados.rdata\"))\n\n\n\n\n\n\n\nNota\n\n\n\nRecordemos que podemos contar con bases de datos que tengan factor de expansión (ponderador) o no. Esta distinción se presenta cuando trabajamos con muestras simples o complejas. Al trabajar con muestras complejas debemos identificar cuál es la variable del ponderador e incorporarla en nuestro cálculo, como veremos a continuación.\n\n\n\nIntervalos de confianza sin ponderador\nPodemos calcular intervalos de confianza con muestras representativas sin ponderadores o factores de expansión. Supongamos que es el caso.\n\nIC para Medias\nCalculemos un intervalo de confianza para la media de ingresos de personas ocupadas:\n\npsych::describe(esi$ing_t_p)\n\n   vars     n     mean       sd   median  trimmed      mad min      max\nX1    1 37124 586360.4 697362.9 405347.7 474473.1 255411.6   0 38206253\n      range skew kurtosis      se\nX1 38206253   12   402.32 3619.36\n\n\n\nPublish::ci.mean(esi$ing_t_p, alpha = 0.05)\n\n mean      CI-95%               \n 586360.41 [579266.37;593454.45]\n\n\nAl no aplicar factores de expansión, contamos con una media de ingresos de $586.360 como estimación puntual. Pero también podemos decir que con un 95% de confianza el parámetro poblacional se encontrará entre $579.266 y $593.454.\n\n\nIC para Proporciones\nPara calcular un intervalo de confianza para la proporción por la variable sexo, usamos:\n\nsjmisc::frq(esi$sexo)\n\nx &lt;numeric&gt; \n# total N=37124 valid N=37124 mean=1.44 sd=0.50\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\n    1 | 20806 | 56.04 |   56.04 |  56.04\n    2 | 16318 | 43.96 |   43.96 | 100.00\n &lt;NA&gt; |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nprop.test(x = 20806, n = 37124, conf.level = 0.95)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  20806 out of 37124, null probability 0.5\nX-squared = 542.32, df = 1, p-value &lt; 0.00000000000000022\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5553777 0.5655019\nsample estimates:\n        p \n0.5604461 \n\n\nEn este caso, sabemos que el total de las personas ocupadas de la muestra son n=37.124, y que la cantidad de hombres son 20.806, correspondientes al 56% como estimación puntual. También podemos sostener con un 95% que la proporción de hombres en la población se encuentra entre 55.54% y 56.6%.\n\n\n\nIntervalos de confianza con ponderador\nPara muestras complejas que cuentan con ponderador (o factor de expansión) también podemos hacer este ejercicio.\nPrimero, es necesario identificar la variable de factor de expansión o ponderador:\n\nesi_pond &lt;- esi %&gt;% as_survey_design(ids = 1, # indica conglomerados de muestreo; ~0 o ~1 cuando no hay\n                                     strata = estrato, # indica efecto de diseño muestral\n                                     weights = fact_cal_esi) # indica el ponderador\n\noptions(survey.lonely.psu = \"certainty\") # seteamos para que ids no moleste\n\n\nIC para Medias\nAhora, teniendo en consideración el factor de expansión, podemos señalar que:\n\nesi_pond %&gt;% \n  summarise(media = survey_mean(ing_t_p, vartype = \"ci\", levels = 0.95, na.rm=TRUE)) # usamos funcion survey_mean\n\n# A tibble: 1 × 3\n    media media_low media_upp\n    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 681039.   666563.   695516.\n\n\nEl promedio de ingresos de personas ocupadas ponderado en la población corresponde a $681.039 como estimación puntual, pero que es posible afirmar con un 95% de confianza que el parámetro poblacional se encuentra entre $666.562 y $695.516.\n\n\nIC para Proporciones\nFinalmente, si calculamos la proporción de hombres ocupados en la población considerando el factor de expansión:\n\nsjmisc::frq(esi$sexo)\n\nx &lt;numeric&gt; \n# total N=37124 valid N=37124 mean=1.44 sd=0.50\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\n    1 | 20806 | 56.04 |   56.04 |  56.04\n    2 | 16318 | 43.96 |   43.96 | 100.00\n &lt;NA&gt; |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\nesi_pond %&gt;% \n  group_by(sexo) %&gt;% # agrupamos por sexo\n  summarise(prop = survey_prop(vartype = \"ci\", levels = 0.95, na.rm = TRUE))\n\n# A tibble: 2 × 4\n   sexo  prop prop_low prop_upp\n  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     1 0.582    0.575    0.590\n2     2 0.418    0.410    0.425\n\n\nTenemos que, con un 95% de conafianza, podemos afirmar que la proporción de hombre ocupados se encuentra entre el 57% y 58%."
  },
  {
    "objectID": "resource/08-resource.html",
    "href": "resource/08-resource.html",
    "title": "Práctica 5 Supuestos de regresión lineal",
    "section": "",
    "text": "La siguiente práctica tiene el objetivo de introducir en los supuestos y robustez del modelo de regresión. Por esta razón, volveremos a algunos de los contenidos previos relacionados con la estimación, análisis de residuos y ajuste. Para ello, utilizaremos la base de datos de la tercera ola del Estudio Longitudinal Social del Chile 2018 con el objetivo de analizar los determinantes de la Participación Ciudadana.\nLa versión original de este ejercicio proviene del curso de Estadística multivariada versión 2022."
  },
  {
    "objectID": "resource/08-resource.html#explorar-datos",
    "href": "resource/08-resource.html#explorar-datos",
    "title": "Práctica 5 Supuestos de regresión lineal",
    "section": "Explorar datos",
    "text": "Explorar datos\nA partir de la siguiente tabla se obtienen estadísticos descriptivos que luego serán relevantes para realizar las transformaciones y análisis posteriores.\n\nview(dfSummary(elsoc, headings = FALSE, method = \"render\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo\nVariable\nLabel\nStats / Values\nFreqs (% of Valid)\nGraph\nValid\nMissing\n\n\n\n\n1\nsexo [numeric]\nSexo entrevistado\n\n\n\nMin : 0\n\n\nMean : 0.6\n\n\nMax : 1\n\n\n\n\n\n\n0\n:\n1446\n(\n38.6%\n)\n\n\n1\n:\n2302\n(\n61.4%\n)\n\n\n\n\n3748 (100.0%)\n0 (0.0%)\n\n\n2\nedad [numeric]\nEdad entrevistado\n\n\n\nMean (sd) : 47.1 (15.5)\n\n\nmin ≤ med ≤ max:\n\n\n18 ≤ 47 ≤ 90\n\n\nIQR (CV) : 25 (0.3)\n\n\n\n70 distinct values\n\n3748 (100.0%)\n0 (0.0%)\n\n\n3\neduc [factor]\nNivel educacional\n\n\n\n1. 1\n\n\n2. 2\n\n\n3. 3\n\n\n4. 4\n\n\n5. 5\n\n\n\n\n\n\n450\n(\n12.0%\n)\n\n\n370\n(\n9.9%\n)\n\n\n1600\n(\n42.7%\n)\n\n\n598\n(\n16.0%\n)\n\n\n725\n(\n19.4%\n)\n\n\n\n\n3743 (99.9%)\n5 (0.1%)\n\n\n4\npospol [factor]\nAutoubicacion escala izquierda-derecha\n\n\n\n1. 1\n\n\n2. 2\n\n\n3. 3\n\n\n4. 4\n\n\n\n\n\n\n807\n(\n22.0%\n)\n\n\n952\n(\n26.0%\n)\n\n\n734\n(\n20.0%\n)\n\n\n1171\n(\n32.0%\n)\n\n\n\n\n3664 (97.8%)\n84 (2.2%)\n\n\n5\npart01 [numeric]\nFrecuencia: Firma carta o peticion apoyando causa\n\n\n\nMean (sd) : 1.5 (0.9)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 1 ≤ 5\n\n\nIQR (CV) : 1 (0.6)\n\n\n\n\n\n\n1\n:\n2717\n(\n72.6%\n)\n\n\n2\n:\n476\n(\n12.7%\n)\n\n\n3\n:\n411\n(\n11.0%\n)\n\n\n4\n:\n117\n(\n3.1%\n)\n\n\n5\n:\n21\n(\n0.6%\n)\n\n\n\n\n3742 (99.8%)\n6 (0.2%)\n\n\n6\npart02 [numeric]\nFrecuencia: Asiste a marcha o manifestacion pacifica\n\n\n\nMean (sd) : 1.2 (0.6)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 1 ≤ 5\n\n\nIQR (CV) : 0 (0.5)\n\n\n\n\n\n\n1\n:\n3289\n(\n87.8%\n)\n\n\n2\n:\n195\n(\n5.2%\n)\n\n\n3\n:\n191\n(\n5.1%\n)\n\n\n4\n:\n51\n(\n1.4%\n)\n\n\n5\n:\n19\n(\n0.5%\n)\n\n\n\n\n3745 (99.9%)\n3 (0.1%)\n\n\n7\npart03 [numeric]\nFrecuencia: Participa en huelga\n\n\n\nMean (sd) : 1.2 (0.5)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 1 ≤ 5\n\n\nIQR (CV) : 0 (0.5)\n\n\n\n\n\n\n1\n:\n3407\n(\n91.0%\n)\n\n\n2\n:\n152\n(\n4.1%\n)\n\n\n3\n:\n146\n(\n3.9%\n)\n\n\n4\n:\n29\n(\n0.8%\n)\n\n\n5\n:\n11\n(\n0.3%\n)\n\n\n\n\n3745 (99.9%)\n3 (0.1%)\n\n\n8\npart04 [numeric]\nFrecuencia: Usa redes sociales para opinar en temas publicos\n\n\n\nMean (sd) : 1.6 (1.1)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 1 ≤ 5\n\n\nIQR (CV) : 1 (0.7)\n\n\n\n\n\n\n1\n:\n2598\n(\n69.4%\n)\n\n\n2\n:\n310\n(\n8.3%\n)\n\n\n3\n:\n514\n(\n13.7%\n)\n\n\n4\n:\n223\n(\n6.0%\n)\n\n\n5\n:\n98\n(\n2.6%\n)\n\n\n\n\n3743 (99.9%)\n5 (0.1%)\n\n\n9\nquintilemiss [factor]\n\n\n\n\n1. Quintil 1\n\n\n2. Quintil 2\n\n\n3. Quintil 3\n\n\n4. Quintil 4\n\n\n5. Quintil 5\n\n\n6. Missing\n\n\n\n\n\n\n711\n(\n19.0%\n)\n\n\n711\n(\n19.0%\n)\n\n\n710\n(\n18.9%\n)\n\n\n710\n(\n18.9%\n)\n\n\n710\n(\n18.9%\n)\n\n\n196\n(\n5.2%\n)\n\n\n\n\n3748 (100.0%)\n0 (0.0%)\n\n\n\n\nGenerated by summarytools 1.0.1 (R version 4.3.2)2024-07-11\n\n\n\n\nview_df(elsoc,max.len = 50)\n\n\nData frame: elsoc\n\n\n\n\n\n\n\n\n\nID\nName\nLabel\nValues\nValue Labels\n\n\n1\nsexo\nSexo entrevistado\n0\n1\nHombre\nMujer\n\n\n2\nedad\nEdad entrevistado\nrange: 18-90\n\n\n3\neduc\nNivel educacional\n1\n2\n3\n4\n5\nPrimaria incompleta menos\nPrimaria y secundaria baja\nSecundaria alta\nTerciaria ciclo corto\nTerciaria y Postgrado\n\n\n4\npospol\nAutoubicacion escala izquierda-derecha\n1\n2\n3\n4\nDerecha\nCentro\nIzquierda\nIndep./Ninguno\n\n\n5\npart01\nFrecuencia: Firma carta o peticion apoyando causa\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n6\npart02\nFrecuencia: Asiste a mbackground-color:#eeeeeeha o manifestacion\npacifica\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n7\npart03\nFrecuencia: Participa en huelga\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n8\npart04\nFrecuencia: Usa redes sociales para opinar en\ntemas publicos\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n9\nquintilemiss\n\n\nQuintil 1\nQuintil 2\nQuintil 3\nQuintil 4\nQuintil 5\nMissing\n\n\n\n\n\n\nelsoc &lt;- elsoc %&gt;% mutate(partpol=rowSums(select(., part01,part02,part03,part04)))"
  },
  {
    "objectID": "resource/08-resource.html#diágnosticos",
    "href": "resource/08-resource.html#diágnosticos",
    "title": "Práctica 5 Supuestos de regresión lineal",
    "section": "Diágnosticos",
    "text": "Diágnosticos\n\nCasos influyentes\nPara determinar si un outlier es un caso influyente, es decir que su presencia/ausencia genera un cambio importante en la estimación de los coeficientes de regresión, calculamos la Distancia de Cook..\nPosteriormente, se establece un punto de corte de \\(4/(n-k-1)\\):\n\nn&lt;- nobs(fit04) #n de observaciones\nk&lt;- length(coef(fit04)) # n de parametros\ndcook&lt;- 4/(n-k-1) #punt de corte\n\nSi lo graficamos se ve de la siguiente manera:\n\nfinal &lt;- broom::augment_columns(fit04,data = elsoc)\nfinal$id &lt;- as.numeric(row.names(final))\n# identify obs with Cook's D above cutoff\nggplot(final, aes(id, .cooksd)) +\n  geom_bar(stat=\"identity\", position=\"identity\") +\n  xlab(\"Obs. Number\")+ # Modificación nombre eje x\n  ylab(\"Cook's distance\")+ # Modificación nombre eje y\n  geom_hline(yintercept=dcook)+ # Incluir una línea horizontal\n  geom_text(aes(label=ifelse((.cooksd&gt;dcook),id,\"\")), # geom text agrega nombre a los casos, en esta oportunidad solo a los valores mayores a dcook\n            vjust=-0.2, hjust=0.5)\n\n\n\n\n\n\n\n\nIdentificamos los casos influyentes y filtramos la base de datos:\n\nident&lt;- final %&gt;% filter(.cooksd&gt;dcook)\nelsoc02 &lt;- final %&gt;% filter(!(id %in% ident$id))\n\nEstimación sin casos influyentes:\n\nfit05&lt;- lm(partpol~sexo+edad+quintilemiss+pospol,data=elsoc02)\n\nlabs02 &lt;- c(\"Intercepto\",\"Sexo (mujer=1)\",\"Edad\",\n            \"Quintil 2\",\"Quintil 3\",\"Quintil 4\",\"Quintil 5\",\"Quintil perdido\",\n            \"Izquierda (ref. derecha)\",\"Centro\",\"Idep./Ninguno\")\n\nhtmlreg(list(fit04,fit05), \n        doctype = FALSE,\n        custom.model.names = c(\"Modelo 4\", \"Modelo 5\"),\n        custom.coef.names = labs02)\n\n\nStatistical models\n\n\n\n\n \n\n\nModelo 4\n\n\nModelo 5\n\n\n\n\n\n\nIntercepto\n\n\n7.97***\n\n\n7.05***\n\n\n\n\n \n\n\n(0.16)\n\n\n(0.11)\n\n\n\n\nSexo (mujer=1)\n\n\n0.12\n\n\n0.07\n\n\n\n\n \n\n\n(0.07)\n\n\n(0.05)\n\n\n\n\nEdad\n\n\n-0.04***\n\n\n-0.03***\n\n\n\n\n \n\n\n(0.00)\n\n\n(0.00)\n\n\n\n\nQuintil 2\n\n\n0.21\n\n\n0.11\n\n\n\n\n \n\n\n(0.11)\n\n\n(0.08)\n\n\n\n\nQuintil 3\n\n\n0.51***\n\n\n0.34***\n\n\n\n\n \n\n\n(0.11)\n\n\n(0.08)\n\n\n\n\nQuintil 4\n\n\n0.50***\n\n\n0.32***\n\n\n\n\n \n\n\n(0.11)\n\n\n(0.08)\n\n\n\n\nQuintil 5\n\n\n0.88***\n\n\n0.57***\n\n\n\n\n \n\n\n(0.12)\n\n\n(0.08)\n\n\n\n\nQuintil perdido\n\n\n0.59***\n\n\n0.31*\n\n\n\n\n \n\n\n(0.18)\n\n\n(0.13)\n\n\n\n\nIzquierda (ref. derecha)\n\n\n-1.04***\n\n\n-0.65***\n\n\n\n\n \n\n\n(0.10)\n\n\n(0.07)\n\n\n\n\nCentro\n\n\n-1.13***\n\n\n-0.71***\n\n\n\n\n \n\n\n(0.11)\n\n\n(0.08)\n\n\n\n\nIdep./Ninguno\n\n\n-1.60***\n\n\n-1.14***\n\n\n\n\n \n\n\n(0.10)\n\n\n(0.07)\n\n\n\n\nR2\n\n\n0.17\n\n\n0.18\n\n\n\n\nAdj. R2\n\n\n0.17\n\n\n0.18\n\n\n\n\nNum. obs.\n\n\n3656\n\n\n3460\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\nEn términos generales, el sentido y significación estadística de los coeficientes del Modelo 5 se mantiene respecto al Modelo 4. Adicionalmente, si observamos que el modelo sin casos influyentes presenta una mejora en ajuste. Por lo tanto, los análisis posteriores se realizaran en base a este modelo.\n\n\nLinealidad\nPara analizar la linealidad respecto de un modelo de regresión, debemos analizar la distribución de los residuos con respecto a la recta de regresión.\n\nLos residuos deben ser independientes de los valores predichos (fitted values).\nCualquier correlación entre residuo y valores predichos violarían este supuesto.\nLa presencia de un patrón no lineal, es señal de que el modelo está especificado incorrectamente.\n\n\nggplot(fit05, aes(.fitted, .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0) +\n  geom_smooth(se = TRUE)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nRelación entre residuos y valores predichos\n\n\n\n\nEl gráfico nos indica que existe un patrón en la distribución de los residuos. Para intentar mejorar la estimación podemos realizar una transformación de variables. A continuación presentaremos un ejemplo para la Edad y para los Ingresos.\n\nPolinomio: \\(\\text{Edad}^2\\)\n\n\nelsoc02$edad2 &lt;- elsoc02$edad^2\nfit06&lt;- lm(partpol~sexo+edad+edad2+quintilemiss+pospol,data=elsoc02)\n\n\nedad&lt;- fit06$model$edad\nfit&lt;- fit06$fitted.values\ndata01 &lt;- as.data.frame(cbind(edad,fit))\n\nggplot(data01, aes(x = edad, y = fit)) +\n  theme_bw() +\n  geom_point()+\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nEfecto cuadrático de la edad (Modelo 5)\n\n\n\n\n\nfit07 &lt;- lm(partpol~sexo+edad+edad2+quintilemiss+pospol,data=elsoc02)\n\nlabs03 &lt;- c(\"Intercepto\",\"Sexo (mujer=1)\",\"Edad\",\n            \"Quintil 2\",\"Quintil 3\",\"Quintil 4\",\"Quintil 5\",\"Quintil perdido\",\n            \"Izquierda (ref. derecha)\",\"Centro\",\"Idep./Ninguno\", \"Edad²\")\n\nhtmlreg(list(fit05, fit06, fit07), doctype = FALSE,\n        custom.model.names = c(\"Modelo 4\", \"Modelo 5\", \"Modelo 6\"), \n          custom.coef.names = labs03)\n\n\nStatistical models\n\n\n\n\n \n\n\nModelo 4\n\n\nModelo 5\n\n\nModelo 6\n\n\n\n\n\n\nIntercepto\n\n\n7.05***\n\n\n7.62***\n\n\n7.62***\n\n\n\n\n \n\n\n(0.11)\n\n\n(0.24)\n\n\n(0.24)\n\n\n\n\nSexo (mujer=1)\n\n\n0.07\n\n\n0.08\n\n\n0.08\n\n\n\n\n \n\n\n(0.05)\n\n\n(0.05)\n\n\n(0.05)\n\n\n\n\nEdad\n\n\n-0.03***\n\n\n-0.06***\n\n\n-0.06***\n\n\n\n\n \n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.01)\n\n\n\n\nQuintil 2\n\n\n0.11\n\n\n0.11\n\n\n0.11\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.08)\n\n\n(0.08)\n\n\n\n\nQuintil 3\n\n\n0.34***\n\n\n0.34***\n\n\n0.34***\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.08)\n\n\n(0.08)\n\n\n\n\nQuintil 4\n\n\n0.32***\n\n\n0.32***\n\n\n0.32***\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.08)\n\n\n(0.08)\n\n\n\n\nQuintil 5\n\n\n0.57***\n\n\n0.57***\n\n\n0.57***\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.08)\n\n\n(0.08)\n\n\n\n\nQuintil perdido\n\n\n0.31*\n\n\n0.31*\n\n\n0.31*\n\n\n\n\n \n\n\n(0.13)\n\n\n(0.13)\n\n\n(0.13)\n\n\n\n\nIzquierda (ref. derecha)\n\n\n-0.65***\n\n\n-0.65***\n\n\n-0.65***\n\n\n\n\n \n\n\n(0.07)\n\n\n(0.07)\n\n\n(0.07)\n\n\n\n\nCentro\n\n\n-0.71***\n\n\n-0.70***\n\n\n-0.70***\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.08)\n\n\n(0.08)\n\n\n\n\nIdep./Ninguno\n\n\n-1.14***\n\n\n-1.13***\n\n\n-1.13***\n\n\n\n\n \n\n\n(0.07)\n\n\n(0.07)\n\n\n(0.07)\n\n\n\n\nEdad²\n\n\n \n\n\n0.00**\n\n\n0.00**\n\n\n\n\n \n\n\n \n\n\n(0.00)\n\n\n(0.00)\n\n\n\n\nR2\n\n\n0.18\n\n\n0.19\n\n\n0.19\n\n\n\n\nAdj. R2\n\n\n0.18\n\n\n0.18\n\n\n0.18\n\n\n\n\nNum. obs.\n\n\n3460\n\n\n3460\n\n\n3460\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "resource/08-resource.html#referencias",
    "href": "resource/08-resource.html#referencias",
    "title": "Práctica 5 Supuestos de regresión lineal",
    "section": "Referencias",
    "text": "Referencias\nDarlington & Hayes 2016 Cap16 Detecting and Managing Irregularities\nDarlington & Hayes 2016 Cap12 Nonlinear relationships"
  },
  {
    "objectID": "resource/02-resource.html",
    "href": "resource/02-resource.html",
    "title": "Práctico 2. Matrices de correlación y tamaños de efecto",
    "section": "",
    "text": "El objetivo de esta guía práctica es conocer maneras de reportar coeficientes de correlación y cómo interpretar sus tamaños de efecto en ciencias sociales. Además, nos introduciremos en el tratamiento de valores perdidos y otras medidas de correlación entre variables.\nEn detalle, aprenderemos:\n\nCómo reportar y presentar matrices de correlación.\nInterpretar el tamaño de efecto de una correlación.\nTratamiento de casos perdidos.\nQué es y cómo calcular la correlación de Spearman.\nQué es el coeficiente de determinación \\(R^2\\).\n\n\n\n\n\n\n\nNota\n\n\n\n¿Qué era la correlación?\nLa correlación es una medida de asociación entre variables, que describe el sentido (dirección) y fuerza de la asociación.\nEn otras palabras, nos permite conocer cómo y cuánto se relaciona la variación de una variable, con la variación de otra variable.\n\n\n\n\nEn esta práctica trabajaremos con un subconjunto de datos previamente procesados del Estudio Longitudinal Social de Chile (ELSOC) del año 2016, elaborado por COES. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  ELSOC 2016. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos ELSOC 2016."
  },
  {
    "objectID": "resource/02-resource.html#recursos-de-la-práctica",
    "href": "resource/02-resource.html#recursos-de-la-práctica",
    "title": "Práctico 2. Matrices de correlación y tamaños de efecto",
    "section": "",
    "text": "En esta práctica trabajaremos con un subconjunto de datos previamente procesados del Estudio Longitudinal Social de Chile (ELSOC) del año 2016, elaborado por COES. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  ELSOC 2016. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos ELSOC 2016."
  },
  {
    "objectID": "resource/02-resource.html#tratamiento-de-casos-perdidos",
    "href": "resource/02-resource.html#tratamiento-de-casos-perdidos",
    "title": "Práctico 2. Matrices de correlación y tamaños de efecto",
    "section": "Tratamiento de casos perdidos",
    "text": "Tratamiento de casos perdidos\nTrabajar con datos a menudo implica enfrentar valores perdidos (NA), lo que puede ser un gran desafío. Estos valores indican la ausencia de un valor en una base de datos. Los valores perdidos pueden originarse por diversas razones, como el sesgo de no respuesta en encuestas, errores en la entrada de datos o simplemente la falta de información para ciertas variables.\n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\nNA\n4\n1\nHola\n\n\n7\n1\n4\nNo soy un NA\n\n\n8\nNA\n2\nNA\n\n\n9\nNA\n9\nAmo R\n\n\n3\n3\n6\nNA\n\n\n\n\n\n\n\n\nLa presencia de valores perdidos puede tener un impacto considerable en la precisión y confiabilidad de los análisis estadísticos, lo que a su vez puede conducir a resultados sesgados y conclusiones incorrectas.\nExisten varias formas de tratar valores perdidos, que van desde enfoques simples hasta métodos más complejos, como la imputación. En esta ocasión, nos centraremos en las dos estrategias más comunes:\n\ntrabajar exclusivamente con casos completos (listwise) o\nretener los casos con valores perdidos, pero excluyéndolos al calcular estadísticas (pairwise).\n\n\na) Analísis con casos completos: listwise deletion\nEste enfoque es uno de los más conocidos: implica remover completamente las observaciones que tienen valores perdidos en cualquier variable de interés. En otras palabras, si una fila/caso en un conjunto de datos tiene al menos un valor faltante en alguna de las variables que estás considerando, se eliminará por completo.\nEn R, esto podemos hacerlo con la función na.omit. Para hacer esto, sigamos estos pasos:\n\nrespaldar la base de datos original en el espacio de trabajo (por si queremos en adelante realizar algún análisis referido a casos perdidos)-\ncontamos el número de casos con el comando dim.\ncontamos cuántos y en dónde tenemos casos perdidos.\nborramos los casos perdidos con na.omit.\ncontamos nuevamente con dim para asegurarnos que se borraron.\n\n\nproc_elsoc_original &lt;- proc_elsoc\ndim(proc_elsoc)\n\n[1] 2927    7\n\n\n\nsum(is.na(proc_elsoc))\n\n[1] 81\n\n\n\ncolSums(is.na(proc_elsoc))\n\nmesfuerzo  mtalento       ess    edcine      sexo      edad    pmerit \n       18        20        12         2         0         0        29 \n\n\n\nproc_elsoc &lt;- na.omit(proc_elsoc)\ndim(proc_elsoc)\n\n[1] 2887    7\n\n\nAhora nos quedamos con 2887 observaciones sin casos perdidos.\nAunque es simple de implementar, con este enfoque podemos perder información importante, especialmente si los valores perdidos no se distribuyen aleatoriamente.\n\nSiempre hay que intentar rescatar la mayor cantidad de casos posibles. Por lo tanto, si un listwise genera más de un 10% de casos perdidos se debe detectar qué variables esta produciendo esta pérdida e intentar recuperar datos. Puedes revisar un ejemplo aquí.\n\n\n\nb) Retener pero excluir: pairwise deletion\nA diferencia del anterior, este es un enfoque en el que las observaciones se utilizan para el análisis siempre que tengan datos disponibles para las variables específicas que se están analizando. En lugar de eliminar toda una fila si falta un valor, se eliminan solo los valores faltantes en las variables que se están analizando en ese momento.\nPara hacer esto en R debemos siempre verificar e indicar en nuestro código si queremos (o no) remover los NA para realizar los análisis.\n\nmean(proc_elsoc_original$pmerit); mean(proc_elsoc$edad); mean(proc_elsoc$ess)\n\n[1] NA\n\n\n[1] 45.98337\n\n\n[1] 4.333564\n\nmean(proc_elsoc_original$pmerit, na.rm = TRUE); mean(proc_elsoc$edad, na.rm = TRUE); mean(proc_elsoc$ess, na.rm = TRUE)\n\n[1] 2.653899\n\n\n[1] 45.98337\n\n\n[1] 4.333564\n\n\nCon el primer código no obtuvimos información sustantiva en ciertas variables, pero con el segundo sí al remover los NA solo de dicha variable para un cálculo determinado."
  },
  {
    "objectID": "resource/06-resource.html",
    "href": "resource/06-resource.html",
    "title": "Práctico 6. Asociación con variables categóricas",
    "section": "",
    "text": "El objetivo de esta guía práctica es introducirnos en técnicas de asociación entre variables categóricas, aplicando lo apredendido hasta ahora sobre inferencia estadística.\nEn detalle, aprenderemos:\n\nGenerar y analizar tablas de contingencia (o cruzadas)\nEstimar e interpretar la prueba de Chi-cuadrado\nAplicar coeficientes de correlación entre variables categóricas\n\n\n\nEn esta práctica trabajaremos con un subconjunto de datos previamente procesados de la Encuesta de Caracterización Socioeconómica (CASEN) del año 2022, elaborada por el Ministerio de Desarrollo Social y Familia. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienen la opción de acceder a la misma información a través del siguiente enlace:  CASEN 20222. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos CASEN 2022."
  },
  {
    "objectID": "resource/06-resource.html#recursos-de-la-práctica",
    "href": "resource/06-resource.html#recursos-de-la-práctica",
    "title": "Práctico 6. Asociación con variables categóricas",
    "section": "",
    "text": "En esta práctica trabajaremos con un subconjunto de datos previamente procesados de la Encuesta de Caracterización Socioeconómica (CASEN) del año 2022, elaborada por el Ministerio de Desarrollo Social y Familia. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienen la opción de acceder a la misma información a través del siguiente enlace:  CASEN 20222. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos CASEN 2022."
  },
  {
    "objectID": "assignment/taller-github-quarto/taller-github-quarto.html",
    "href": "assignment/taller-github-quarto/taller-github-quarto.html",
    "title": "Práctico 02: Taller de Quarto y Github",
    "section": "",
    "text": "Crear cuenta en www.github.com\nDescargar Github Desktop",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 02: Taller quarto y github"
    ]
  },
  {
    "objectID": "assignment/taller-github-quarto/taller-github-quarto.html#introducción-al-flujo-de-investigación-reproducible",
    "href": "assignment/taller-github-quarto/taller-github-quarto.html#introducción-al-flujo-de-investigación-reproducible",
    "title": "Práctico 02: Taller de Quarto y Github",
    "section": "",
    "text": "Crear cuenta en www.github.com\nDescargar Github Desktop",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 02: Taller quarto y github"
    ]
  },
  {
    "objectID": "assignment/taller-github-quarto/taller-github-quarto.html#github",
    "href": "assignment/taller-github-quarto/taller-github-quarto.html#github",
    "title": "Práctico 02: Taller de Quarto y Github",
    "section": "Github",
    "text": "Github\n\nDescripción\nGithub es una plataforma de desarrollo colaborativo que permite alojar proyectos utilizando el sistema de control de versiones Git. Se utiliza principalmente para la creación de código fuente de programas (software).\n\n\n\n\n\n\nNota\n\n\n\nEl 4 de junio de 2018 Microsoft compró GitHub por la cantidad de 7500 millones de dólares. Al inicio, el cambio de propietario generó preocupaciones y la salida de algunos proyectos de este sitio; sin embargo, no fueron representativos. GitHub continúa siendo la plataforma más importante de colaboración para proyectos de código abierto.\n\n\n\n\nRepositorios\nUn repositorio contiene todo el código, tus archivos y el historial de revisiones y cambios de cada uno de ellos. Es el elemento más básico de Github.\nLos repositorios pueden contar con múltiples colaboradores y pueden ser públicos o privados.\n\n\nPrincipales términos\n\n\n\n\n\n\n\nTérmino\nDefinición\n\n\n\n\nBranch\nUna versión paralela del código contenido en el repositorio, pero que no afecta a la rama principal.\n\n\nClonar\nPara descargar una copia completa de los datos de un repositorio de GitHub.com, incluidas todas las versiones de cada archivo y carpeta.\n\n\nFork\nUn nuevo repositorio que comparte la configuración de visibilidad y código con el repositorio «ascendente» original.\n\n\nMerge\nPara aplicar los cambios de una rama y en otra.\n\n\nPull request\nUna solicitud para combinar los cambios de una branch en otra.\n\n\nRemote\nUn repositorio almacenado en GitHub, no en el equipo.\n\n\nUpstream\nLa branch de un repositorio original que se ha forkeado o clonado. La branch correspondiente de la branch clonada o forkeada se denomina «descendente».\n\n\n\n\n\nCrear cuenta e instalación\n\nAcceder a la página de github\n\nRegistrarse ingresando correo electrónico y siguiendo los pasos descritos (crear contraseña y nombre de usuario)\n\nLa personalización de la cuenta se puede saltar haciendo click en skip abajo de la selección de opciones\n\nDescargar e instalar Github Desktop\n\n\n\n\nCrear repositorio\nEn la página principal de github hacer click en el ícono de usuario de la esquina superior derecha y luego ir a Tus repositorios\n\nUna vez accedemos a Tus repositorios hacemos click en New/Nuevo\n\nLuego le ponemos un nombre a nuestro repositorio, evitando siempre espacios, ñ y tíldes, y apretamos Crear repositorio\n\n\n\n\n\n\n\nNota\n\n\n\nUn buen nombre debería intentar resumir las principales características del proyecto de investigación en no más de 3 o 4 conceptos (por ejemplo, movilidad-social-AL para proyecto sobre movilidad social en América Latina)\nEn esta oportunidad, una recomendación sería generar un repositorio “ejercicios-practicos” para almacenar los distintos scripts de ejercicios prácticos que realizaremos en el curso.\nLuego pueden generar un segundo repositorio que sea “Trabajo” para almacenar el desarrollo de sus trabajos de investigación.\n\n\n\n\nGithub desktop\nUna vez creado un repositorio, lo que nos interesa es descargarlo. Al abrir la aplicación de Github desktop por primera vez (descargada anteriormente), nos debería aparecer la opción de clonar nuestro repositorio ejercicios-practicos en la pantalla de inicio. Lo clonamos y seleccionamos una carpeta de nuestro computador para almacenarlo.\nPara todas las siguientes veces, las instrucciones son estas:\n1- Apretamos Repositorio actual en la esquina superior izquierda\n2- Apretamos añadir\n3- Apretamos clonar repositorio…\n\n4- Seleccionamos nuestro repositorio\n5- seleccionamos la carpeta donde se almacenará. Siempre evitando tener tíldes, ñ y espacios en la dirección de almacenamiento y apretamos ‘clone’.\n\n7- Creamos las carpetas pertenecientes al protocolo IPO (input-procesamiento-output) para organizar nuestro proyecto)",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 02: Taller quarto y github"
    ]
  },
  {
    "objectID": "assignment/taller-github-quarto/taller-github-quarto.html#rstudio-projects",
    "href": "assignment/taller-github-quarto/taller-github-quarto.html#rstudio-projects",
    "title": "Práctico 02: Taller de Quarto y Github",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\nFile -&gt; New Project\n\n\n\n\nAbriendo la sesión de RStudio como proyecto\n\nidentificar en la carpeta respectiva el archivo .Rproj\nejecutar y se abre R / RStudio con ese directorio como raíz\n\n\n\nRutas relativas en código\n\nforma de “señalar el camino” para abrir y guardar archivos al interior de una carpeta de proyecto autocontenido (= sin referencias locales)\neste camino tiene básicamente 3 direcciones:\n\nbajar -&gt; hacia subcarpetas\nsubir -&gt; hacia carpetas superiores\nsubir y bajar -&gt; hacia otras subcarpetas\n\n\n\nbajando\n\npara “bajar” hacia a una subcarpeta, simplemente damos la ruta de la carpeta/archivo\n\nej: si estoy en el archivo paper.Rmd (directorio raíz), y quiero incluir una imagen (directorio input/images/imagen.jpg), entonces la ruta es input/images/imagen.jpg\no para señalar la ruta al bib desde paper.Rmd (en raíz): input/bib/referencias.bib\n\n\n\n\nsubiendo\n\npara subir se utilizan los caracteres ../ por cada nivel.\nEj: si quiero guardar una tabla en el directorio raíz generada desde un archivo de código en la subcarpeta proc, entonces la ruta es ../tabla.html\n\n\n\nsubiendo y bajando\n\ncombinación de las anteriores\nEj: para abrir la base de datos original en la subcarpeta input/data desde el código de procesamiento en la subcarpeta proc, entonces:\n\n../input/data/original.dat",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 02: Taller quarto y github"
    ]
  },
  {
    "objectID": "assignment/taller-github-quarto/taller-github-quarto.html#código-de-análisis-de-ejemplo",
    "href": "assignment/taller-github-quarto/taller-github-quarto.html#código-de-análisis-de-ejemplo",
    "title": "Práctico 02: Taller de Quarto y Github",
    "section": "Código de análisis de ejemplo",
    "text": "Código de análisis de ejemplo\nPara poder escribir código de análisis en un documento Quarto debemos generar trozo de código llamado ‘Chunk’, que se puede crear con ctrl+alt+i o directamente en el menú de arriba en ‘Code -&gt; Insert Chunk’.\n\n\nCargar paquetes\n\npacman::p_load(sjlabelled,\n               dplyr, #Manipulacion de datos\n              stargazer, #Tablas\n              sjmisc, # Tablas\n              summarytools, # Tablas\n              kableExtra, #Tablas\n              sjPlot, #Tablas y gráficos\n              corrplot, # Correlaciones\n              sessioninfo, # Información de la sesión de trabajo\n              ggplot2) # Para la mayoría de los gráficos\n\n\n\nCargar bases de datos\nCargamos ambas bases de datos desde internet\n\nload(url(\"https://github.com/Kevin-carrasco/R-data-analisis/raw/main/files/data/latinobarometro_total.RData\")) #Cargar base de datos\nload(url(\"https://github.com/Kevin-carrasco/R-data-analisis/raw/main/files/data/data_wvs.RData\")) #Cargar base de datos\n\nPara trabajar con ambas bases, agruparemos las variables de interés por país, por lo que ya no trabajaremos directamente con individuos.\n\ncontext_data &lt;- wvs %&gt;% group_by(B_COUNTRY) %&gt;% # Agrupar por país\n  summarise(gdp = mean(GDPpercap1, na.rm = TRUE), # Promedio de GDP per capita\n         life_exp = mean(lifeexpect, na.rm = TRUE), # Promedio esperanza de vida\n         gini = mean(giniWB, na.rm = TRUE)) %&gt;%  # Promedio gini\n  rename(idenpa=B_COUNTRY) # Para poder vincular ambas bases, es necesario que la variable de identificación se llamen igual\ncontext_data$idenpa &lt;- as.numeric(context_data$idenpa) # Como era categórica, la dejamos numérica\n\nproc_data &lt;- proc_data %&gt;% group_by(idenpa) %&gt;%  # agrupamos por país\n  summarise(promedio = mean(conf_inst, na.rm = TRUE)) # promedio de confianza en instituciones por país\n\n\n\nUnir bases de datos\nPara vincular nuestras bases de datos existen múltiples opciones, la primera es ‘merge’ de R base y las siguientes tres vienen desde dplyr: ‘right_join’, ‘full_join’ y ‘left_join’. Cada una tiene sus propias potencialidades y limitaciones y dependerá de cada caso cuál usemos\n\nProbemos merge\n\ndata &lt;- merge(proc_data, context_data, by=\"idenpa\")\n\n\ndata &lt;- data %&gt;%\n  mutate(idenpa = as.character(idenpa)) %&gt;%\n  mutate(idenpa = case_when(\n    idenpa == \"32\" ~ \"Argentina\",\n    idenpa == \"68\" ~ \"Bolivia\",\n    idenpa == \"76\" ~ \"Brasil\",\n    idenpa == \"152\" ~ \"Chile\",\n    idenpa == \"170\" ~ \"Colombia\",\n    idenpa == \"188\" ~ \"Costa Rica\",\n    idenpa == \"214\" ~ \"Cuba\",\n    idenpa == \"218\" ~ \"República Dominicana\",\n    idenpa == \"222\" ~ \"Ecuador\",\n    idenpa == \"320\" ~ \"El Salvador\",\n    idenpa == \"340\" ~ \"Guatemala\",\n    idenpa == \"484\" ~ \"Honduras\",\n    idenpa == \"558\" ~ \"México\",\n    idenpa == \"591\" ~ \"Nicaragua\",\n    idenpa == \"600\" ~ \"Panamá\",\n    idenpa == \"604\" ~ \"Paraguay\",\n    idenpa == \"858\" ~ \"Uruguay\",\n    idenpa == \"862\" ~ \"Venezuela\"))\n\ndata$gdp &lt;- as.numeric(data$gdp)\ndata$gdp[data$gdp==0] &lt;- NA\ndata &lt;- na.omit(data)",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 02: Taller quarto y github"
    ]
  },
  {
    "objectID": "assignment/taller-github-quarto/taller-github-quarto.html#guardamos-esta-nueva-base-en-nuestra-carpeta-input",
    "href": "assignment/taller-github-quarto/taller-github-quarto.html#guardamos-esta-nueva-base-en-nuestra-carpeta-input",
    "title": "Práctico 02: Taller de Quarto y Github",
    "section": "Guardamos esta nueva base en nuestra carpeta input",
    "text": "Guardamos esta nueva base en nuestra carpeta input\n\nsave(data, file=\"input/data/proc/data.RData\")",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 02: Taller quarto y github"
    ]
  },
  {
    "objectID": "assignment/taller-github-quarto/taller-github-quarto.html#visualizaciones",
    "href": "assignment/taller-github-quarto/taller-github-quarto.html#visualizaciones",
    "title": "Práctico 02: Taller de Quarto y Github",
    "section": "Visualizaciones",
    "text": "Visualizaciones\nPodemos establecer referencias cruzadas para las tablas y gráficos dentro del texto, para poder automatizarlo, como ejemplo así, pero dentro del chunk:\n#| label: tbl-sjmisc\n#| tbl-cap: “Descriptivos con sjmisc”\n\nDescriptivos\nEl Chunk se debería ver así:\n#| label: tbl-sjmisc\n#| tbl-cap: “Descriptivos con sjmisc”\nsjmisc::descr(data,\n  show = c(\"label\",\"range\", \"mean\", \"sd\", \"NA.prc\", \"n\"))%&gt;% # Selecciona estadísticos\n\n  kable(.,\"markdown\") # Esto es para que se vea bien en quarto\n\nsjmisc::descr(data,\n      show = c(\"label\",\"range\", \"mean\", \"sd\", \"NA.prc\", \"n\"))%&gt;% # Selecciona estadísticos\n      kable(.,\"markdown\") # Esto es para que se vea bien en quarto\n\n\n\nTabla 1: Descriptivos con sjmisc\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar\nlabel\nn\nNA.prc\nmean\nsd\nrange\n\n\n\n\n4\npromedio\npromedio\n11\n0\n3.40077\n1.016976\n3.59 (2.3-5.9)\n\n\n1\ngdp\ngdp\n11\n0\n15528.18364\n6480.045512\n19523.79 (5631.2-25154.99)\n\n\n3\nlife_exp\nlife_exp\n11\n0\n75.90909\n2.286593\n8.8 (71.24-80.04)\n\n\n2\ngini\ngini\n11\n0\n45.46364\n4.156266\n14.2 (39.7-53.9)\n\n\n\n\n\n\n\n\nLuego de establecer el link y el nombre de la tabla, podemos referenciar acá con un @, así: @ tbl-sjmisc (pero junto), y que se vería así Tabla 1\n\n\nGráficos\nY para los gráficos se hace de la misma forma:\n#| label: fig-gdp\n#| fig-cap: “Plots”\n\ngraph1&lt;-ggplot(data, aes(x = idenpa, y = gdp)) +\n  geom_point() +\n  labs(x = \"País\", y = \"Gdp\") +\n  theme_minimal()+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\ngraph1\n\n\n\n\n\n\n\nFigura 1: Producto interno bruto por país\n\n\n\n\n\nSin embargo la Figura 1 entrega información desordenada. Mejor ordenar por tamaño de PIB que por orden alfabético de los países. Para eso\n\ndata_sorted &lt;- data %&gt;% arrange(desc(gdp))\ngraph2&lt;-ggplot(data_sorted, aes(x = factor(idenpa, levels = idenpa), y = gdp)) +\n  geom_point() +\n  labs(x = \"País\", y = \"GDP\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\ngraph2\n\n\n\n\n\n\n\nFigura 2: Producto interno bruto por país ordenado\n\n\n\n\n\nAhora sí la Figura 2 muestra un gráfico más ordenado.\n\n\nGuardamos este nuevo gráfico en la carpeta output\n\nggsave(graph2, file=\"output/graphs/graph2.png\")\n\nY comparar el promedio de confianza en instituciones según producto interno bruto por país?\n\ndata %&gt;%\n  ggplot(aes(x = gdp, y = promedio, label = idenpa)) +\n  geom_point() +\n  geom_text(vjust = -0.5) +\n  labs(x = \"GDP\", y = \"Promedio\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigura 3: Confianza en instituciones según el producto interno bruto por país\n\n\n\n\n\nLa Figura 3 muestra la relación que existe entre el producto interno bruto y la confianza en instituciones para los 18 países analizados. Es interesante comparar los casos de Chile y urugay, que al tener similar GDP, tienen un nivel de confianza en instituciones muy diferente.\n\nLuego renderizamos\n\n\ny se debería ver así:\n\n\n\nAhora que tenemos nuestra investigación podemos subirla a Github Pages a través de Github Desktop.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 02: Taller quarto y github"
    ]
  },
  {
    "objectID": "assignment/taller-github-quarto/taller-github-quarto.html#github-desktop-1",
    "href": "assignment/taller-github-quarto/taller-github-quarto.html#github-desktop-1",
    "title": "Práctico 02: Taller de Quarto y Github",
    "section": "Github desktop",
    "text": "Github desktop",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 02: Taller quarto y github"
    ]
  },
  {
    "objectID": "assignment/taller-github-quarto/taller-github-quarto.html#github-pages",
    "href": "assignment/taller-github-quarto/taller-github-quarto.html#github-pages",
    "title": "Práctico 02: Taller de Quarto y Github",
    "section": "Github pages",
    "text": "Github pages\nAhora podemos ver los documentos modificados en nuestro repositorio online de github.\n\nVamos a settings\n\n\n\nDentro de Settings vamos a Pages, luego ‘none’ y seleccionamos ‘main’. Luego apretamos Save\n\n\nLuego de aproximadamente un minuto se actualiza la página y aparecerá un link en la parte superior, algo así como kevin-carrasco.github.io/ipo que es nuestra página principal de nuestro sitio web de github.\nEl link para llegar a nuestro documento renderizado de quarto sigue la estructura del repositorio:\nkevin-carrasco.github.io/ipo/trabajo.html",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 02: Taller quarto y github"
    ]
  },
  {
    "objectID": "assignment/taller-github-quarto/taller-github-quarto.html#footnotes",
    "href": "assignment/taller-github-quarto/taller-github-quarto.html#footnotes",
    "title": "Práctico 02: Taller de Quarto y Github",
    "section": "Notas",
    "text": "Notas\n\n\nEsta es la nota al pie↩︎",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 02: Taller quarto y github"
    ]
  },
  {
    "objectID": "resource/01-resource.html",
    "href": "resource/01-resource.html",
    "title": "Práctico 1. Cálculo y reporte de Correlación",
    "section": "",
    "text": "El objetivo de esta guía práctica es aprender a calcular y graficar la correlación entre dos variables utilizando R.\nEn detalle, aprenderemos:\n\nQué es una correlación\nCuál es la correlación de Pearson\nCómo calcular una correlación de Pearson y graficarla"
  },
  {
    "objectID": "resource/01-resource.html#diagrama-de-dispersión-nube-de-puntos-o-scatterplot",
    "href": "resource/01-resource.html#diagrama-de-dispersión-nube-de-puntos-o-scatterplot",
    "title": "Práctico 1. Cálculo y reporte de Correlación",
    "section": "Diagrama de dispersión (nube de puntos o scatterplot)",
    "text": "Diagrama de dispersión (nube de puntos o scatterplot)\nSiempre es recomendable acompañar el valor de la correlación con una exploración gráfica de la distribución bivariada de los datos. El gráfico o diagrama de dispersión es una buena herramienta, ya que muestra la forma, la dirección y la fuerza de la relación entre dos variables cuantitativas.\nEste tipo de gráfico lo podemos realizar usando la librería ggplot2.\n\npacman::p_load(ggplot2)\nplot1 &lt;- ggplot(data, \n                aes(x=educ, y=ing)) + \n                geom_point(colour = \"red\", \n                size = 5)\nplot1\n\n\n\n\n\n\n\n\nEn el gráfico podemos ver como se crea una nube de puntos en las intersecciones de los valores para ambas variables de cada caso."
  },
  {
    "objectID": "resource/01-resource.html#el-cuarteto-de-anscombe-y-las-limitaciones-de-la-correlación-lineal",
    "href": "resource/01-resource.html#el-cuarteto-de-anscombe-y-las-limitaciones-de-la-correlación-lineal",
    "title": "Práctico 1. Cálculo y reporte de Correlación",
    "section": "El cuarteto de Anscombe y las limitaciones de la correlación lineal",
    "text": "El cuarteto de Anscombe y las limitaciones de la correlación lineal\nAhora, revisaremos un muy buen ejemplo de la importancia de la exploración gráfica de los datos mediante un ejemplo de Anscombe (1973), que permite visualizar las limitaciones del coeficiente de correlación.\nPrimero, crearemos la base de datos:\n\nanscombe &lt;- data.frame(\n  x1 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),\n  y1 = c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68),\n  x2 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),\n  y2 = c(9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74),\n  x3 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),\n  y3 = c(7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73),\n  x4 = c(8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8),\n  y4 = c(6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89)\n)\n\nCalculamos la correlación pares de datos\n\ncor(anscombe$x1, anscombe$y1)\n\n[1] 0.8164205\n\ncor(anscombe$x2, anscombe$y2)\n\n[1] 0.8162365\n\ncor(anscombe$x3, anscombe$y3)\n\n[1] 0.8162867\n\ncor(anscombe$x4, anscombe$y4)\n\n[1] 0.8165214\n\n\nPodemos observar que los valores de las correlaciones son equivalentes, por lo tanto podríamos pensar que todos los pares de columnas se encuentran correlacionados de manera similar.\nPero, ¿será suficiente con esa información? Pasemos a revisar los gráficos de dispersión de cada par de variables.\n\nggplot(anscombe, aes(x = x1, y = y1)) +\n  geom_point(colour = \"red\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso I\")\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x2, y = y2)) +\n  geom_point(colour = \"green\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso II\")\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x3, y = y3)) +\n  geom_point(colour = \"yellow\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso III\")\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x4, y = y4)) +\n  geom_point(colour = \"orange\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso IV\")\n\n\n\n\n\n\n\n\nComo vemos, con distintas distribuciones las correlaciones pueden ser las mismas, principalmente porque Pearson es una medida que solo captura relaciones lineales (rectas), además de verse influido fuertemente por valores extremos. Por lo mismo, es relevante siempre una buena visualización de la distribución bivariada de los datos como complemento al cálculo del coeficiente de correlación."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Programa",
    "section": "",
    "text": "Daniela Olivares Collío\n   FACSO - sala 328\n   &lt;a href=“mailto:danielaolivarescollio@gmail.com”&gt;danielaolivarescollio@gmail.com\n   Schedule an appointment\n\n\n\n   Kevin Carrasco Quintanilla\n   FACSO - sala 328\n   &lt;a href=“mailto:danielaolivarescollio@gmail.com”&gt;kevin.carrasco@ug.uchile.cl\n   kevincarrascoq1\n\n\n\n\n\n   Viernes\n   Marzo – Julio, 2025\n   09:00-11:30 hrs.\n   Sala 334, FACSO"
  },
  {
    "objectID": "syllabus.html#resumen",
    "href": "syllabus.html#resumen",
    "title": "Programa",
    "section": "Resumen",
    "text": "Resumen\nEste curso busca dar acercamiento a la investigación social cuantitativa, abarcando el análisis e interpretación de modelos explicativos de investigación social. Asimismo, se busca que los y las estudiantes logren familiarizarse con el uso de Rstudio para el análisis de datos sociales.\nLa metodología incluye clases lectivas y trabajo práctico en R."
  },
  {
    "objectID": "syllabus.html#objetivo-general",
    "href": "syllabus.html#objetivo-general",
    "title": "Programa",
    "section": "Objetivo general",
    "text": "Objetivo general\nAl finalizar el curso, el/la estudiante podrá elaborar y analizar diseños de investigación social de carácter cuantitativo, así como describir cuantitativamente un conjunto de datos utilizando el lenguaje R."
  },
  {
    "objectID": "syllabus.html#objetivos-específicos",
    "href": "syllabus.html#objetivos-específicos",
    "title": "Programa",
    "section": "Objetivos específicos",
    "text": "Objetivos específicos\nAl concluir el curso lo/as estudiantes deberán haber alcanzado los siguientes resultados de aprendizaje:\n\nConocer las etapas de un diseño de investigación social cuantitativa y sus principales elementos.\nFormular diseños de investigación social cuantitativa.\nConocer y aplicar instrumentos de medición y tipos de estudios cuantitativos.\nInterpretar y analizar los elementos centrales de una base de datos con información social.\nAplicar e interpretar técnicas de estadística descriptiva según las distintas características de los datos.\nAplicar e interpretar técnicas de estadística correlacional e inferencia estadística para variables con distinta unidad de medida.\nAplicar e interpretar técnicas de regresión lineal y logística para variables numéricas y variables categóricas."
  },
  {
    "objectID": "syllabus.html#contenidos",
    "href": "syllabus.html#contenidos",
    "title": "Programa",
    "section": "Contenidos",
    "text": "Contenidos\n\nMódulo 1: Inferencia y estadística correlacional\n1.1. Inferencia estadística y asociación\n\nÁrea de una distribución, probabilidad en la curva normal, error tipo 1 y tipo 2\nIntervalos de confianza para medias y proporciones usando distribución Z\nConcepto de valor-p\nDistribución t de Student y grados de libertad\n\n1.2. Asociación entre dos variables cuantitativas\n\nConcepto de covarianza y relación/correlación lineal y no-lineal\nCorrelación de Pearson\n\n1.3. Asociación con variables categóricas\n\nTablas de contingencia y determinación de la asociación\nAsociación poblacional mediante chi-cuadrado\n\n\n\nMódulo 3: Regresión lineal y regresión logística\n3.1 Regresión lineal de mínimos cuadrados\n\nAspectos centrales y supuestos de la regresión MCO\nInterpretación de coeficientes (variables cuantitativas y cualitativas) y efectos de interacción\nRepresentación gráfica de coeficientes de regresión lineal\n\n3.2 Regresión logística binaria\n\nAspectos básicos de la regresión logística\nTipos de coeficientes e interpretación\nRepresentación gráfica (cálculo de probabilidades predichas)"
  },
  {
    "objectID": "syllabus.html#metodología",
    "href": "syllabus.html#metodología",
    "title": "Programa",
    "section": "Metodología",
    "text": "Metodología\nEl curso se organiza en sesiones semanales, con una parte lectiva seguida de una práctica. En la parte lectiva se transmiten y discuten los conceptos centrales de la investigación cuantitativa. En la parte práctica se aplicarán los conceptos transmitidos en la parte lectiva, además de resolver dudas en el avance de los trabajos de investigación."
  },
  {
    "objectID": "syllabus.html#evaluación",
    "href": "syllabus.html#evaluación",
    "title": "Programa",
    "section": "Evaluación",
    "text": "Evaluación\nModalidad:\nUn reporte de investigación en parejas con dos entregas.\nPonderaciones:\n\nPrimera entrega 40%\nSegunda entrega 60%"
  },
  {
    "objectID": "syllabus.html#requisitos-de-aprobación",
    "href": "syllabus.html#requisitos-de-aprobación",
    "title": "Programa",
    "section": "Requisitos de aprobación",
    "text": "Requisitos de aprobación\n\nNota mínima de aprobación: 4,0 (en escala de 1 a 7).\nRequisitos de presentación a exámen: Nota entre 3,5 y 4,9\n\nAcerca del plagio: Cualquier información vertida en documentos calificables, que no se indique su debida procedencia, conociéndose de autor externo, y/o cualquier similitud, se considera plagio, conducente a la rendición del examen final."
  },
  {
    "objectID": "syllabus.html#palabras-clave",
    "href": "syllabus.html#palabras-clave",
    "title": "Programa",
    "section": "Palabras Clave",
    "text": "Palabras Clave\n\nEstadística, investigación cuantitativa, manipulación de datos, visualización de datos, interpretación de coeficientes"
  },
  {
    "objectID": "syllabus.html#bibliografía",
    "href": "syllabus.html#bibliografía",
    "title": "Programa",
    "section": "Bibliografía",
    "text": "Bibliografía\nWickham, Hadley & Grolemund, Garrett (2017). R for Data Science. Visualize, model, transform, tidy and import data. / Versión en español disponible acá\nMoore, D. S., & Comas, J. (2010). Estadística aplicada básica. Barcelona: Antoni Bosch.\nWooldridge, J. M. (2008). Introducción a la econometría: un enfoque moderno. Paraninfo Cengage Learning.\nCamarero, et al (2017) Regresión Logística: Fundamentos y aplicación a la investigación sociológica.\nHair, Joseph F., et al. (2004). Análisis multivariante. 5ta ed. Madrid: Prentice Hall.\nCharte, Francisco (2014). Análisis exploratorio y Visualización de datos con R."
  },
  {
    "objectID": "syllabus.html#recursos-web",
    "href": "syllabus.html#recursos-web",
    "title": "Programa",
    "section": "Recursos web",
    "text": "Recursos web\n\nSitio web del curso\nDescarga de R y Rstudio\nR4DS en español"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Planificación",
    "section": "",
    "text": "Los dos componentes centrales del curso son las clases teóricas y las actividades prácticas. Las clases se realizarán los días Viernes 09:00 a 10:50 en sala 334\n\nClases ( ): Lecturas, documentos de presentación y video (en caso que la sesión sea grabada)\nPrácticos y evaluaciones (): Actividades prácticas a desarrollar en clases y durante la semana.\nLecturas (): Llegar a la clase con los textos leídos.\n\n\n\n\n\n\n\n\n\n\n\n Clases\n Prácticas y evaluaciones\n Lecturas y material adicional\n\n\n\n\n\n\nUNIDAD 0: Introducción\n\n\n\n Marzo \n\n\n\n\n\nViernes 21\nIntroducción\nAproximación inicial a R\n- Leer detalladamente programa del curso\n\n\nViernes 28\nDocumentos Dinámicos y Markdown/Quarto\nDocumentos Dinámicos y Markdown/Quarto\n\n\n\n Abril \n\n\n\n\n\nViernes 04\nIntroducción a R/Rstudio y Estadística Descriptiva\n\n\n\n\nViernes 11\n1° Semana de integración\n\n\n\n\nViernes 18\nFeriado\n\n\n\n\n\n\n\nUNIDAD 1: Inferencia y Estadística Correlacional\n\n\n\nViernes 25\nCorrelación 1: Inferencia\n\n\n\n\n Mayo \n\n\n\n\n\nViernes 02\nInterferiado\n\n\n\n\nViernes 09\nCorrelación 2: Asociación\n\n\n\n\nViernes 16\n2° Semana de integración\n\n\n\n\nViernes 23\nSemana de pausa Escuela de Pregrado\n\n\n\n\nViernes 30\nCorrelación 3: Categóricas\n\n\n\n\n\n\n\nUNIDAD 2: Regresión lineal y regresión logística\n\n\n\n Junio \n\n\n\n\n\nViernes 06\nRegresión lineal 1\n\n\n\n\nViernes 13\n3° Semana de integración\n\n\n\n\nViernes 20\nFeriado\n\n\n\n\nViernes 27\nRegresión lineal 2\n\n\n\n\n Julio \n\n\n\n\n\nViernes 04\nRegresión logística\n\n\n\n\nViernes 11\nDocumentos Dinámicos y Markdown/Quarto\n\n\n\n\nViernes 18\n\nEvaluación\n\n\n\n—–\n—-\n———–\n———–"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Presentaciones, lecturas y actividades",
    "section": "",
    "text": "En esta sección se encuentran los documentos de presentación correspondientes a cada clase, lecturas y también actividades prácticas.",
    "crumbs": [
      "Clases",
      "Información general",
      "Presentaciones, lecturas y actividades"
    ]
  },
  {
    "objectID": "content/10-content.html#lecturas",
    "href": "content/10-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas"
  },
  {
    "objectID": "content/08-content.html#lecturas",
    "href": "content/08-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas"
  },
  {
    "objectID": "content/06-content.html#lecturas",
    "href": "content/06-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas\nR for Data Science"
  },
  {
    "objectID": "content/04-content.html#lecturas",
    "href": "content/04-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas\nR for Data Science"
  },
  {
    "objectID": "content/01-content.html#lecturas",
    "href": "content/01-content.html#lecturas",
    "title": "Clase 01",
    "section": "Lecturas",
    "text": "Lecturas\nR for Data Science",
    "crumbs": [
      "Clases",
      "Sesiones",
      "Sesión 1"
    ]
  },
  {
    "objectID": "assignment/index.html",
    "href": "assignment/index.html",
    "title": "Descripción Prácticos",
    "section": "",
    "text": "Las clases y guías prácticas de este curso se realizarán en el software R y su interfaz RStudio. A continuación dejamos una guía de instalación.\n\n\n\nAcceder a la página de posit, desarrollador del software desde octubre de 2022. Link directo a la descarga acá.\n\n\n\n\n\nEn posit se verán las opciones para descargar R y RStudio.\n\n\n\n\n\nSeleccionamos sistema operativo según corresponda\n\nDescargar R por primera vez y seguir instrucciones. En esta oportunidad trabajaremos con la última versión de R 4.3.2\n\n\n\n\n\n\nDescarga directa en la página de inicio\n\n\n\n\n\nInstalar ambos archivos siguiendo las instrucciones de instalación\n\n\n\n\n\n\nSe debería ver similar a esto (o con otro color de fondo):\n\n\n\n\n\n\n\n\nNota\n\n\n\nAlternativa a lo anterior: https://posit.cloud/\n\nVersión online de Rstudio\nUtilizar en caso de problemas con PC",
    "crumbs": [
      "Prácticos",
      "Información general prácticos",
      "Descripción Prácticos"
    ]
  },
  {
    "objectID": "assignment/index.html#guía-de-instalación-r-y-rstudio",
    "href": "assignment/index.html#guía-de-instalación-r-y-rstudio",
    "title": "Descripción Prácticos",
    "section": "",
    "text": "Las clases y guías prácticas de este curso se realizarán en el software R y su interfaz RStudio. A continuación dejamos una guía de instalación.\n\n\n\nAcceder a la página de posit, desarrollador del software desde octubre de 2022. Link directo a la descarga acá.\n\n\n\n\n\nEn posit se verán las opciones para descargar R y RStudio.\n\n\n\n\n\nSeleccionamos sistema operativo según corresponda\n\nDescargar R por primera vez y seguir instrucciones. En esta oportunidad trabajaremos con la última versión de R 4.3.2\n\n\n\n\n\n\nDescarga directa en la página de inicio\n\n\n\n\n\nInstalar ambos archivos siguiendo las instrucciones de instalación\n\n\n\n\n\n\nSe debería ver similar a esto (o con otro color de fondo):\n\n\n\n\n\n\n\n\nNota\n\n\n\nAlternativa a lo anterior: https://posit.cloud/\n\nVersión online de Rstudio\nUtilizar en caso de problemas con PC",
    "crumbs": [
      "Prácticos",
      "Información general prácticos",
      "Descripción Prácticos"
    ]
  },
  {
    "objectID": "assignment/index.html#recomendaciones-generales",
    "href": "assignment/index.html#recomendaciones-generales",
    "title": "Descripción Prácticos",
    "section": "2. Recomendaciones generales",
    "text": "2. Recomendaciones generales\nRespecto a las clases y guías prácticas, se recomienda lo siguiente:\n\nevitar uso de tilde, ñ, espacios y mayúsculas tanto en carpetas y archivos, así como también en los nombres de las variables\nal momento de hacer consultas sobre problemas en la ejecución del código, adjuntar la siguiente información:\n\nCódigo completo hasta que se produce el problema\nIndicar línea del código donde se produce el problema\nAdjuntar el resultado del output de la información de la sesión (sessionInfo())",
    "crumbs": [
      "Prácticos",
      "Información general prácticos",
      "Descripción Prácticos"
    ]
  },
  {
    "objectID": "content/03-content.html#lecturas",
    "href": "content/03-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas\nR for Data Science"
  },
  {
    "objectID": "content/05-content.html#lecturas",
    "href": "content/05-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas\nR for Data Science"
  },
  {
    "objectID": "content/07-content.html#lecturas",
    "href": "content/07-content.html#lecturas",
    "title": "Regresión lineal I",
    "section": "Lecturas",
    "text": "Lecturas\nMoore: 2. Análisis de relaciones (97-131)",
    "crumbs": [
      "Clases",
      "Sesiones",
      "Regresión lineal I"
    ]
  },
  {
    "objectID": "content/09-content.html#lecturas",
    "href": "content/09-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas"
  },
  {
    "objectID": "content/11-content.html#lecturas",
    "href": "content/11-content.html#lecturas",
    "title": "Presentación",
    "section": "Lecturas",
    "text": "Lecturas"
  },
  {
    "objectID": "resource/index.html",
    "href": "resource/index.html",
    "title": "Instrucciones generales",
    "section": "",
    "text": "Los prácticos consisten en el desarrollo de una guía práctica (por lo general cada semana de clases) donde se aplican y profundizan los contenidos de las clases, y donde también se abordan otras temáticas relacionadas al manejo y repote de datos."
  },
  {
    "objectID": "resource/index.html#descripción",
    "href": "resource/index.html#descripción",
    "title": "Instrucciones generales",
    "section": "",
    "text": "Los prácticos consisten en el desarrollo de una guía práctica (por lo general cada semana de clases) donde se aplican y profundizan los contenidos de las clases, y donde también se abordan otras temáticas relacionadas al manejo y repote de datos."
  },
  {
    "objectID": "resource/index.html#trabajo-con-software-r",
    "href": "resource/index.html#trabajo-con-software-r",
    "title": "Instrucciones generales",
    "section": "Trabajo con software R",
    "text": "Trabajo con software R\nPara los análisis estadísticos de este curso usamos el programa R, en parte porque es gratuito, pero la principal razón es que es de código abierto. Esto quiere decir que cualquier persona puede revisar cómo está hecho y aportar con modificaciones y procedimientos nuevos, como son las librerías que realizan funciones específicas.\nEl carácter de apertura de R posee muchas ventajas, pero también conlleva complicaciones. Se actualiza permanentemente, así como también las librerías, y esto puede generar problemas de compatibilidad y de fallas en ejecución del código de análisis.\nPara minimizar estos posibles problemas en este curso, vamos a:\n\ntrabajar con la misma y última versión de R, que es la 4.3 (Chequear con sessionInfo())\nevitar uso de tilde, ñ, espacios y mayúsculas tanto en carpetas y archivos, así como también en los nombres de las variables"
  },
  {
    "objectID": "resource/index.html#sobre-errores-y-consultas-respecto-a-problemas-con-r-y-ejecución-de-código",
    "href": "resource/index.html#sobre-errores-y-consultas-respecto-a-problemas-con-r-y-ejecución-de-código",
    "title": "Instrucciones generales",
    "section": "Sobre errores y consultas respecto a problemas con R y ejecución de código",
    "text": "Sobre errores y consultas respecto a problemas con R y ejecución de código\nEn caso de problemas con ejecución de código, se sugiere intentar solucionarlo autónomamente por no más de 10 minutos, si los problemas siguen entonces consultar.\nSe sugiere que las consultas sobre problemas en la ejecución del código y otros se realicen en los foros al final de los prácticos correspondientes, para lo cual se requiere solo habilitar una cuenta en Github. Al hacer la consulta, adjuntar la siguiente información:\n\nCódigo completo hasta que se produce el problema\nIndicar línea del código donde se produce el problema\nAdjuntar el resultado del output de la información de la sesión (sessionInfo())\n\n\nSobre el trabajo en hojas de código en RStudio\n\nEl trabajo de análisis en RStudio se efectua en una hoja de código (o R script o sintaxis, o para los usuarios de Stata la do-file), que es donde se anotan los comandos y funciones. Para abrir una hoja, en RStudio ir a File &gt; New File &gt; R Script (o ctrl+shift+N),y aparecerá un panel con una pestaña “Untitled” (sin título). Esta es la hoja de código donde se anotan los comandos.\nLos contenidos de las hojas de código son básicamente 2:\n\ncomandos o funciones: se escriben en la hoja, y para ejecutarlos se debe posicionar el cursor en la línea respectiva y ctrl+enter, el resultado aparecerá en el panel de resultados o Consola.\ntexto: para escribir títulos, comentarios, y todo lo que permita entender qué se está haciendo, al principio de la línea respectiva escribir el signo #\n\nPara grabar nuestra hoja de código y así respaldar nuestros análisis, File &gt; Save (o ctrl+s), y dar un nombre al archivo. Recordar: breve, sin espacios ni tildes ni eñes. Por defecto, la extensión de estos archivos es .R"
  },
  {
    "objectID": "trabajos.html",
    "href": "trabajos.html",
    "title": "Evaluaciones",
    "section": "",
    "text": "La presente evaluación tiene por objetivo que las y los estudiantes apliquen de forma integral los contenidos del curso a una temática de interés específica utilizando la base de datos del Estudio Social Longitudinal de Chile (ELSOC) 2022. Se espera un ejercicio de investigación que logre dar cuenta del aprendizaje de las herramientas de análisis estadístico que configuran las habilidades básicas para desarrollar procesos de investigación social, analizando fuentes de información de carácter cuantitativo desde una perspectiva sociológica.\n\n\n\nEl trabajo debe ser realizado en parejas.\nSe realizarán dos entregas: una parcial y una final tipo reporte de investigación breve.\nSe recomienda que elaboren un problema de investigación que trate sobre las características sociales de algún problema actual de la sociedad chilena, y que sea factible de investigar con los datos del Estudio Social Longitudinal de Chile (ELSOC) 2022."
  },
  {
    "objectID": "trabajos.html#presentación-y-directrices-generales",
    "href": "trabajos.html#presentación-y-directrices-generales",
    "title": "Evaluaciones",
    "section": "",
    "text": "La presente evaluación tiene por objetivo que las y los estudiantes apliquen de forma integral los contenidos del curso a una temática de interés específica utilizando la base de datos del Estudio Social Longitudinal de Chile (ELSOC) 2022. Se espera un ejercicio de investigación que logre dar cuenta del aprendizaje de las herramientas de análisis estadístico que configuran las habilidades básicas para desarrollar procesos de investigación social, analizando fuentes de información de carácter cuantitativo desde una perspectiva sociológica.\n\n\n\nEl trabajo debe ser realizado en parejas.\nSe realizarán dos entregas: una parcial y una final tipo reporte de investigación breve.\nSe recomienda que elaboren un problema de investigación que trate sobre las características sociales de algún problema actual de la sociedad chilena, y que sea factible de investigar con los datos del Estudio Social Longitudinal de Chile (ELSOC) 2022."
  },
  {
    "objectID": "trabajos.html#instrucciones-para-el-informe",
    "href": "trabajos.html#instrucciones-para-el-informe",
    "title": "Evaluaciones",
    "section": "Instrucciones para el Informe",
    "text": "Instrucciones para el Informe\nPara la entrega del informe se espera que, habiendo seleccionado un fenómeno específico de interés, se desarrollen los siguientes temas:\n\n\n\n\n\n\n\n\nComponente\nDetalle\nPuntaje\n\n\n\n\nFormulación del problema\nFormular una pregunta y un objetivo general de investigación. Fundamentar el interés sociológico que habilita el estudio de la temática elegida, utilizando al menos 3 referencias bibliográficas. El objetivo de investigación formulado debe permitir entender cómo se analizará la temática elegida. (1 plana)\n2.0\n\n\nDefinición base de datos\nDefinir la fuente de información a utilizar indicando el nombre de la base de datos y la institución que la disponibiliza, la población que busca representar el estudio, el procedimiento de muestreo utilizado y el tamaño de la muestra. Dado que trabajaremos solo con el Estudio Social Longitudinal de Chile (ELSOC) 2022, se espera que las parejas describan qué busca medir y representar el módulo (o selección de variables) elegido. (½ plana)\n1.0\n\n\nOperacionalización y descripción de variables\nOperacionalizar correctamente las variables de interés. Esta sección también incluye una tabla de descriptivos básicos, y una descripción detallada de la operacionalización y medición de las variables. - Tabla de descriptivos: etiquetas claras, debe ser posible identificar cada variable - Descripción de la operacionalización de cada variable y recodificación apropiada (por ejemplo, de menos a más presencia del atributo medido)\n3.0\n\n\nInferencia y Correlación\nEstimar, visualizar e interpretar los coeficientes de correlación entre variables. - Matriz de correlaciones (tabla): 1 - interpretación de tamaño de efecto: 1 - inferencia: 1\n3.0\n\n\nRegresión lineal\nEstimar, visualizar e interpretar coeficientes de regresión lineal - tabla de regresión: 0.5 - interpretación de coeficientes (interpretación de efectos de interacción es opcional, pero se valorará positivamente): 1 - inferencia/significación: 1 - ajuste global del modelo (R2): 0.5.\n3.0\n\n\nRegresión logística\nEstimar, visualizar e interpretar coeficientes de regresión logística - tabla de regresión: 0.5 - interpretación coeficientes (interpretación de efectos de interacción es opcional, pero se valorará positivamente): 1 - inferencia: 1 - ajuste: 0.5\n3.0\n\n\nConclusiones\nEn las conclusiones se deben sintetizar los aspectos más relevantes de los análisis estadísticos ya expuestos, articulando toda la información trabajada debe presentarse una respuesta tentativa a la pregunta de investigación. Además, debe reflexionar sobre los límites de su diseño de investigación y señalar posibles ejes de investigación que podrían ser considerados en futuras indagaciones (1 plana).\n2.0\n\n\nTrabajo en R\nAdemás del informe, entregar la carpeta con el Proyecto R, que contenga un archivo .Rproject, y las carpetas input, procesamiento y output. - La carpeta input debe contener la base de datos, manual de usuario, libro de códigos. - La carpeta procesamiento debe contener un archivo de sintaxis para el procesamiento y otro para el análisis de los datos. - La carpeta output debe contener la base de datos procesada y el resto de salidas asociadas (tablas, gráficos, etc.). - Se evaluará que los códigos utilizados generen las salidas (tablas, gráficos, etc.) que se presentan en el informe.\n1.0"
  },
  {
    "objectID": "trabajos.html#fecha-y-formato-de-entrega",
    "href": "trabajos.html#fecha-y-formato-de-entrega",
    "title": "Evaluaciones",
    "section": "Fecha y formato de entrega",
    "text": "Fecha y formato de entrega\nEl formato de cada informe debe ser un documento en formato PDF (.pdf), que debe estar alojado en la carpeta output del Proyecto R.\nEntrega parcial:\n\nComprende los componentes: 1, 2, 3, 4 y 8.\nFecha entrega: Viernes 13 de junio, 23:59 hrs. vía módulo Tareas en plataforma U-Cursos.\nEntregas atrasadas hasta las 02:00 del sábado 14 de junio tendrán 0,5 puntos de descuento sobre la nota final.\nEntregas atrasadas hasta el sábado 14 de julio hasta las 23:59 tendrán 1,0 punto de descuento sobre la nota final. No serán evaluadas entregas posteriores a esta fecha.\n\nEntrega final:\n\nComprende los componentes: corrección de 1, 2, 3, 4, además de 5, 6, 7 y 8.\nFecha entrega: Viernes 18 de julio, 23:59 hrs. vía módulo Tareas en plataforma U-Cursos.\nEntregas atrasadas hasta las 02:00 del sábado 19 de julio tendrán 0,5 puntos de descuento sobre la nota final.\nEntregas atrasadas hasta el sábado 19 de julio hasta las 23:59 tendrán 1,0 punto de descuento sobre la nota final. No serán evaluadas entregas posteriores a esta fecha."
  },
  {
    "objectID": "trabajos.html#aspectos-formales",
    "href": "trabajos.html#aspectos-formales",
    "title": "Evaluaciones",
    "section": "Aspectos formales",
    "text": "Aspectos formales\nPara la construcción del reporte de investigación por favor considere:\n\nEl trabajo debe tener una portada que incluya: título, logo de la universidad, nombre de los estudiantes, profesor/a, fecha y ayudantes.\nDebe incluirse un índice. Tablas, referencias y bibliografía utilizada debe presentarse en formato APA.\nLa fuente a utilizar debe ser Letra Times New Roman 12, interlineado simple y justificado. Notas a pie de página en tamaño 10, en mismo formato que el texto central.\nEl trabajo debe tener una redacción adecuada y sin errores de ortografía.\nSe descontarán hasta 5 décimas sobre la nota final por errores de este tipo."
  },
  {
    "objectID": "trabajos.html#sobre-plagio",
    "href": "trabajos.html#sobre-plagio",
    "title": "Evaluaciones",
    "section": "Sobre plagio",
    "text": "Sobre plagio\nTodos los trabajos se procesan en software para detección de plagio: evidencia de una situación de plagio implica obtención de la nota mínima en la evaluación (1,0) junto con constituirse como causal de reprobación de la asignatura."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Metodología Cuantitativa Avanzada\n        ",
    "section": "",
    "text": "Metodología Cuantitativa Avanzada\n        \n        \n            Magister en Ciencias Sociales, mención Sociología de la Modernización\n        \n        \n            MCS7171-1 • Primer Semestre 2025Departamento de Sociología FACSOUniversidad de Chile\n        \n    \n    \n        \n    \n\n\n\n\n\n\nProfesores\n\n\n\n\n   Daniela Olivares Collío\n   FACSO - sala 328\n   &lt;a href=“mailto:danielaolivarescollio@gmail.com”&gt;danielaolivarescollio@gmail.com\n   \n\n\n\n\n\n\n   Kevin Carrasco Quintanilla\n   FACSO - sala 328\n   &lt;a href=“mailto:danielaolivarescollio@gmail.com”&gt;kevin.carrasco@ug.uchile.cl\n   kevincarrascoq1\n\n\n\nInformación del curso\n\n   Viernes\n   Marzo – Julio, 2025\n   09:00-11:30 hrs.\n   Sala 334, FACSO \n\n\n\n\nContacto\nA través de correo o U-Cursos\n\n\nVersiones anteriores del curso\n\n2024"
  },
  {
    "objectID": "assignment/03-practico.html",
    "href": "assignment/03-practico.html",
    "title": "Práctico 03. Conocimientos básicos de programación en R",
    "section": "",
    "text": "El objetivo de esta guía práctica es introducirnos en los procedimientos básicos del uso del lenguaje y ambiente R.\nEn detalle, aprenderemos:\n\nLenguaje orientado a objetos\nOperadores en R\nTipos de datos\nCargar librerías\nImportar datos\n\n\n\n\n\n\n\nNota\n\n\n\nRevisemos algunos conocimientos básicos para la programación en R. Pero antes, tengamos dos cosas en mente:\n\nPrimero, ¿qué es codificar?, en programación codificar corresponde a un proceso de entrega de instrucciones en un lenguaje específico, siguiendo un orden lógico y coherente.\nSegundo, de aquí en adelante nos manejaremos con una máxima en el curso; existe un acuerdo implícito entre tú y R: R hará todos los cálculos por ti, pero en cambio tú debes dar las instrucciones con total precisión.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 03: Conocimientos básicos R"
    ]
  },
  {
    "objectID": "assignment/03-practico.html#objetivos-de-la-práctica",
    "href": "assignment/03-practico.html#objetivos-de-la-práctica",
    "title": "Práctico 03. Conocimientos básicos de programación en R",
    "section": "",
    "text": "El objetivo de esta guía práctica es introducirnos en los procedimientos básicos del uso del lenguaje y ambiente R.\nEn detalle, aprenderemos:\n\nLenguaje orientado a objetos\nOperadores en R\nTipos de datos\nCargar librerías\nImportar datos\n\n\n\n\n\n\n\nNota\n\n\n\nRevisemos algunos conocimientos básicos para la programación en R. Pero antes, tengamos dos cosas en mente:\n\nPrimero, ¿qué es codificar?, en programación codificar corresponde a un proceso de entrega de instrucciones en un lenguaje específico, siguiendo un orden lógico y coherente.\nSegundo, de aquí en adelante nos manejaremos con una máxima en el curso; existe un acuerdo implícito entre tú y R: R hará todos los cálculos por ti, pero en cambio tú debes dar las instrucciones con total precisión.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 03: Conocimientos básicos R"
    ]
  },
  {
    "objectID": "assignment/03-practico.html#lenguaje-orientado-a-objetos",
    "href": "assignment/03-practico.html#lenguaje-orientado-a-objetos",
    "title": "Práctico 03. Conocimientos básicos de programación en R",
    "section": "1. Lenguaje orientado a objetos",
    "text": "1. Lenguaje orientado a objetos\nR es un lenguaje de programación orientado a objetos. ¿Qué significa eso?, implica que podemos crear elementos dentro del ambiente de R, a los cuales les asignaremos información que quedará almacenada, información que puede ir desde números, palabras, cálculos hasta bases de datos.\nTodas las instrucciones en R en las que crees objetos, es decir, instrucciones de asignación, tendrán la misma estructura:\nnombre_objeto &lt;- valor\nEl asignador &lt;- se utiliza para crear objetos y forma parte de uno de los operadores más usados en R.\nVeamos algunos ejemplos de creación de objetos.\n\n1.1. Objetos simples: valores\nLos elementos que podemos asignar a objetos son múltiples, como números, palabras acompañadas siempre de corchetes \" \"\n\nx &lt;- 4 # asignar\n\nx # ejecutar\n\n[1] 4\n\ny &lt;- \"Hola mundo\" # los carácteres alfabéticos siempre van acompañados de corchetes\n\ny \n\n[1] \"Hola mundo\"\n\n\nHagamos un pequeño reto: ¿Cuál es el valor de a y b? Si a &lt;- 5; b &lt;- a; a &lt;- 4\n\na &lt;- 5\nb &lt;- a\na &lt;- 4\n\nprint(a) # imprimir en la consola\n\n[1] 4\n\nprint(b)\n\n[1] 5\n\na + 10\n\n[1] 14\n\n\nAhora, sea z = a^2 ¿qué resultado obtenemos de a * b + z?\n\nz &lt;- a^2 # asignar\n\na * b + z\n\n[1] 36\n\n\n\n\n1.2. Vectores\nLos vectores corresponden a un conjunto o secuencia de elementos del mismo tipo definidos por la funcion de concatenar: c().\n\nedad &lt;- c(18,22,36,19,35) # concatenar (variable de razon)\n\nedad\n\n[1] 18 22 36 19 35\n\ngenero &lt;- c(3,1,1,2,3) # masculino = 1; femenino = 2; transgenero = 3 (variable nominal)\n\ngenero \n\n[1] 3 1 1 2 3\n\ngse &lt;- c(\"ABC1\", \"C2\", \"E\", \"AbC1\", \"E\")  # tambíen se pueden usar carácteres (variable ordinal)\n\ngse\n\n[1] \"ABC1\" \"C2\"   \"E\"    \"AbC1\" \"E\"   \n\n\n\n\n1.3. Data.frames\nAdemás de lo anterior, en R es fundamental la creación de data.frames. Un data.frame es una estructura de datos de dos dimensiones (columnas y filas), donde las columnas pueden ser de diferente naturaleza, pero deben tener el mismo largo. A partir de ella agrupamos variables en una matriz, o sea, construimos una base de datos. Es como “pegar” las columnas (variables) una al lado de otra.\nCreemos un data.frame con los vectores que ya creamos antes. Para crear el data.frame usamos la función que lleva el mismo nombre, colocando dentro del paréntesis los vectores que creamos anteriormente: data.frame(vector1, vector2, vectorX).\n\nbase1 &lt;- data.frame(genero, gse, edad) # Se ve como objeto de \"datos\" en entorno.\n\nbase1\n\n  genero  gse edad\n1      3 ABC1   18\n2      1   C2   22\n3      1    E   36\n4      2 AbC1   19\n5      3    E   35\n\n\nAhora, creemos un data.frame desce cero. En este ejemplo, crearemos los vectores dentro de la función data.frame().\n\n# Ejemplo de como crear un data.frame desde 0: \nbase2 &lt;- data.frame(Sexo=c(\"H\",\"M\",\"H\",\"M\",\"H\",\"M\"),\n                    Estatura=c(1.83,1.76,1.82,1.60,1.90,1.66),\n                    Peso=c(67,58,66,48,75,55))\n\nbase2 \n\n  Sexo Estatura Peso\n1    H     1.83   67\n2    M     1.76   58\n3    H     1.82   66\n4    M     1.60   48\n5    H     1.90   75\n6    M     1.66   55",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 03: Conocimientos básicos R"
    ]
  },
  {
    "objectID": "assignment/03-practico.html#operadores-en-r",
    "href": "assignment/03-practico.html#operadores-en-r",
    "title": "Práctico 03. Conocimientos básicos de programación en R",
    "section": "2. Operadores en R",
    "text": "2. Operadores en R\nAntes de trabajar con datos, debemos conocer el concepto de operadores. Estos símbolos no son de uso exclusivo en R, pero no todos tienen el mismo significado que en otros softwares.\nLos operadores son símbolos que permiten, en los distintos procedimientos de procesamiento, simplificar procesos. Por ejemplo, serán útilizados cuando filtremos nuestros datos para personas de ciertas categorías, cuando calculemos variables nuevas (de manera aritmética o condicional) o, simplemente, cuando queramos hacer procesos “concatenados”.\n\nVeamos algunos ejemplos:\n\n20 == 5 # igualdad\n\n[1] FALSE\n\n30 &gt;= 14 # mayor o igual que\n\n[1] TRUE\n\n22 &lt;= 2 # menor o igual que\n\n[1] FALSE\n\n25 != 10 # no es igual a\n\n[1] TRUE\n\np = 10; y = 5; p &lt;= y # operatoria en objetos\n\n[1] FALSE",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 03: Conocimientos básicos R"
    ]
  },
  {
    "objectID": "assignment/03-practico.html#tipos-de-datos",
    "href": "assignment/03-practico.html#tipos-de-datos",
    "title": "Práctico 03. Conocimientos básicos de programación en R",
    "section": "3. Tipos de datos",
    "text": "3. Tipos de datos\n\n3.1 ¿Qué son los tipos de datos?\nEn R, al igual que en la mayoría de lenguajes de programación, contamos con datos de diversos tipos, en razón de los cuales podemos realizar determinados procedimientos de tratamiento o análisis.\nLos tipos de datos están íntimamente relacionados con el nivel de medición de las variables a las que corresponden. La teoría de los niveles de medición contempla cuatro tipos:\n\n\n\n3.2 ¿Qué tipos de datos encontramos en R?\nEn R encontramos una clasificación de los tipos de datos que tiene un correlato con los niveles de medición de las variables.\nPara mostrar un ejemplo de los tipos de datos con casos reales cargaremos la base de datos de ELSOC, pero en los puntos 4 y 5 mostraremos en detalle como cargar librerías y bases de datos.\n\nlibrary(pacman)\nload(\"input/data/ELSOC_W05_v1.0_R.RData\")\n\n\nI) Character\nLos datos character están directamente asociados a las variables cualitativas (o categóricas). Generalmente suelen ser variables de texto abierto, como es el caso de la variable pais, que detalla el país de procedencia de la persona encuestada.\n\n\n\n\n\n\nNota\n\n\n\nPara conocer cuál es el tipo de variable en R, utilizamos el comando class().\nPara detallar dentro de la base de datos cuál es la variable de interés, utilizamos el símbolo $ posterior a la base de datos. Siempre es la misma estructura: base$variable\n\n\n\nclass(elsoc_2021$comuna) \n\n[1] \"character\"\n\n\nSin embargo, estas variables no tienden a ser las mejores a la hora de presentar nuestros resultados. Como solución, tenemos las variables de tipo Factor.\n\n\nII) Factor\nLas variables de tipo factor son ideales para trabajar con variables de tipo nominal u ordinal. Esto es así debido a que permiten establecer un orden entre las categorías de la variable, lo cual es fundamental si trabajamos, por ejemplo, con variables nominales como el sexo de los encuestados, o si trabajamos con variables ordinales como su ideología política.\n\nclass(elsoc_2021$m0_sexo)\n\n[1] \"factor\"\n\nclass(elsoc_2021$m38) # religion\n\n[1] \"factor\"\n\n\n\n\nIII) Numeric\nLas variables de tipo numeric son variables de tipo númerica, las cuales pueden ser intervalares o de razón. Así, por ejemplo, cuando trabajamos con variables de razón trabajamos con variables como el número de hijos o la edad (aunque sería extraño encuestar a alguien con 0 años).\n\nclass(elsoc_2021$m0_edad)\n\n[1] \"numeric\"",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 03: Conocimientos básicos R"
    ]
  },
  {
    "objectID": "assignment/03-practico.html#cargar-librerías",
    "href": "assignment/03-practico.html#cargar-librerías",
    "title": "Práctico 03. Conocimientos básicos de programación en R",
    "section": "4. Cargar librerías",
    "text": "4. Cargar librerías\nEn R se trabaja a partir de paquetes (packages). ¿Qué son? De forma resumida, los paquetes son un conjunto de funciones o herramientas que pueden ser usadas en R. Los directorios de R donde se almacenan los paquetes se denominan librerías. La lógica es instalar paquetes y luego cargar (o llamar) las librerías cada vez que es necesario usarlas.\nUsualmente para cargar paquetes lo hacemos de la siguiente manera:\ninstall.packages(\"paquete\")\nlibrary(paquete)\nPero en esta ocasión utilizaremos un paquete llamado pacman, que facilita y agiliza la lectura (instalación y carga) de los paquetes a utilizar en R. De esta forma lo instalamos 1 única vez así:\n\ninstall.packages(\"pacman\")\nlibrary(pacman)\n\nLuego instalaremos y cargaremos los paquetes de R de la siguiente manera, volviendo más eficiente el procedimiento de carga de paquetes.\nEn este práctico utilizaremos cuatro paquetes\n\npacman: este facilita y agiliza la lectura de los paquetes a utilizar en R\ndplyr: nos permite seleccionar variables de un set de datos\nhaven: cargar y exportar bases de datos en formatos .sav y .dta\ncar: para recodificar/agrupar valores de variables\n\n\npacman::p_load(dplyr, \n               haven,\n               car)\n\n\n\n\n\n\n\nNota\n\n\n\nComo se puede ver, antes de la función p_load hay un ::, esto se refiere a que se “fuerza” que esa función provenga de ese paquete (en este caso del paquete pacman).",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 03: Conocimientos básicos R"
    ]
  },
  {
    "objectID": "assignment/03-practico.html#importar-datos",
    "href": "assignment/03-practico.html#importar-datos",
    "title": "Práctico 03. Conocimientos básicos de programación en R",
    "section": "5. Importar datos",
    "text": "5. Importar datos\nEn R es es posible importar y exportar datos que se encuentren en cualquier formato: ya sea .csv, .dta, .sav, .xlsx y, por supuesto, .rds y .RData. Sin embargo, para poder hacerlo, lo primero es instalar y cargar las librerías que contienen las funciones necesarias para la importación de distintos tipos de archivos.\nPero, ¿dónde están mis datos? Como hemos mencionado, nuestros datos los dejaremos en la carpeta input/data de nuestro proyecto. La base con la que trabajaremos en este práctico pueden encontrarla en la página oficial de ELSOC en este enlace.\nLuego de descargar la base de datos, asegurate de dejar el archivo .sav en la carpeta input/data de tu proyecto.\nUna vez descargados los datos, procedemos a importar nuestra base de datos. Para ello, en nuestro script, dejamos indicado que a partir de la lectura de los datos con load(), crearemos un objeto que contiene la base de datos. Fijate en el Enviroment, ya que si lo anterior se logra, el objeto aparecerá allí.\nLa estructura general para importar datos es la siguiente:\nread_*(\"ruta_hacia_archivo/nombre_archivo.*\")\nSin embargo, por esta vez podemos descargar la base desde internet\n\nload(url((\"https://github.com/cursos-metodos-facso/metod1-MCS/raw/main/resource/files/ELSOC_W05_v1.0_R.RData\"))\n\n\n\n\n\n\n\nNota\n\n\n\nPara importar los datos en R debemos tener en consideración tres cosas:\n\nCómo se llaman los datos (en nuestro caso ELSOC_W05_v1.0_SPSS)\nEl formato de nuestros datos (en nuestro caso .sav)\nEl lugar de donde están alojados nuestros datos\n\n\n\n\n5.1. Importar datos en otros formatos\nNo siempre nuestros datos vendrán en un único formato. Para ello, R cuenta con otras formas de leer distintos tipos de formatos.\n\nreadxl para archivos .xlsx\nhaven para archivos .sav\nreadr para .csv",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 03: Conocimientos básicos R"
    ]
  },
  {
    "objectID": "assignment/06-practico.html",
    "href": "assignment/06-practico.html",
    "title": "Práctico 6. Asociación con variables categóricas",
    "section": "",
    "text": "El objetivo de esta guía práctica es introducirnos en técnicas de asociación entre variables categóricas, aplicando lo apredendido hasta ahora sobre inferencia estadística.\nEn detalle, aprenderemos:\n\nGenerar y analizar tablas de contingencia (o cruzadas)\nEstimar e interpretar la prueba de Chi-cuadrado\nAplicar coeficientes de correlación entre variables categóricas",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 06: Correlación categóricas"
    ]
  },
  {
    "objectID": "assignment/06-practico.html#objetivo-de-la-práctica",
    "href": "assignment/06-practico.html#objetivo-de-la-práctica",
    "title": "Práctico 6. Asociación con variables categóricas",
    "section": "",
    "text": "El objetivo de esta guía práctica es introducirnos en técnicas de asociación entre variables categóricas, aplicando lo apredendido hasta ahora sobre inferencia estadística.\nEn detalle, aprenderemos:\n\nGenerar y analizar tablas de contingencia (o cruzadas)\nEstimar e interpretar la prueba de Chi-cuadrado\nAplicar coeficientes de correlación entre variables categóricas",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 06: Correlación categóricas"
    ]
  },
  {
    "objectID": "assignment/06-practico.html#aplicación-práctica",
    "href": "assignment/06-practico.html#aplicación-práctica",
    "title": "Práctico 6. Asociación con variables categóricas",
    "section": "0. Aplicación práctica",
    "text": "0. Aplicación práctica\nEn esta práctica trabajaremos con un subconjunto de datos previamente procesados de la Encuesta de Caracterización Socioeconómica (CASEN) del año 2022, elaborada por el Ministerio de Desarrollo Social y Familia. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienen la opción de acceder a la misma información a través del siguiente enlace:  CASEN 20222. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos CASEN 2022.\n\n0.1. Carga de librerías y datos datos\nComencemos por preparar nuestros datos. Iniciamos cargando las librerías necesarias.\n\npacman::p_load(tidyverse, # Manipulacion datos\n               sjPlot, # Tablas\n               psych, # Correlaciones\n               DescTools, # Tablas\n               gginference, # Visualizacion \n               rempsyc, # Reporte\n               broom) # Varios\n\noptions(scipen = 999) # para desactivar notacion cientifica\nrm(list = ls()) # para limpiar el entorno de trabajo\n\nCargamos los datos directamente desde internet.\n\nload(url(\"https://github.com/cursos-metodos-facso/datos-ejemplos/raw/main/casen_proc.RData\")) #Cargar base de datos\n\ndim(proc_casen)\n\n[1] 3000   29\n\n\nContamos con 29 variables (columnas) y 3000 observaciones (filas).",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 06: Correlación categóricas"
    ]
  },
  {
    "objectID": "assignment/06-practico.html#tablas-de-contingencia",
    "href": "assignment/06-practico.html#tablas-de-contingencia",
    "title": "Práctico 6. Asociación con variables categóricas",
    "section": "1. Tablas de contingencia",
    "text": "1. Tablas de contingencia\nUna tabla de contingencia es una de las maneras más simples y útiles para representar el cruce entre dos variables categóricas.\nCon ella, podemos obtener en las celdas las frecuencias conjuntas entre ambas variables, es decir, cuántos casos de una determinada categoría de la variable Y ocurren conjuntamente con una determinada categoría de la variable X.\nAdemás, podemos presentar los totales de cada fila y columna al exterior de la tabla, también conocidas como frecuencias marginales.\nVeamos un ejemplo con ss_salud y universitaria:\n\nsjPlot::sjt.xtab(var.row = proc_casen$ss_salud, var.col = proc_casen$universitaria, \n                 show.summary = F, emph.total = T)\n\n\n\n\n\n\n\n\n\n\ns13. ¿A qué sistema\nprevisional de salud\npertenece?\nEducación superior\nalcanzada (si/no)\nTotal\n\n\nNo\nSí\n\n\n1. Sistema Público\nFONASA\n2027\n524\n2551\n\n\n2. Isapre\n111\n167\n278\n\n\n3. FF.AA. y del\nOrden\n37\n22\n59\n\n\n4. Ninguno\n(particular)\n61\n15\n76\n\n\nTotal\n2236\n728\n2964\n\n\n\n\n\nSumado a esto, tenemos:\n\nFrecuencias absolutas: números que aparencen en la tabla (ya sean conjuntas o marginales)\nFrecuencias porcentuales:\n\nporcentaje fila: % que cada frecuencia conjunta representa sobre la marginal de su fila\nporcentaje columna: % que cada frecuencia conjunta representa sobre la marginal de su columna\nporcentaje total: % que cada frecuencia conjunta representa sobre el número total de casos de la tabla\n\n\nVeamos cómo incorporar el porcentaje fila y columna en la tabla.\n\nsjPlot::sjt.xtab(var.row = proc_casen$ss_salud, \n                 var.col = proc_casen$universitaria, \n                 show.summary = F, \n                 emph.total = T, \n                 show.row.prc = T, # porcentaje fila\n                 show.col.prc = T # porcentaje columna\n                 )\n\n\n\n\n\n\n\n\n\n\ns13. ¿A qué sistema\nprevisional de salud\npertenece?\nEducación superior\nalcanzada (si/no)\nTotal\n\n\nNo\nSí\n\n\n1. Sistema Público\nFONASA\n2027\n79.5 %\n90.7 %\n524\n20.5 %\n72 %\n2551\n100 %\n86.1 %\n\n\n2. Isapre\n111\n39.9 %\n5 %\n167\n60.1 %\n22.9 %\n278\n100 %\n9.4 %\n\n\n3. FF.AA. y del\nOrden\n37\n62.7 %\n1.7 %\n22\n37.3 %\n3 %\n59\n100 %\n2 %\n\n\n4. Ninguno\n(particular)\n61\n80.3 %\n2.7 %\n15\n19.7 %\n2.1 %\n76\n100 %\n2.6 %\n\n\nTotal\n2236\n75.4 %\n100 %\n728\n24.6 %\n100 %\n2964\n100 %\n100 %\n\n\n\n\n\nAquí, los porcentajes fila aparecen en azul y los porcentajes columna en verde.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 06: Correlación categóricas"
    ]
  },
  {
    "objectID": "assignment/06-practico.html#prueba-de-hipótesis-con-chi-cuadrado",
    "href": "assignment/06-practico.html#prueba-de-hipótesis-con-chi-cuadrado",
    "title": "Práctico 6. Asociación con variables categóricas",
    "section": "2. Prueba de hipótesis con Chi-cuadrado",
    "text": "2. Prueba de hipótesis con Chi-cuadrado\nLa prueba de Chi-cudrado (\\(\\chi^2\\)) es una herramienta estadística utilizada para evaluar si existe una asociación significativa entre dos variables categóricas.\nTesteamos la hipótesis de independencia en una tabla de contigencia. Por ejemplo, si decimos que existe una relación entre tener educación superior y la preferencia por algún sistema de salud, esperamos encontrar que un porcentaje más alto de quienes tienen educación superior se inclinen por un sistema de salud determinado.\nSe basa en la comparación de las frecuencias observadas en una tabla de contingencia con las frecuencias esperadas si las variables fueran independientes. Si existe una gran diferencia entre las esperadas y las observadas, podemos suponer que hay una relación entre variables.\n\n\n\n\n\n\nPrueba de Chi-cuadrado\n\n\n\nContrastamos la hipótesis nula (o de trabajo) de que las variables son idenpendientes entre ellas: \\[  H_{0}: \\pi_{fc} =  \\pi_{f}\\pi_{c} \\]\nEn relación a una hipótesis alternativa sobre que las variables están relacionadas: \\[  H_{A}:  \\pi_{fc} \\neq  \\pi_{f}\\pi_{c} \\]\n\n\nVeamos un ejemplo con nuestros datos. Evaluemos si el nivel educacional se relaciona con el tipo de sistema de salud al que pertenecen las personas en Chile durante el 2022.\nApliquemos cinco pasos para inferencia.\n\nFormulamos nuestras hipótesis:\n\n\n\\(H_{0}\\): No hay asociación entre el nivel educacional y la preferencia por un sistema de salud.\n\\(H_{A}\\): El nivel educacional se asocia con la preferencia por un sistema de salud.\n\n\nGenerar tabla de contingencia con frecuencias observadas y esperadas:\n\n\nconti_table &lt;- table(proc_casen$ss_salud, proc_casen$universitaria)\nconti_table\n\n   \n       0    1\n  1 2027  524\n  2  111  167\n  3   37   22\n  4   61   15\n\n\nAdemás, podemos añadir las frecuencias marginales.\n\nrow_sum &lt;- margin.table(conti_table, 1)\ncol_sum &lt;- margin.table(conti_table, 2)\n\nconti_added &lt;- cbind(conti_table, as.vector(row_sum))\nconti_added &lt;- rbind(conti_added, c(as.vector(col_sum), sum(conti_table)))\nconti_added\n\n     0   1     \n1 2027 524 2551\n2  111 167  278\n3   37  22   59\n4   61  15   76\n  2236 728 2964\n\n\nAhora, generamos una tabla de contingencia con frecuencias esperadas.\nLas frecuencias observadas corresponden a:\n\\[ f_{e_c} = \\frac{{\\text{(Total marginal de fila para la celda)} \\times \\text{(Total marginal de columna para la celda)}}}{{N}} \\]\nUsando nuestros datos la frecuencia esperada para la primera celda sería:\n\\[ f_{e_c} = \\frac{(2551)*(2236)}{{2964}} = 1924.4\\]\nEsto debemos repetirlo para cada celda… pero podemos hacerlo más rápido con DescTools.\n\nexp_table &lt;- DescTools::ExpFreq(conti_added, freq = \"abs\")\nround(exp_table, 1)\n\n       0     1     \n1 1924.4 626.6 2551\n2  209.7  68.3  278\n3   44.5  14.5   59\n4   57.3  18.7   76\n  2236.0 728.0 2964\n\n\n\nCalcular el valor estimado de la prueba:\n\nEstablecemos la diferencia entre lo observado y lo esperado, siendo:\n\\[\\chi^2=\\sum\\frac{(f_o-f_e)^2}{f_e}\\]\n\nchi_stat &lt;- sum(\n  (2027-1924.4)^2/1924.4,\n  (524-626.6)^2/626.6,\n  (111-209.7)^2/209.7,\n  (167-68.3)^2/68.3,\n  (37-44.5)^2/44.5,\n  (22-14.5)^2/14.5,\n  (61-57.3)^2/57.3,\n  (15-18.7)^2/18.7\n  )\nchi_stat\n\n[1] 217.4706\n\n\n\nEspecifica el valor crítico de la prueba:\n\n\ndf &lt;- (4-1)*(2-1) # definimos grados de libertad\n\nchi_critico &lt;- qchisq(p = 0.05, df, lower.tail = F)\nchi_critico\n\n[1] 7.814728\n\n\n\nContrasta el valor estimado con el crítico e interpreta los resultados:\n\n\nchi_stat &gt; chi_critico\n\n[1] TRUE\n\n\n\nEn el análisis utilizando la prueba de \\(\\chi^2\\) de Pearson para la asociación entre el tipo de sistema de salud y el nivel educativo, se encontró una relación significativa (χ2 = 217.56 , df = 3, p&lt; .001). Por tanto, rechazamos la \\(H_{0}\\) sobre no asociación con un 95% de confianza, existiendo evidencia a favor de nuestra \\(H_{A}\\) ya que hay evidencia de una relación entre el sistema de salud y el nivel educativo.\n\n\nY el cálculo directo en R:\n\n\nchi_results &lt;- chisq.test(table(proc_casen$ss_salud, proc_casen$universitaria))\n\nstats.table &lt;- tidy(chi_results, conf_int = T)\nnice_table(stats.table)\n\nstatisticpparameterMethod217.56&lt; .001***3Pearson's Chi-squared test\n\n\nVisualicemos la distribución de esta prueba y su zona de rechazo.\n\nggchisqtest(chi_results)",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 06: Correlación categóricas"
    ]
  },
  {
    "objectID": "assignment/06-practico.html#correlación-entre-categóricas",
    "href": "assignment/06-practico.html#correlación-entre-categóricas",
    "title": "Práctico 6. Asociación con variables categóricas",
    "section": "3. Correlación entre categóricas",
    "text": "3. Correlación entre categóricas\nAl igual que otros coeficientes de correlación, las correlaciones entre categóricas:\n\nOscila entre -1 y 1.\nIndica la dirección y fuerza de asociación entre variables.\nSu tamaño de efecto se puede interpretar a partir de ciertos estándares.\nSe interpreta de la misma forma que otros coeficientes de correlación.\n\n\n3.1. Punto biserial\nLa correlación punto biserial se utiliza para calcular la correlación entre una variable categórica dicotómica y una variable continua.\nVeamos la frecuencia de sexo y la media de ingresos y1.\n\nsjmisc::frq(proc_casen$sexo)\n\nSexo (x) &lt;numeric&gt; \n# total N=3000 valid N=3000 mean=1.53 sd=0.50\n\nValue |     Label |    N | Raw % | Valid % | Cum. %\n---------------------------------------------------\n    1 | 1. Hombre | 1404 | 46.80 |   46.80 |  46.80\n    2 |  2. Mujer | 1596 | 53.20 |   53.20 | 100.00\n &lt;NA&gt; |      &lt;NA&gt; |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nmean(proc_casen$y1, na.rm = T)\n\n[1] 619435.3\n\n\nObtengamos la correlación punto biserial entre sexo e ingresos.\n\ncor.test(proc_casen$sexo, proc_casen$y1)\n\n\n    Pearson's product-moment correlation\n\ndata:  proc_casen$sexo and proc_casen$y1\nt = -4.0718, df = 912, p-value = 0.00005069\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.19676351 -0.06937883\nsample estimates:\n       cor \n-0.1336231 \n\n\n\n\n3.2. Tetracórica\nLa correlación tetracórica se utiliza para calcular la correlación entre dos variables binarias categóricas, es decir, variables nominales dicómoticas (solo dos posibles valores).\nVeamos las frecuencias de sexo y disc_fisica.\n\nsjmisc::frq(proc_casen$sexo)\n\nSexo (x) &lt;numeric&gt; \n# total N=3000 valid N=3000 mean=1.53 sd=0.50\n\nValue |     Label |    N | Raw % | Valid % | Cum. %\n---------------------------------------------------\n    1 | 1. Hombre | 1404 | 46.80 |   46.80 |  46.80\n    2 |  2. Mujer | 1596 | 53.20 |   53.20 | 100.00\n &lt;NA&gt; |      &lt;NA&gt; |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nsjmisc::frq(proc_casen$disc_fisica)\n\nDiscriminado por su apariencia física (x) &lt;numeric&gt; \n# total N=3000 valid N=3000 mean=1.03 sd=0.17\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\n    1 | 2916 | 97.20 |   97.20 |  97.20\n    2 |   84 |  2.80 |    2.80 | 100.00\n &lt;NA&gt; |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\nObtengamos la correlación tetrácorica entre sexo y discriminación por apariencia física.\n\nmatriz &lt;- proc_casen %&gt;% select(sexo, disc_fisica) # creamos matriz con var de interes\n\npsych::tetrachoric(matriz, na.rm = T)\n\nCall: psych::tetrachoric(x = matriz, na.rm = T)\ntetrachoric correlation \n            sexo  dsc_f\nsexo         1.00      \ndisc_fisica -0.05  1.00\n\n with tau of \n       sexo disc_fisica \n      -0.08        1.91 \n\n\n\n\n3.3. Policórica\nLa correlación policórica se utiliza para calcular la correlación entre dos variables ordinales categóricas, es decir, variables ordinales cuyos posibles valores siguen un orden (por ejemplo, variables tipo Likert).\nVeamos las frecuencias de ayuda_moverse y ayuda_thogar.\n\nsjmisc::frq(proc_casen$ayuda_moverse)\n\ns33c. Últ. 30 días, ayuda de otra persona para: Moverse dentro de la casa (x) &lt;numeric&gt; \n# total N=3000 valid N=68 mean=3.21 sd=1.49\n\nValue |            Label |    N | Raw % | Valid % | Cum. %\n----------------------------------------------------------\n  -99 |      No responde |    0 |  0.00 |    0.00 |   0.00\n  -88 |          No sabe |    0 |  0.00 |    0.00 |   0.00\n    1 |         1. Nunca |   13 |  0.43 |   19.12 |  19.12\n    2 |    2. Casi nunca |    9 |  0.30 |   13.24 |  32.35\n    3 | 3. Algunas veces |   18 |  0.60 |   26.47 |  58.82\n    4 |  4. Muchas veces |    7 |  0.23 |   10.29 |  69.12\n    5 |       5. Siempre |   21 |  0.70 |   30.88 | 100.00\n &lt;NA&gt; |             &lt;NA&gt; | 2932 | 97.73 |    &lt;NA&gt; |   &lt;NA&gt;\n\nsjmisc::frq(proc_casen$ayuda_thogar)\n\ns33h. Últ. 30 días, ayuda de otra persona para: Realizar sus tareas del hogar (x) &lt;numeric&gt; \n# total N=3000 valid N=80 mean=3.80 sd=1.39\n\nValue |            Label |    N | Raw % | Valid % | Cum. %\n----------------------------------------------------------\n  -99 |      No responde |    0 |  0.00 |    0.00 |   0.00\n  -88 |          No sabe |    0 |  0.00 |    0.00 |   0.00\n    1 |         1. Nunca |   10 |  0.33 |   12.50 |  12.50\n    2 |    2. Casi nunca |    3 |  0.10 |    3.75 |  16.25\n    3 | 3. Algunas veces |   17 |  0.57 |   21.25 |  37.50\n    4 |  4. Muchas veces |   13 |  0.43 |   16.25 |  53.75\n    5 |       5. Siempre |   37 |  1.23 |   46.25 | 100.00\n &lt;NA&gt; |             &lt;NA&gt; | 2920 | 97.33 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\nObtengamos la correlación policórica entre si la persona necesitó ayuda para moverse dentro de la casa y si necesitó ayuda para realizar tareas dentro del hogar, en los últimos 30 días.\n\nmatriz &lt;- proc_casen %&gt;% select(ayuda_moverse, ayuda_thogar) # creamos matriz con var de interes\n\npsych::polychoric(matriz, na.rm = T)\n\nCall: psych::polychoric(x = matriz, na.rm = T)\nPolychoric correlations \n              ayd_m ayd_t\nayuda_moverse 1.00       \nayuda_thogar  0.82  1.00 \n\n with tau of \n                  1     2     3     4\nayuda_moverse -0.87 -0.46  0.22 0.499\nayuda_thogar  -1.15 -0.98 -0.32 0.094",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 06: Correlación categóricas"
    ]
  },
  {
    "objectID": "assignment/01-practico.html",
    "href": "assignment/01-practico.html",
    "title": "Práctico 01. Aproximación inicial a R",
    "section": "",
    "text": "Esta práctica tiene dos objetivos:\n\nGenerar un primer acercamiento al uso de R y Rstudio, conociendo su interfaz y sus principales funcionalidades y\nRevisar algunos procedimientos básicos de la preparación de datos con R, que son necesarios para luego poder aplicar los contenidos más específicos de este curso.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 01: Aproximación inicial a R"
    ]
  },
  {
    "objectID": "assignment/01-practico.html#objetivos-de-la-práctica",
    "href": "assignment/01-practico.html#objetivos-de-la-práctica",
    "title": "Práctico 01. Aproximación inicial a R",
    "section": "",
    "text": "Esta práctica tiene dos objetivos:\n\nGenerar un primer acercamiento al uso de R y Rstudio, conociendo su interfaz y sus principales funcionalidades y\nRevisar algunos procedimientos básicos de la preparación de datos con R, que son necesarios para luego poder aplicar los contenidos más específicos de este curso.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 01: Aproximación inicial a R"
    ]
  },
  {
    "objectID": "assignment/01-practico.html#r-y-rstudio",
    "href": "assignment/01-practico.html#r-y-rstudio",
    "title": "Práctico 01. Aproximación inicial a R",
    "section": "1. R y Rstudio",
    "text": "1. R y Rstudio\n\nGuía de instalación\nSe encuentra un guía detallada para la instalación de R y RStudio en la descripción de los prácticos.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 01: Aproximación inicial a R"
    ]
  },
  {
    "objectID": "assignment/01-practico.html#primeros-pasos",
    "href": "assignment/01-practico.html#primeros-pasos",
    "title": "Práctico 01. Aproximación inicial a R",
    "section": "2. Primeros pasos",
    "text": "2. Primeros pasos\nEn primer lugar vamos a abrir un archivo de R (script). Esto se puede hacer manualmente con File -&gt; new file -&gt; R script o directamente con ctrl + shift + N.\n\nEsta es nuestra hoja de código, que utilizaremos para procesar bases de datos, modificar variables y crear tablas y gráficos.\n\n\n2.a. R como calculadora\nUno de los usos más sencillos y que están a la base de R, es usarlo como una calculadora.\n\n5+5\n\n[1] 10\n\n25/5\n\n[1] 5\n\n2*2\n\n[1] 4\n\n27-2\n\n[1] 25\n\n\nComo podrás ver, el resultado de estas instrucciones aparecen como un [1] en la consola. También podemos hacer operatorias más complejas y con más cálculos.\n\n12*(7+2)+(45-32)+8\n\n[1] 129\n\n22^2 - 2^2\n\n[1] 480\n\n1/200 * 30\n\n[1] 0.15\n\n\n\n\n2.b. Creación de objetos\nSe pueden crear objetos y asignarles valores\n\na &lt;- 28\nb &lt;- 8\n\na + b\n\n[1] 36\n\n\nO asignar operaciones a un objeto\n\nc &lt;- a + b\n\nAsí como también agregar texto\n\nd &lt;- \"hola\"\n\nd\n\n[1] \"hola\"\n\n\nY operaciones un poco más complejas\n\ne &lt;- b^2\n\ne + a * c\n\n[1] 1072\n\n\nSin embargo, la mayor parte del tiempo usamos funciones que ya existen en R\n\nsum(28,8)\n\n[1] 36\n\n\n\nround(10.14536) #aproximar\n\n[1] 10",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 01: Aproximación inicial a R"
    ]
  },
  {
    "objectID": "assignment/01-practico.html#flujo-de-trabajo-en-r",
    "href": "assignment/01-practico.html#flujo-de-trabajo-en-r",
    "title": "Práctico 01. Aproximación inicial a R",
    "section": "3. Flujo de trabajo en R",
    "text": "3. Flujo de trabajo en R\nTal vez una de las dificultades más comunes o cotidianas del uso de R es el orden de trabajo, en donde tenemos cientos de archivos, scripts, gráficos, bases de datos u otros repartidos desordenadamente en nuestro computador. También se da mucho el caso en que, cuando queremos trabajar con alguien, tenemos que cambiar las rutas de los archivos, por ejemplo en dónde están las bases de datos, ya que nuestros ordenadores y usuarios se llaman y son escencialmente distintos.\n¿Cómo podemos sortear eso? Como se mencionó de manera introductoria en el práctico 01, lo podemos hacer siguiendo un flujo de trabajo reproducible, autocontenido y ordenado. En este curso trabajaremos R con un flujo de trabajo reproducible, basado en el sistema IPO. El protocolo IPO es una plantilla/protocolo de estructura digital de carpetas que tiene por objetivo el organizar, procesar y documentar los datos de un proyecto de investigación con miras a la apertura de los datos en un repositorio público y de acceso libre. En concreto, el sistema IPO se propone abordar brevemente todo lo referente a los Datos, Métodos y Resultados.\nLleva este nombre por el sistema de carpetas que se implementan: Input, Procesamiento y Output. En la carpeta Input guardaremos todos aquellso recursos iniciales que usaremos, como las bases de datos, el libro de códigos, entre otros. En la carpeta de Procesamiento, como dice el nombre, guardaremos todos los archivos que procesen y analicen datos. En la carpeta Output guardaremos todo aquello que hayamos producido en los archivos de procesamiento, como las bases de datos procesadas listas para compartir o publicas, los documentos de reporte, informes o analísis, gráficos o tablas.\n La implementación de la reproducibilidad en este tipo de protocolos se basa en generar un conjunto de archivos auto-contenidos organizado en una estructura de proyecto que cualquier persona pueda compartir y ejecutar. En otras palabras, debe tener todo lo que necesita para ejecutar y volver a ejecutar el análisis. Para conocer más, visita el Laboratorio de Ciencia Abierta.\n\n\n3.1. Rproject\nUn Rproject es una herramienta de R que nos permite establecer un directorio de trabajo en una carpeta de nuestro computador. Al hacerlo, establecemos un espacio de trabajo que permite crear una estructura de carpetas donde guardar los documentos asociados al proyecto. De esta forma, creamos un conjunto de archivos autocontenidos en un solo lugar que nos permite organizar nuestro trabajo y facilitar la reproducibilidad. En las próximas sesiones estableceremos un protocolo de trabajo que permite organizar y armonizar el trabajo: el protocolo IPO.\n\n\n\n3.2. Crear un Rproject\n\nAbrir Rstudio\nSeleccionar Archivo -&gt; Nuevo proyecto\n\n\n\nSeleccionamos la opción de nuevo directorio\n\n\n\nSeleccionamos la opción de nuevo proyecto\n\n\n\nLe ponemos nombre al proyecto, en browse indicamos la ubicación donde se guardará, y apretamos el botón de crear proyecto",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 01: Aproximación inicial a R"
    ]
  },
  {
    "objectID": "assignment/01-practico.html#ejemplo",
    "href": "assignment/01-practico.html#ejemplo",
    "title": "Práctico 01. Aproximación inicial a R",
    "section": "4. Ejemplo",
    "text": "4. Ejemplo\n\n4.1. Librerías a utilizar\nComo veremos en detalle en la próxima sesión, muchas de las funciones que utilizamos en R están contenidas en librerías o paquetes (packages).\nLa lógica de R es instalar librerías (solo 1 vez, con install.packages(\"librería\")), y luego cargarlas cada vez que es necesario usarlas (con library(librería)). El problema de esto es que a veces no se sabe claramente qué librerías están instaladas y cuales no, lo que va a arrojar error al cargarlas. Y, como suele suceder en R, existe una librería para solucionar este problema que se llama pacman (package manager). Lo que hace pacman es cargar la librería, y si no está instalada, la instala y la carga:\nPara utilizar la primera vez (si es que no está instalada):\n\ninstall.packages(\"pacman\")\n\nY en adelante, las librerías se cargan así:  pacman::p_load(libreria1, libreria2, libreriaX) \n\npacman::p_load(dplyr, guaguas, ggplot2)\n\nPara esta sesión las librerías que vamos a utilizar son:\n\ndplyr: ajuste general de datos\nguaguas: paquete que contiene los datos de nombres de guaguas (bebés) registrados en Chile entre 1920 y 2021 según el Registro Civil e Identificación\nggplot2: para gráficos\n\n\nAjustar espacio de trabajo\nPrevio a la carga de nuestra base de datos, y posterior a la carga de librerías, se recomienda ejecutar los siguientes comandos:\n\nrm(list=ls())       \noptions(scipen=999) \n\n\nLa función rm(list=ls()) permite comenzar con un espacio de trabajo (environment) vacío y sin otros objetos.\nLa función options(scipen=999) desactiva la notación científica, es decir, veremos los valores numéricos con todos sus decimales.\n\n\n\n\n4.2. Cargar base de datos\nCargamos la base de datos desde el paquete (para otras bases de datos se deben importar de otra forma, esto es solo como ejemplo)\n\nbase &lt;- guaguas\n\n\n\n4.3. Exploración de datos\nConocemos las dimensiones de la base de datos:\n\ndim(base)\n\n[1] 858782      5\n\n\nSon 858782 casos y 5 variables.\nLos nombres de estas variables son:\n\nnames(base)\n\n[1] \"anio\"       \"nombre\"     \"sexo\"       \"n\"          \"proporcion\"\n\n\nY la base se ve así:\n\nhead(base)      # muestra los primeros 6 casos\n\n# A tibble: 6 × 5\n   anio nombre sexo      n proporcion\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1  1920 María  F      2130     0.104 \n2  1920 José   M       984     0.0483\n3  1920 Juan   M       636     0.0312\n4  1920 Luis   M       631     0.0310\n5  1920 Rosa   F       426     0.0209\n6  1920 Ana    F       340     0.0167\n\n\n\n\n4.4. Breve análisis de datos\nAhora probemos algunas funciones para seguir explorando la base:\n\ntable(base$sexo)      # Muestra las frecuencias por categoría de respuesta\n\n\n     F      I      M \n531038    318 327426 \n\n\nPodemos ver la cantidad de nombres “F” (femenino), “M” (masculino) e “I” (indefinido) inscritos entre 1920 y 2021.\nPueden buscar sus nombres y probar, utilizamos la funcion filter del paquete dplyr:\n\ndplyr::filter(base, nombre==\"Kevin\")\n\no:\n\nfilter(base, nombre==\"Kevin\")\n\n# A tibble: 63 × 5\n    anio nombre sexo      n proporcion\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1  1931 Kevin  M         1 0.0000120 \n 2  1963 Kevin  M         1 0.0000035 \n 3  1964 Kevin  M         1 0.00000344\n 4  1967 Kevin  M         4 0.0000131 \n 5  1970 Kevin  M         6 0.0000210 \n 6  1971 Kevin  M         3 0.00000936\n 7  1972 Kevin  M         3 0.00000945\n 8  1973 Kevin  M         2 0.00000633\n 9  1974 Kevin  M         5 0.0000163 \n10  1976 Kevin  M         2 0.00000724\n# ℹ 53 more rows\n\n\nE incluso pueden ver la cantidad de personas con su nombre, en el mismo año que ustedes nacieron:\n\nd &lt;- filter(base, nombre==\"Kevin\" & anio==1996)\nsum(d$n)\n\n[1] 1312\n\n\nAvanzando un poco más, podemos utilizar ggplot2 para hacer un gráfico de líneas que muestre la evolución en el tiempo:\n\ndatos &lt;- filter(base, nombre==\"Kevin\")\nggplot(datos, aes(x = anio, y = n)) +\n  geom_line() + \n  labs(x = \"Año\", y = \"Número de personas\", title = \"Número de personas llamadas Kevin por año\")\n\n\n\n\n\n\n\n\n¿Qué puede explicar el peak de “Kevins” previo a los 2000?\nspoiler: link",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 01: Aproximación inicial a R"
    ]
  },
  {
    "objectID": "assignment/01-practico.html#otro-ejemplo",
    "href": "assignment/01-practico.html#otro-ejemplo",
    "title": "Práctico 01. Aproximación inicial a R",
    "section": "4. Otro ejemplo",
    "text": "4. Otro ejemplo\n\nguaguas %&gt;% \n  filter(nombre %in% c(\"Salvador\", \"Augusto\"), anio &gt;= 1960 & anio &lt;= 1979) %&gt;% \n  ggplot(aes(anio, n, color = nombre)) + \n  geom_line() +\n  labs(x = \"año\", y = \"total inscripciones\", color = \"nombre\", \n       title = \"Inscripciones de 'Salvador' y 'Augusto' entre 1960 - 1979\")",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 01: Aproximación inicial a R"
    ]
  },
  {
    "objectID": "resource/05-resource.html",
    "href": "resource/05-resource.html",
    "title": "Práctico 5. Documentos reproducibles",
    "section": "",
    "text": "La siguiente práctica tiene el objetivo de introducir en los supuestos y robustez del modelo de regresión. Por esta razón, volveremos a algunos de los contenidos previos relacionados con la estimación, análisis de residuos y ajuste. Para ello, utilizaremos la base de datos de la tercera ola del Estudio Longitudinal Social del Chile 2018 con el objetivo de analizar los determinantes de la Participación Ciudadana.\nLa versión original de este ejercicio proviene del curso de Estadística multivariada versión 2022."
  },
  {
    "objectID": "resource/05-resource.html#explorar-datos",
    "href": "resource/05-resource.html#explorar-datos",
    "title": "Práctico 5. Documentos reproducibles",
    "section": "Explorar datos",
    "text": "Explorar datos\nA partir de la siguiente tabla se obtienen estadísticos descriptivos que luego serán relevantes para realizar las transformaciones y análisis posteriores.\n\nview(dfSummary(elsoc, headings = FALSE, method = \"render\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo\nVariable\nLabel\nStats / Values\nFreqs (% of Valid)\nGraph\nValid\nMissing\n\n\n\n\n1\nsexo [numeric]\nSexo entrevistado\n\n\n\nMin : 0\n\n\nMean : 0.6\n\n\nMax : 1\n\n\n\n\n\n\n0\n:\n1446\n(\n38.6%\n)\n\n\n1\n:\n2302\n(\n61.4%\n)\n\n\n\n\n3748 (100.0%)\n0 (0.0%)\n\n\n2\nedad [numeric]\nEdad entrevistado\n\n\n\nMean (sd) : 47.1 (15.5)\n\n\nmin ≤ med ≤ max:\n\n\n18 ≤ 47 ≤ 90\n\n\nIQR (CV) : 25 (0.3)\n\n\n\n70 distinct values\n\n3748 (100.0%)\n0 (0.0%)\n\n\n3\neduc [factor]\nNivel educacional\n\n\n\n1. 1\n\n\n2. 2\n\n\n3. 3\n\n\n4. 4\n\n\n5. 5\n\n\n\n\n\n\n450\n(\n12.0%\n)\n\n\n370\n(\n9.9%\n)\n\n\n1600\n(\n42.7%\n)\n\n\n598\n(\n16.0%\n)\n\n\n725\n(\n19.4%\n)\n\n\n\n\n3743 (99.9%)\n5 (0.1%)\n\n\n4\npospol [factor]\nAutoubicacion escala izquierda-derecha\n\n\n\n1. 1\n\n\n2. 2\n\n\n3. 3\n\n\n4. 4\n\n\n\n\n\n\n807\n(\n22.0%\n)\n\n\n952\n(\n26.0%\n)\n\n\n734\n(\n20.0%\n)\n\n\n1171\n(\n32.0%\n)\n\n\n\n\n3664 (97.8%)\n84 (2.2%)\n\n\n5\npart01 [numeric]\nFrecuencia: Firma carta o peticion apoyando causa\n\n\n\nMean (sd) : 1.5 (0.9)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 1 ≤ 5\n\n\nIQR (CV) : 1 (0.6)\n\n\n\n\n\n\n1\n:\n2717\n(\n72.6%\n)\n\n\n2\n:\n476\n(\n12.7%\n)\n\n\n3\n:\n411\n(\n11.0%\n)\n\n\n4\n:\n117\n(\n3.1%\n)\n\n\n5\n:\n21\n(\n0.6%\n)\n\n\n\n\n3742 (99.8%)\n6 (0.2%)\n\n\n6\npart02 [numeric]\nFrecuencia: Asiste a marcha o manifestacion pacifica\n\n\n\nMean (sd) : 1.2 (0.6)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 1 ≤ 5\n\n\nIQR (CV) : 0 (0.5)\n\n\n\n\n\n\n1\n:\n3289\n(\n87.8%\n)\n\n\n2\n:\n195\n(\n5.2%\n)\n\n\n3\n:\n191\n(\n5.1%\n)\n\n\n4\n:\n51\n(\n1.4%\n)\n\n\n5\n:\n19\n(\n0.5%\n)\n\n\n\n\n3745 (99.9%)\n3 (0.1%)\n\n\n7\npart03 [numeric]\nFrecuencia: Participa en huelga\n\n\n\nMean (sd) : 1.2 (0.5)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 1 ≤ 5\n\n\nIQR (CV) : 0 (0.5)\n\n\n\n\n\n\n1\n:\n3407\n(\n91.0%\n)\n\n\n2\n:\n152\n(\n4.1%\n)\n\n\n3\n:\n146\n(\n3.9%\n)\n\n\n4\n:\n29\n(\n0.8%\n)\n\n\n5\n:\n11\n(\n0.3%\n)\n\n\n\n\n3745 (99.9%)\n3 (0.1%)\n\n\n8\npart04 [numeric]\nFrecuencia: Usa redes sociales para opinar en temas publicos\n\n\n\nMean (sd) : 1.6 (1.1)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 1 ≤ 5\n\n\nIQR (CV) : 1 (0.7)\n\n\n\n\n\n\n1\n:\n2598\n(\n69.4%\n)\n\n\n2\n:\n310\n(\n8.3%\n)\n\n\n3\n:\n514\n(\n13.7%\n)\n\n\n4\n:\n223\n(\n6.0%\n)\n\n\n5\n:\n98\n(\n2.6%\n)\n\n\n\n\n3743 (99.9%)\n5 (0.1%)\n\n\n9\nquintilemiss [factor]\n\n\n\n\n1. Quintil 1\n\n\n2. Quintil 2\n\n\n3. Quintil 3\n\n\n4. Quintil 4\n\n\n5. Quintil 5\n\n\n6. Missing\n\n\n\n\n\n\n711\n(\n19.0%\n)\n\n\n711\n(\n19.0%\n)\n\n\n710\n(\n18.9%\n)\n\n\n710\n(\n18.9%\n)\n\n\n710\n(\n18.9%\n)\n\n\n196\n(\n5.2%\n)\n\n\n\n\n3748 (100.0%)\n0 (0.0%)\n\n\n\n\nGenerated by summarytools 1.0.1 (R version 4.3.2)2024-07-11\n\n\n\n\nview_df(elsoc,max.len = 50)\n\n\nData frame: elsoc\n\n\n\n\n\n\n\n\n\nID\nName\nLabel\nValues\nValue Labels\n\n\n1\nsexo\nSexo entrevistado\n0\n1\nHombre\nMujer\n\n\n2\nedad\nEdad entrevistado\nrange: 18-90\n\n\n3\neduc\nNivel educacional\n1\n2\n3\n4\n5\nPrimaria incompleta menos\nPrimaria y secundaria baja\nSecundaria alta\nTerciaria ciclo corto\nTerciaria y Postgrado\n\n\n4\npospol\nAutoubicacion escala izquierda-derecha\n1\n2\n3\n4\nDerecha\nCentro\nIzquierda\nIndep./Ninguno\n\n\n5\npart01\nFrecuencia: Firma carta o peticion apoyando causa\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n6\npart02\nFrecuencia: Asiste a mbackground-color:#eeeeeeha o manifestacion\npacifica\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n7\npart03\nFrecuencia: Participa en huelga\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n8\npart04\nFrecuencia: Usa redes sociales para opinar en\ntemas publicos\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n9\nquintilemiss\n\n\nQuintil 1\nQuintil 2\nQuintil 3\nQuintil 4\nQuintil 5\nMissing\n\n\n\n\n\n\nelsoc &lt;- elsoc %&gt;% mutate(partpol=rowSums(select(., part01,part02,part03,part04)))"
  },
  {
    "objectID": "resource/05-resource.html#diágnosticos",
    "href": "resource/05-resource.html#diágnosticos",
    "title": "Práctico 5. Documentos reproducibles",
    "section": "Diágnosticos",
    "text": "Diágnosticos\n\nCasos influyentes\nPara determinar si un outlier es un caso influyente, es decir que su presencia/ausencia genera un cambio importante en la estimación de los coeficientes de regresión, calculamos la Distancia de Cook..\nPosteriormente, se establece un punto de corte de \\(4/(n-k-1)\\):\n\nn&lt;- nobs(fit04) #n de observaciones\nk&lt;- length(coef(fit04)) # n de parametros\ndcook&lt;- 4/(n-k-1) #punt de corte\n\nSi lo graficamos se ve de la siguiente manera:\n\nfinal &lt;- broom::augment_columns(fit04,data = elsoc)\nfinal$id &lt;- as.numeric(row.names(final))\n# identify obs with Cook's D above cutoff\nggplot(final, aes(id, .cooksd)) +\n  geom_bar(stat=\"identity\", position=\"identity\") +\n  xlab(\"Obs. Number\")+ # Modificación nombre eje x\n  ylab(\"Cook's distance\")+ # Modificación nombre eje y\n  geom_hline(yintercept=dcook)+ # Incluir una línea horizontal\n  geom_text(aes(label=ifelse((.cooksd&gt;dcook),id,\"\")), # geom text agrega nombre a los casos, en esta oportunidad solo a los valores mayores a dcook\n            vjust=-0.2, hjust=0.5)\n\n\n\n\n\n\n\n\nIdentificamos los casos influyentes y filtramos la base de datos:\n\nident&lt;- final %&gt;% filter(.cooksd&gt;dcook)\nelsoc02 &lt;- final %&gt;% filter(!(id %in% ident$id))\n\nEstimación sin casos influyentes:\n\nfit05&lt;- lm(partpol~sexo+edad+quintilemiss+pospol,data=elsoc02)\n\nlabs02 &lt;- c(\"Intercepto\",\"Sexo (mujer=1)\",\"Edad\",\n            \"Quintil 2\",\"Quintil 3\",\"Quintil 4\",\"Quintil 5\",\"Quintil perdido\",\n            \"Izquierda (ref. derecha)\",\"Centro\",\"Idep./Ninguno\")\n\nhtmlreg(list(fit04,fit05), \n        doctype = FALSE,\n        custom.model.names = c(\"Modelo 4\", \"Modelo 5\"),\n        custom.coef.names = labs02)\n\n\nStatistical models\n\n\n\n\n \n\n\nModelo 4\n\n\nModelo 5\n\n\n\n\n\n\nIntercepto\n\n\n7.97***\n\n\n7.05***\n\n\n\n\n \n\n\n(0.16)\n\n\n(0.11)\n\n\n\n\nSexo (mujer=1)\n\n\n0.12\n\n\n0.07\n\n\n\n\n \n\n\n(0.07)\n\n\n(0.05)\n\n\n\n\nEdad\n\n\n-0.04***\n\n\n-0.03***\n\n\n\n\n \n\n\n(0.00)\n\n\n(0.00)\n\n\n\n\nQuintil 2\n\n\n0.21\n\n\n0.11\n\n\n\n\n \n\n\n(0.11)\n\n\n(0.08)\n\n\n\n\nQuintil 3\n\n\n0.51***\n\n\n0.34***\n\n\n\n\n \n\n\n(0.11)\n\n\n(0.08)\n\n\n\n\nQuintil 4\n\n\n0.50***\n\n\n0.32***\n\n\n\n\n \n\n\n(0.11)\n\n\n(0.08)\n\n\n\n\nQuintil 5\n\n\n0.88***\n\n\n0.57***\n\n\n\n\n \n\n\n(0.12)\n\n\n(0.08)\n\n\n\n\nQuintil perdido\n\n\n0.59***\n\n\n0.31*\n\n\n\n\n \n\n\n(0.18)\n\n\n(0.13)\n\n\n\n\nIzquierda (ref. derecha)\n\n\n-1.04***\n\n\n-0.65***\n\n\n\n\n \n\n\n(0.10)\n\n\n(0.07)\n\n\n\n\nCentro\n\n\n-1.13***\n\n\n-0.71***\n\n\n\n\n \n\n\n(0.11)\n\n\n(0.08)\n\n\n\n\nIdep./Ninguno\n\n\n-1.60***\n\n\n-1.14***\n\n\n\n\n \n\n\n(0.10)\n\n\n(0.07)\n\n\n\n\nR2\n\n\n0.17\n\n\n0.18\n\n\n\n\nAdj. R2\n\n\n0.17\n\n\n0.18\n\n\n\n\nNum. obs.\n\n\n3656\n\n\n3460\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\nEn términos generales, el sentido y significación estadística de los coeficientes del Modelo 5 se mantiene respecto al Modelo 4. Adicionalmente, si observamos que el modelo sin casos influyentes presenta una mejora en ajuste. Por lo tanto, los análisis posteriores se realizaran en base a este modelo.\n\n\nLinealidad\nPara analizar la linealidad respecto de un modelo de regresión, debemos analizar la distribución de los residuos con respecto a la recta de regresión.\n\nLos residuos deben ser independientes de los valores predichos (fitted values).\nCualquier correlación entre residuo y valores predichos violarían este supuesto.\nLa presencia de un patrón no lineal, es señal de que el modelo está especificado incorrectamente.\n\n\nggplot(fit05, aes(.fitted, .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0) +\n  geom_smooth(se = TRUE)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nRelación entre residuos y valores predichos\n\n\n\n\nEl gráfico nos indica que existe un patrón en la distribución de los residuos. Para intentar mejorar la estimación podemos realizar una transformación de variables. A continuación presentaremos un ejemplo para la Edad y para los Ingresos.\n\nPolinomio: \\(\\text{Edad}^2\\)\n\n\nelsoc02$edad2 &lt;- elsoc02$edad^2\nfit06&lt;- lm(partpol~sexo+edad+edad2+quintilemiss+pospol,data=elsoc02)\n\n\nedad&lt;- fit06$model$edad\nfit&lt;- fit06$fitted.values\ndata01 &lt;- as.data.frame(cbind(edad,fit))\n\nggplot(data01, aes(x = edad, y = fit)) +\n  theme_bw() +\n  geom_point()+\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nEfecto cuadrático de la edad (Modelo 5)\n\n\n\n\n\nfit07 &lt;- lm(partpol~sexo+edad+edad2+quintilemiss+pospol,data=elsoc02)\n\nlabs03 &lt;- c(\"Intercepto\",\"Sexo (mujer=1)\",\"Edad\",\n            \"Quintil 2\",\"Quintil 3\",\"Quintil 4\",\"Quintil 5\",\"Quintil perdido\",\n            \"Izquierda (ref. derecha)\",\"Centro\",\"Idep./Ninguno\", \"Edad²\")\n\nhtmlreg(list(fit05, fit06, fit07), doctype = FALSE,\n        custom.model.names = c(\"Modelo 4\", \"Modelo 5\", \"Modelo 6\"), \n          custom.coef.names = labs03)\n\n\nStatistical models\n\n\n\n\n \n\n\nModelo 4\n\n\nModelo 5\n\n\nModelo 6\n\n\n\n\n\n\nIntercepto\n\n\n7.05***\n\n\n7.62***\n\n\n7.62***\n\n\n\n\n \n\n\n(0.11)\n\n\n(0.24)\n\n\n(0.24)\n\n\n\n\nSexo (mujer=1)\n\n\n0.07\n\n\n0.08\n\n\n0.08\n\n\n\n\n \n\n\n(0.05)\n\n\n(0.05)\n\n\n(0.05)\n\n\n\n\nEdad\n\n\n-0.03***\n\n\n-0.06***\n\n\n-0.06***\n\n\n\n\n \n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.01)\n\n\n\n\nQuintil 2\n\n\n0.11\n\n\n0.11\n\n\n0.11\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.08)\n\n\n(0.08)\n\n\n\n\nQuintil 3\n\n\n0.34***\n\n\n0.34***\n\n\n0.34***\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.08)\n\n\n(0.08)\n\n\n\n\nQuintil 4\n\n\n0.32***\n\n\n0.32***\n\n\n0.32***\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.08)\n\n\n(0.08)\n\n\n\n\nQuintil 5\n\n\n0.57***\n\n\n0.57***\n\n\n0.57***\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.08)\n\n\n(0.08)\n\n\n\n\nQuintil perdido\n\n\n0.31*\n\n\n0.31*\n\n\n0.31*\n\n\n\n\n \n\n\n(0.13)\n\n\n(0.13)\n\n\n(0.13)\n\n\n\n\nIzquierda (ref. derecha)\n\n\n-0.65***\n\n\n-0.65***\n\n\n-0.65***\n\n\n\n\n \n\n\n(0.07)\n\n\n(0.07)\n\n\n(0.07)\n\n\n\n\nCentro\n\n\n-0.71***\n\n\n-0.70***\n\n\n-0.70***\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.08)\n\n\n(0.08)\n\n\n\n\nIdep./Ninguno\n\n\n-1.14***\n\n\n-1.13***\n\n\n-1.13***\n\n\n\n\n \n\n\n(0.07)\n\n\n(0.07)\n\n\n(0.07)\n\n\n\n\nEdad²\n\n\n \n\n\n0.00**\n\n\n0.00**\n\n\n\n\n \n\n\n \n\n\n(0.00)\n\n\n(0.00)\n\n\n\n\nR2\n\n\n0.18\n\n\n0.19\n\n\n0.19\n\n\n\n\nAdj. R2\n\n\n0.18\n\n\n0.18\n\n\n0.18\n\n\n\n\nNum. obs.\n\n\n3460\n\n\n3460\n\n\n3460\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "resource/05-resource.html#referencias",
    "href": "resource/05-resource.html#referencias",
    "title": "Práctico 5. Documentos reproducibles",
    "section": "Referencias",
    "text": "Referencias\nDarlington & Hayes 2016 Cap16 Detecting and Managing Irregularities\nDarlington & Hayes 2016 Cap12 Nonlinear relationships"
  },
  {
    "objectID": "resource/04-resource.html",
    "href": "resource/04-resource.html",
    "title": "Práctico 4. Inferencia Estadística y Curva Normal",
    "section": "",
    "text": "El objetivo de esta guía práctica es continuar profundizando en la inferencia estadística, en particular en .\n\nAplicar pruebas de hipótesis de diferencia de medias.\nAplicar pruebas de hipótesis direccionales.\nAplicar inferencia estadística a proporciones.\nEmplear la correlación en contexto de inferencia.\n\n\n\nEn esta práctica trabajaremos con un subconjunto de datos previamente procesados de la Encuesta de Caracterización Socioeconómica (CASEN) del año 2022, elaborada por el Ministerio de Desarrollo Social y Familia. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  CASEN 20222. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos CASEN 2022."
  },
  {
    "objectID": "resource/04-resource.html#recursos-de-la-práctica",
    "href": "resource/04-resource.html#recursos-de-la-práctica",
    "title": "Práctico 4. Inferencia Estadística y Curva Normal",
    "section": "",
    "text": "En esta práctica trabajaremos con un subconjunto de datos previamente procesados de la Encuesta de Caracterización Socioeconómica (CASEN) del año 2022, elaborada por el Ministerio de Desarrollo Social y Familia. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  CASEN 20222. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos CASEN 2022."
  },
  {
    "objectID": "resource/04-resource.html#pruebas-de-dos-colas-o-una-cola",
    "href": "resource/04-resource.html#pruebas-de-dos-colas-o-una-cola",
    "title": "Práctico 4. Inferencia Estadística y Curva Normal",
    "section": "Pruebas de dos colas o una cola",
    "text": "Pruebas de dos colas o una cola\nEn estadística, la formulación de hipótesis que implica dos variables (o la comparación de grupos) busca determinar si existen diferencias en una variable entre grupos y, de ser el caso, evaluar si esta diferencia es estadísticamente significativa.\nExisten los contrastes de hipótesis sobre diferencias entre grupos, también se les llama hipótesis de dos colas.\n\n\n\n\n\n\nPrueba de dos colas\n\n\n\nContrastamos la hipótesis nula (o de trabajo) de no diferencias entre grupos: \\[  H_{0}: \\mu_{1} - \\mu_{2} = 0 \\] En relación a una hipótesis alternativa sobre diferencias entre grupos: \\[  H_{A}: \\mu_{1} - \\mu_{2} \\neq 0 \\]\n\n\nSin embargo, también podemos plantear hipótesis respecto a que el valor de cierto parámetro para un grupo puede ser mayor o menor al de otro grupo. A esto se le conoce como hipótesis de una cola.\n\n\n\n\n\n\nPrueba de una cola\n\n\n\n\\[  H_{0}: \\mu_{0} ≥ \\mu_{1} ; \\mu_{0} ≤ \\mu_{1}\\]\n\\[  H_{A}: \\mu_{0} &gt; \\mu_{1} \\]\n\\[  H_{A}: \\mu_{0} &lt; \\mu_{1} \\]"
  },
  {
    "objectID": "resource/04-resource.html#cálculo-paso-a-paso-de-estadístico-t",
    "href": "resource/04-resource.html#cálculo-paso-a-paso-de-estadístico-t",
    "title": "Práctico 4. Inferencia Estadística y Curva Normal",
    "section": "Cálculo paso a paso de estadístico t",
    "text": "Cálculo paso a paso de estadístico t\nEn esta sección se realizará el cálculo paso a paso del estadístico \\(t\\) del ejemplo anterior para demostrar cómo se origina la información que aparece en el output de R.\nLa fórmula de t:\n\\(t=\\frac{(\\bar{x}_1-\\bar{x}_2)}{\\sqrt{\\frac{s_1²}{\\sqrt{n_1}}+\\frac{s_2²}{\\sqrt{n_2}} }}\\)\nDonde en la parte superior se encuentra la diferencia de medias entre dos grupos, y en la inferior el error estándar de t.\nPasos:\n\nSe calcula la diferencia de medias\nSe calcula el error estándar de la diferencia de medias\nCálculo del valor t\nSe fija un \\(\\alpha\\) (usualmente 0.05) para rechazar \\(H_0\\), y se busca el valor crítico asociado a este \\(\\alpha\\) (en una tabla de valores t, o en R)\nSi nuestro t es superior al valor crítico, se rechaza \\(H_0\\)\n\nPaso 1: Calculamos la diferencia de medias \\((\\bar{x}_1-\\bar{x}_2)\\)\n\nmuestra %&gt;%\n  dplyr::group_by(sexo=sjlabelled::as_label(sexo)) %&gt;% # se agrupan por la variable categórica y se usan sus etiquetas con as_label\n  dplyr::summarise(Obs.=n(),Promedio=mean(edad, na.rm=TRUE),SD=sd(edad, na.rm=TRUE)) %&gt;% # se agregan las operaciones a presentar en la tabla\n  kable(format = \"markdown\")\n\n\n\n\nsexo\nObs.\nPromedio\nSD\n\n\n\n\n1\n6\n32.33333\n1.505545\n\n\n2\n4\n25.25000\n2.629956\n\n\n\n\ndif_medias &lt;- 32.333 - 25.250\ndif_medias\n\n[1] 7.083\n\n\nPaso 2: Calculamos el error estándar de la diferencia de medias: \\(\\sqrt{\\frac{s_1²}{\\sqrt{n_1}}+\\frac{s_2²}{\\sqrt{n_2}}}\\)\n\nmuestra_h &lt;- muestra %&gt;% filter(sexo==1)\nmuestra_m &lt;- muestra %&gt;% filter(sexo==2)\n  \ns_h &lt;- sd(muestra_h$edad)\nn_h &lt;- length(muestra_h$edad)\ns_m &lt;- sd(muestra_m$edad)\nn_m &lt;- length(muestra_m$edad)\n\nee &lt;- sqrt((s_h^2)/n_h + (s_m^2)/n_m)\nee\n\n[1] 1.451532\n\n\nPaso 3: Cálculo del valor t\n\nte &lt;- dif_medias/ee\nte\n\n[1] 4.879673\n\n\nPaso 4: Fijamos un \\(\\alpha\\) y se busca el valor crítico de t asociado al \\(\\alpha\\). En este caso utilizaremos el valor usual de \\(\\alpha = 0.05\\).\n\ntt &lt;- qt(0.05/2,df=9,lower.tail=F)\ntt\n\n[1] 2.262157\n\n\nPaso 5: test de hipótesis\nSegún la distribución t, el valor crítico para poder rechazar \\(H_0\\) con un 95% de confianza es 2.26. El t calculado con información de la muestra (o t empírico) es 4.87. Este valor es superior al t crítico, por lo tanto se rechaza \\(H_0\\) con un 95% de confianza, o una probabilidad de error p&lt;0.05."
  },
  {
    "objectID": "resource/04-resource.html#ejemplo-casos-reales",
    "href": "resource/04-resource.html#ejemplo-casos-reales",
    "title": "Práctico 4. Inferencia Estadística y Curva Normal",
    "section": "Ejemplo casos reales",
    "text": "Ejemplo casos reales\nComencemos por preparar nuestros datos. Iniciamos cargando las librerías necesarias.\n\npacman::p_load(tidyverse, # Manipulacion datos\n               sjPlot, #tablas\n               confintr, # IC\n               gginference, # Visualizacion \n               rempsyc, # Reporte\n               broom) # Varios\npackage 'confintr' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\kevin\\AppData\\Local\\Temp\\Rtmp8A9Vyv\\downloaded_packages\npackage 'gginference' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\kevin\\AppData\\Local\\Temp\\Rtmp8A9Vyv\\downloaded_packages\npackage 'rempsyc' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\kevin\\AppData\\Local\\Temp\\Rtmp8A9Vyv\\downloaded_packages\n\noptions(scipen = 999) # para desactivar notacion cientifica\nrm(list = ls()) # para limpiar el entorno de trabajo\n\nCargamos los datos directamente desde internet.\n\nload(url(\"https://github.com/cursos-metodos-facso/datos-ejemplos/raw/main/proc_casen.RData\")) #Cargar base de datos\n\nA continuación, exploramos la base de datos proc_casen.\n\nnames(proc_casen) # Nombre de columnas\n\n [1] \"id_vivienda\"      \"folio\"            \"id_persona\"       \"hogar\"           \n [5] \"nucleo\"           \"varunit\"          \"varstrat\"         \"expr\"            \n [9] \"edad\"             \"sexo\"             \"educ\"             \"activ\"           \n[13] \"y1\"               \"ytrabajocor\"      \"pobreza_multi_5d\" \"o15\"             \n[17] \"qaut\"             \"fdt\"              \"ocupado\"          \"desocupado\"      \n[21] \"inact\"            \"hijo\"             \"n_educ\"           \"universitaria\"   \n[25] \"tipo_ocup\"        \"ss_salud\"         \"ayuda_moverse\"    \"ayuda_thogar\"    \n[29] \"disc_fisica\"     \n\ndim(proc_casen) # Dimensiones\n\n[1] 202111     29\n\n\nContamos con 24 variables (columnas) y 202.111 observaciones (filas).\nEvaluemos si el promedio de ingresos del trabajo de las mujeres es distinto al de los hombres en Chile en el 2022.\nApliquemos nuestros cinco pasos para inferencia.\n\nFormulamos nuestras hipótesis y dirección de la prueba:\n\n\n\\(H_{0}\\): \\(\\mu_{hombres}\\) \\(-\\) \\(\\mu_{mujeres}\\) \\(=\\) \\(0\\)\n\\(H_{A}\\): \\(\\mu_{hombres}\\) \\(-\\) \\(\\mu_{mujeres}\\) \\(\\neq\\) \\(0\\)\n\n\nCalcula el error estándar (SE) para diferencia de medias:\n\n\nocupados &lt;- proc_casen %&gt;%\n  filter(ocupado == 1) %&gt;% \n  na.omit() #  subset de datos solo con personas ocupadas\n\ndatos_t &lt;- ocupados %&gt;% \n  group_by(sexo) %&gt;% \n  summarise(media = mean(ytrabajocor, na.rm = T),\n            ds = sd(ytrabajocor, na.rm = T),\n            n = n())\n\ndatos_t\n\n# A tibble: 2 × 4\n   sexo   media      ds     n\n  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n1     1 578107. 388352.    64\n2     2 516235. 405302.   109\n\n\nObtenemos la diferencia de medias (\\(\\bar{x_1}\\) - \\(\\bar{x_2}\\))\n\ndif_medias &lt;- 817688.2 - 674428.3\ndif_medias\n\n[1] 143259.9\n\n\nAhora, calculamos el error estándar.\n\ns_h &lt;- 837710.9 \ns_m &lt;- 638044.1\n\nn_h &lt;- 32019 \nn_m &lt;- 26313    \n\nse_dif &lt;- sqrt((s_h^2)/n_h + (s_m^2)/n_m)\nse_dif\n\n[1] 6114.607\n\n\n\nCalcula el valor estimado de la prueba (t para diferencia de medias):\n\n\nt_stat &lt;- dif_medias/se_dif\nt_stat\n\n[1] 23.42912\n\n\n\nEspecifica el valor crítico:\n\n\ndf &lt;- n_h + n_m - 2 # definimos grados de libertad\n\nt_critico &lt;- qt(p = 0.05/2, df, lower.tail = FALSE)\nt_critico\n\n[1] 1.960005\n\n\n\nContrasta el valor estimado con el crítico e interpreta los resultados:\n\n\nt_stat &gt; t_critico\n\n[1] TRUE\n\n\nComparamos el valor estimado con el valor crítico para dos colas. Por tanto, nuestro valor estimado queda dentro de la zona de rechazo de \\(H_0\\). En consecuencia, podemos decir que:\n\nLa prueba T que evalúa la diferencia de medias entre el ingreso del trabajo según sexo sugiere que el efecto es positivo y estadísticamente signficativo (diferencia = 143.260, t(58004.33) = 23.43, p &lt; .001). Por tanto, rechazamos la \\(H_{0}\\) sobre igualdad de medias con un 95% de confianza, existiendo evidencia a favor de nuestra \\(H_{A}\\) ya que hay diferencias salariales significativas entre hombres y mujeres.\n\n\nY el cálculo directo en R:\n\n\nt_results &lt;- t.test(ocupados$ytrabajocor ~ ocupados$sexo, \n       alternative = \"two.sided\")\n\n# stats.table &lt;- tidy(t_results, conf_int = T)\n# nice_table(stats.table, broom = \"t.test\")\n\nVisualicemos la distribución de esta prueba y su zona de rechazo.\n\nggttest(t_results)\n\n\n\n\n\n\n\n\nAdemás, podemos calcular un intervalo de confianza que acompaña nuestra estimación. En este caso, vemos que el IC para la diferencia de medias oscila entre [131.275 - 155.245] y no contiene el cero, por lo que podemos rechazar la hipótesis nula."
  },
  {
    "objectID": "assignment/07-practico.html",
    "href": "assignment/07-practico.html",
    "title": "Práctico 7. Regresión lineal I",
    "section": "",
    "text": "El desarrollo de esta guía tiene por objetivo revisar algunos procedimientos para la estimación de regresiones lineales y múltiples en R.\nPor temas de orden y reproducibilidad, en este curso hemos separado en dos momentos el trabajo con datos, y dos archivos de código correspondientes.\n\n\n\nCohesión barrial con elsoc 2016.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 07: Regresión lineal I"
    ]
  },
  {
    "objectID": "assignment/07-practico.html#objetivo-de-la-práctica",
    "href": "assignment/07-practico.html#objetivo-de-la-práctica",
    "title": "Práctico 7. Regresión lineal I",
    "section": "",
    "text": "El desarrollo de esta guía tiene por objetivo revisar algunos procedimientos para la estimación de regresiones lineales y múltiples en R.\nPor temas de orden y reproducibilidad, en este curso hemos separado en dos momentos el trabajo con datos, y dos archivos de código correspondientes.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 07: Regresión lineal I"
    ]
  },
  {
    "objectID": "assignment/07-practico.html#antecedentes-de-los-datos-a-utilizar",
    "href": "assignment/07-practico.html#antecedentes-de-los-datos-a-utilizar",
    "title": "Práctico 7. Regresión lineal I",
    "section": "",
    "text": "Cohesión barrial con elsoc 2016.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 07: Regresión lineal I"
    ]
  },
  {
    "objectID": "assignment/07-practico.html#librerias",
    "href": "assignment/07-practico.html#librerias",
    "title": "Práctico 7. Regresión lineal I",
    "section": "1. Librerías principales (de R) a utilizar en el análisis",
    "text": "1. Librerías principales (de R) a utilizar en el análisis\n\npacman::p_load(dplyr,haven, car, sjmisc, sjPlot, sjlabelled, stargazer, kableExtra, corrplot, texreg, ggplot2, ggpubr, psych)",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 07: Regresión lineal I"
    ]
  },
  {
    "objectID": "assignment/07-practico.html#cargar-base-de-datos",
    "href": "assignment/07-practico.html#cargar-base-de-datos",
    "title": "Práctico 7. Regresión lineal I",
    "section": "2. Cargar base de datos",
    "text": "2. Cargar base de datos\nAjustar espacio de trabajo\nPrevio a la carga de nuestra base de datos, se recomienda ejecutar los siguientes comandos:\n\nrm(list=ls())       # borrar todos los objetos en el espacio de trabajo\noptions(scipen=999) # valores sin notación científica\n\nLa función rm(list=ls()) permite comenzar con un espacio de trabajo (environment) vacío y sin otros objetos. Así también, la función options(scipen=999) desactiva la notación científica, es decir, veremos los valores numéricos con todos sus decimales.\nDatos\n\n#cargamos la base de datos desde internet\nload(url(\"https://dataverse.harvard.edu/api/access/datafile/7245118\")) #Cargar base de datos\n\nRealizamos un chequeo básico de la lectura de datos: nombres de las variables y tamaño de la base en términos de casos y variables (en este ejemplo, 18035, 750 ).\n\ndim(elsoc_long_2016_2022.2) # dimension de la base\n\n[1] 18035   750",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 07: Regresión lineal I"
    ]
  },
  {
    "objectID": "assignment/07-practico.html#selección-de-variables-a-utilizar",
    "href": "assignment/07-practico.html#selección-de-variables-a-utilizar",
    "title": "Práctico 7. Regresión lineal I",
    "section": "3. Selección de variables a utilizar",
    "text": "3. Selección de variables a utilizar\nEste paso consiste en crear un subset reducido de datos que contenga solo las variables de interés. Para ello lo más fácil es revisar el libro de códigos de cada base de datos. Además filtramos por la ola 1 para trabajar solo con datos del 2016.\n\nproc_data &lt;- elsoc_long_2016_2022.2 %&gt;% filter(ola==\"1\") %&gt;% \n  select(t02_01, # Este barrio es ideal para mi\n         t02_02, # Me siento incluido en este barrio\n         t02_03, # Me identifico con la gente de este barrio\n         t02_04, # Este barrio es parte de mi\n         m01,# nivel educacional\n         m0_sexo,# sexo\n         m0_edad# edad\n         )\n\n# Comprobar\nnames(proc_data)\n\n[1] \"t02_01\"  \"t02_02\"  \"t02_03\"  \"t02_04\"  \"m01\"     \"m0_sexo\" \"m0_edad\"\n\n\nMediante el comando get_label obtenemos el atributo label de las variables.\n\nsjlabelled::get_label(proc_data)\n\n                                                       t02_01 \n          \"Grado de acuerdo: Este es el barrio ideal para mi\" \n                                                       t02_02 \n     \"Grado de acuerdo: Me siento integrado/a en este barrio\" \n                                                       t02_03 \n\"Grado de acuerdo: Me identifico con la gente de este barrio\" \n                                                       t02_04 \n               \"Grado de acuerdo: Este barrio es parte de mi\" \n                                                          m01 \n                                          \"Nivel educacional\" \n                                                      m0_sexo \n                                      \"Sexo del entrevistado\" \n                                                      m0_edad \n                                      \"Edad del entrevistado\" \n\n\nPodemos ver que son largas o con códigos poco informativos, por lo tanto, es necesario cambiarlas por etiquetas más cortas y de fácil identificación.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 07: Regresión lineal I"
    ]
  },
  {
    "objectID": "assignment/07-practico.html#procesamiento-de-variables",
    "href": "assignment/07-practico.html#procesamiento-de-variables",
    "title": "Práctico 7. Regresión lineal I",
    "section": "Procesamiento de variables",
    "text": "Procesamiento de variables\nPara el procesamiento de cada variable se seguirá el siguiente flujo de trabajo:\n\nDescriptivo general\nRecodificación: de casos perdidos y otros valores (en caso necesario)\nEtiquetado: cambio de nombres de variables y valores (en caso necesario)\nOtros ajustes\n\nY se recomienda también un descriptivo final para revisar que el procesamiento de cada variable está ok.\n\ncohesión barrial\na. Descriptivo\nPara los descriptivos se utilizará la función frq, de la librería sjmisc:\n\nfrq(proc_data$t02_01)\n\nGrado de acuerdo: Este es el barrio ideal para mi (x) &lt;numeric&gt; \n# total N=2927 valid N=2927 mean=3.31 sd=16.51\n\nValue |                                 Label |    N | Raw % | Valid % | Cum. %\n-------------------------------------------------------------------------------\n -999 |                           No Responde |    0 |  0.00 |    0.00 |   0.00\n -888 |                               No Sabe |    1 |  0.03 |    0.03 |   0.03\n -777 |       Valor perdido por error tecnico |    0 |  0.00 |    0.00 |   0.03\n -666 | Valor perdido por encuesta incompleta |    0 |  0.00 |    0.00 |   0.03\n    1 |              Totalmente en desacuerdo |  114 |  3.89 |    3.89 |   3.93\n    2 |                         En desacuerdo |  413 | 14.11 |   14.11 |  18.04\n    3 |        Ni de acuerdo ni en desacuerdo |  379 | 12.95 |   12.95 |  30.99\n    4 |                            De acuerdo | 1599 | 54.63 |   54.63 |  85.62\n    5 |                 Totalmente de acuerdo |  421 | 14.38 |   14.38 | 100.00\n &lt;NA&gt; |                                  &lt;NA&gt; |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\nEn esta variable vemos valores asociados a la opción “No contesta” (-999) y “No sabe” (-888), (-777) y (-666) que corresponde definirlos como casos perdidos (en el caso de R, como casos NA). El resto de los valores y etiquetas se encuentran en un orden correcto. Sin embargo, si queremos construir una escala, lo mejor es dejar los valores de 0 a 4\nb. Recodificación\nDespués de revisar el libro de códigos, no hay variables en que los valores negativos representen alguna otra característica, así que podemos usar set_na\n\nproc_data &lt;- proc_data %&gt;% set_na(., na = c(-999, -888, -777, -666))\n\n\nfrq(proc_data$t02_01)\n\nGrado de acuerdo: Este es el barrio ideal para mi (x) &lt;numeric&gt; \n# total N=2927 valid N=2926 mean=3.62 sd=1.02\n\nValue |                          Label |    N | Raw % | Valid % | Cum. %\n------------------------------------------------------------------------\n    1 |       Totalmente en desacuerdo |  114 |  3.89 |    3.90 |   3.90\n    2 |                  En desacuerdo |  413 | 14.11 |   14.11 |  18.01\n    3 | Ni de acuerdo ni en desacuerdo |  379 | 12.95 |   12.95 |  30.96\n    4 |                     De acuerdo | 1599 | 54.63 |   54.65 |  85.61\n    5 |          Totalmente de acuerdo |  421 | 14.38 |   14.39 | 100.00\n &lt;NA&gt; |                           &lt;NA&gt; |    1 |  0.03 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\nPara reordenar las categorías volvemos a utilizar la función recode, de la librería car\n\nproc_data$t02_01 &lt;- recode(proc_data$t02_01, \"1=0; 2=1; 3=2; 4=3; 5=4\")\nproc_data$t02_02 &lt;- recode(proc_data$t02_02, \"1=0; 2=1; 3=2; 4=3; 5=4\")\nproc_data$t02_03 &lt;- recode(proc_data$t02_03, \"1=0; 2=1; 3=2; 4=3; 5=4\")\nproc_data$t02_04 &lt;- recode(proc_data$t02_04, \"1=0; 2=1; 3=2; 4=3; 5=4\")\n\nc - Etiquetado\nVamos a dar un nombre más sustantivo a las variables con la función rename, de la librería dplyr:\n\nproc_data &lt;- proc_data %&gt;% rename(\"ideal\"=t02_01, \n                                  \"integracion\"=t02_02, \n                                  \"identificacion\"=t02_03, \n                                  \"pertenencia\"=t02_04)\n\nAdemás de cambiar el nombre, queremos cambiar las etiquetas de las variables.\n\nproc_data$ideal &lt;- set_label(x = proc_data$ideal,label = \"Este barrio es ideal para mi\")\nget_label(proc_data$ideal)\n\n[1] \"Este barrio es ideal para mi\"\n\nproc_data$integracion  &lt;- set_label(x = proc_data$integracion, label = \"Me siento integrado en este barrio\")\nget_label(proc_data$integracion)\n\n[1] \"Me siento integrado en este barrio\"\n\nproc_data$identificacion  &lt;- set_label(x = proc_data$identificacion, label = \"Me identifico con la gente de este barrio\")\nget_label(proc_data$identificacion)\n\n[1] \"Me identifico con la gente de este barrio\"\n\nproc_data$pertenencia  &lt;- set_label(x = proc_data$pertenencia, label = \"Me siento parte de este barrio\")\nget_label(proc_data$pertenencia)\n\n[1] \"Me siento parte de este barrio\"\n\n\nRevisión final\nNuevamente un descriptivo de cada variable para confirmar que el procesamiento está ok:\n\nfrq(proc_data$ideal)\n\nEste barrio es ideal para mi (x) &lt;numeric&gt; \n# total N=2927 valid N=2926 mean=2.62 sd=1.02\n\nValue |                          Label |    N | Raw % | Valid % | Cum. %\n------------------------------------------------------------------------\n    0 |                              0 |  114 |  3.89 |    3.90 |   3.90\n    1 |       Totalmente en desacuerdo |  413 | 14.11 |   14.11 |  18.01\n    2 |                  En desacuerdo |  379 | 12.95 |   12.95 |  30.96\n    3 | Ni de acuerdo ni en desacuerdo | 1599 | 54.63 |   54.65 |  85.61\n    4 |                     De acuerdo |  421 | 14.38 |   14.39 | 100.00\n    5 |          Totalmente de acuerdo |    0 |  0.00 |    0.00 | 100.00\n &lt;NA&gt; |                           &lt;NA&gt; |    1 |  0.03 |    &lt;NA&gt; |   &lt;NA&gt;\n\nfrq(proc_data$integracion)\n\nMe siento integrado en este barrio (x) &lt;numeric&gt; \n# total N=2927 valid N=2923 mean=2.57 sd=1.00\n\nValue |                          Label |    N | Raw % | Valid % | Cum. %\n------------------------------------------------------------------------\n    0 |                              0 |  109 |  3.72 |    3.73 |   3.73\n    1 |       Totalmente en desacuerdo |  436 | 14.90 |   14.92 |  18.65\n    2 |                  En desacuerdo |  408 | 13.94 |   13.96 |  32.60\n    3 | Ni de acuerdo ni en desacuerdo | 1633 | 55.79 |   55.87 |  88.47\n    4 |                     De acuerdo |  337 | 11.51 |   11.53 | 100.00\n    5 |          Totalmente de acuerdo |    0 |  0.00 |    0.00 | 100.00\n &lt;NA&gt; |                           &lt;NA&gt; |    4 |  0.14 |    &lt;NA&gt; |   &lt;NA&gt;\n\nfrq(proc_data$identificacion)\n\nMe identifico con la gente de este barrio (x) &lt;numeric&gt; \n# total N=2927 valid N=2923 mean=2.52 sd=0.99\n\nValue |                          Label |    N | Raw % | Valid % | Cum. %\n------------------------------------------------------------------------\n    0 |                              0 |  106 |  3.62 |    3.63 |   3.63\n    1 |       Totalmente en desacuerdo |  453 | 15.48 |   15.50 |  19.12\n    2 |                  En desacuerdo |  460 | 15.72 |   15.74 |  34.86\n    3 | Ni de acuerdo ni en desacuerdo | 1612 | 55.07 |   55.15 |  90.01\n    4 |                     De acuerdo |  292 |  9.98 |    9.99 | 100.00\n    5 |          Totalmente de acuerdo |    0 |  0.00 |    0.00 | 100.00\n &lt;NA&gt; |                           &lt;NA&gt; |    4 |  0.14 |    &lt;NA&gt; |   &lt;NA&gt;\n\nfrq(proc_data$pertenencia)\n\nMe siento parte de este barrio (x) &lt;numeric&gt; \n# total N=2927 valid N=2925 mean=2.63 sd=0.99\n\nValue |                          Label |    N | Raw % | Valid % | Cum. %\n------------------------------------------------------------------------\n    0 |                              0 |   91 |  3.11 |    3.11 |   3.11\n    1 |       Totalmente en desacuerdo |  422 | 14.42 |   14.43 |  17.54\n    2 |                  En desacuerdo |  362 | 12.37 |   12.38 |  29.91\n    3 | Ni de acuerdo ni en desacuerdo | 1660 | 56.71 |   56.75 |  86.67\n    4 |                     De acuerdo |  390 | 13.32 |   13.33 | 100.00\n    5 |          Totalmente de acuerdo |    0 |  0.00 |    0.00 | 100.00\n &lt;NA&gt; |                           &lt;NA&gt; |    2 |  0.07 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\nVemos que los valores (labels) de cada categoría de las variables que recodificamos no se corresponden con el nuevo valor. Para re-etiquetar valores usamos la función set_labels, de la librería sjlabelled\n\nproc_data$ideal &lt;- set_labels(proc_data$ideal,\n            labels=c( \"Totalmente en desacuerdo\"=0,\n                      \"En desacuerdo\"=1,\n                      \"Ni de acuerdo ni en desacuerdo\"=2,\n                      \"De acuerdo\"=3,\n                      \"Totalmente de acuerdo\"=4))\n\nproc_data$integracion &lt;- set_labels(proc_data$integracion,\n            labels=c( \"Totalmente en desacuerdo\"=0,\n                      \"En desacuerdo\"=1,\n                      \"Ni de acuerdo ni en desacuerdo\"=2,\n                      \"De acuerdo\"=3,\n                      \"Totalmente de acuerdo\"=4))\n\nproc_data$identificacion &lt;- set_labels(proc_data$identificacion,\n            labels=c( \"Totalmente en desacuerdo\"=0,\n                      \"En desacuerdo\"=1,\n                      \"Ni de acuerdo ni en desacuerdo\"=2,\n                      \"De acuerdo\"=3,\n                      \"Totalmente de acuerdo\"=4))\n\nproc_data$pertenencia &lt;- set_labels(proc_data$pertenencia,\n            labels=c( \"Totalmente en desacuerdo\"=0,\n                      \"En desacuerdo\"=1,\n                      \"Ni de acuerdo ni en desacuerdo\"=2,\n                      \"De acuerdo\"=3,\n                      \"Totalmente de acuerdo\"=4))\n\ny volvemos a revisar\n\nfrq(proc_data$ideal)\n\nEste barrio es ideal para mi (x) &lt;numeric&gt; \n# total N=2927 valid N=2926 mean=2.62 sd=1.02\n\nValue |                          Label |    N | Raw % | Valid % | Cum. %\n------------------------------------------------------------------------\n    0 |       Totalmente en desacuerdo |  114 |  3.89 |    3.90 |   3.90\n    1 |                  En desacuerdo |  413 | 14.11 |   14.11 |  18.01\n    2 | Ni de acuerdo ni en desacuerdo |  379 | 12.95 |   12.95 |  30.96\n    3 |                     De acuerdo | 1599 | 54.63 |   54.65 |  85.61\n    4 |          Totalmente de acuerdo |  421 | 14.38 |   14.39 | 100.00\n &lt;NA&gt; |                           &lt;NA&gt; |    1 |  0.03 |    &lt;NA&gt; |   &lt;NA&gt;\n\nfrq(proc_data$pertenencia)\n\nMe siento parte de este barrio (x) &lt;numeric&gt; \n# total N=2927 valid N=2925 mean=2.63 sd=0.99\n\nValue |                          Label |    N | Raw % | Valid % | Cum. %\n------------------------------------------------------------------------\n    0 |       Totalmente en desacuerdo |   91 |  3.11 |    3.11 |   3.11\n    1 |                  En desacuerdo |  422 | 14.42 |   14.43 |  17.54\n    2 | Ni de acuerdo ni en desacuerdo |  362 | 12.37 |   12.38 |  29.91\n    3 |                     De acuerdo | 1660 | 56.71 |   56.75 |  86.67\n    4 |          Totalmente de acuerdo |  390 | 13.32 |   13.33 | 100.00\n &lt;NA&gt; |                           &lt;NA&gt; |    2 |  0.07 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n4.2. Educación\n\n[m01] = Nivel de estudios alcanzado - Entrevistado\n\na. Descriptivo\n\nfrq(proc_data$m01)\n\nNivel educacional (x) &lt;numeric&gt; \n# total N=2927 valid N=2925 mean=5.26 sd=2.20\n\nValue |                                       Label |   N | Raw % | Valid % | Cum. %\n------------------------------------------------------------------------------------\n    1 |                                Sin estudios |  37 |  1.26 |    1.26 |   1.26\n    2 |  Educacion Basica o Preparatoria incompleta | 322 | 11.00 |   11.01 |  12.27\n    3 |    Educacion Basica o Preparatoria completa | 297 | 10.15 |   10.15 |  22.43\n    4 |    Educacion Media o Humanidades incompleta | 394 | 13.46 |   13.47 |  35.90\n    5 |      Educacion Media o Humanidades completa | 857 | 29.28 |   29.30 |  65.20\n    6 |                 Tecnica Superior incompleta | 102 |  3.48 |    3.49 |  68.68\n    7 |                   Tecnica Superior completa | 381 | 13.02 |   13.03 |  81.71\n    8 |                    Universitaria incompleta | 186 |  6.35 |    6.36 |  88.07\n    9 |                      Universitaria completa | 303 | 10.35 |   10.36 |  98.43\n   10 | Estudios de posgrado (magister o doctorado) |  46 |  1.57 |    1.57 | 100.00\n &lt;NA&gt; |                                        &lt;NA&gt; |   2 |  0.07 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\nEsta vez la vamos a dejar así\n\n\n\n4.3. Sexo\n\n[m0_sexo] = SEXO Sexo\n\na. Descriptivo\n\nfrq(proc_data$m0_sexo)\n\nSexo del entrevistado (x) &lt;numeric&gt; \n# total N=2927 valid N=2927 mean=1.60 sd=0.49\n\nValue |  Label |    N | Raw % | Valid % | Cum. %\n------------------------------------------------\n    1 | Hombre | 1163 | 39.73 |   39.73 |  39.73\n    2 |  Mujer | 1764 | 60.27 |   60.27 | 100.00\n &lt;NA&gt; |   &lt;NA&gt; |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n\n4.4 Edad\n\n[m0_edad] = EDAD Edad.\n\na. Descriptivo\n\nsummary(proc_data$m0_edad)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   33.00   46.00   46.09   58.00   88.00",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 07: Regresión lineal I"
    ]
  },
  {
    "objectID": "assignment/07-practico.html#análisis-descriptivo",
    "href": "assignment/07-practico.html#análisis-descriptivo",
    "title": "Práctico 7. Regresión lineal I",
    "section": "Análisis descriptivo",
    "text": "Análisis descriptivo\n\nsjmisc::descr(proc_data,\n      show = c(\"label\",\"range\", \"mean\", \"sd\", \"NA.prc\", \"n\"))%&gt;% # Selecciona estadísticos\n      kable(.,\"markdown\") # Esto es para que se vea bien en quarto\n\n\n\nTabla 1: Descriptivos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar\nlabel\nn\nNA.prc\nmean\nsd\nrange\n\n\n\n\n1\nideal\nEste barrio es ideal para mi\n2926\n0.0341647\n2.615174\n1.0202541\n4 (0-4)\n\n\n3\nintegracion\nMe siento integrado en este barrio\n2923\n0.1366587\n2.565515\n0.9993502\n4 (0-4)\n\n\n2\nidentificacion\nMe identifico con la gente de este barrio\n2923\n0.1366587\n2.523777\n0.9884856\n4 (0-4)\n\n\n7\npertenencia\nMe siento parte de este barrio\n2925\n0.0683293\n2.627692\n0.9878809\n4 (0-4)\n\n\n4\nm01\nNivel educacional\n2925\n0.0683293\n5.260513\n2.2015019\n9 (1-10)\n\n\n6\nm0_sexo\nSexo del entrevistado\n2927\n0.0000000\n1.602665\n0.4894300\n1 (1-2)\n\n\n5\nm0_edad\nEdad del entrevistado\n2927\n0.0000000\n46.090878\n15.2867983\n70 (18-88)\n\n\n\n\n\n\n\n\nEn la Tabla 1 podemos observar los descriptivos generales de la base de datos procesada que utilizamos en el práctico anterior. Contiene ya creado el índice de cohesión barrial cuya media es de 10,33\nY si queremos visualizar algo más:\n\nproc_data %&gt;% dplyr::select(ideal, integracion, identificacion, pertenencia) %&gt;% \n  sjPlot::plot_stackfrq()+\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigura 1: Frecuencias Cohesión barrial",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 07: Regresión lineal I"
    ]
  },
  {
    "objectID": "assignment/07-practico.html#asociación-de-variables",
    "href": "assignment/07-practico.html#asociación-de-variables",
    "title": "Práctico 7. Regresión lineal I",
    "section": "Asociación de variables",
    "text": "Asociación de variables\nPodemos ver la asociación de todas las variables, como lo muestra la ?@cor-complete\n\nM &lt;- cor(proc_data, use = \"complete.obs\") # Usar solo casos con observaciones completas\n\n\ncorrplot.mixed(M)\n\n\n\n\nCohesión variables elsoc 2016\n\n\n\n\no podemos ver específicamente la asociación de las variables de cohesión barrial\n\nM2 &lt;- cor(dplyr::select(proc_data, ideal, integracion, identificacion, pertenencia), use = \"complete.obs\")\ncorrplot.mixed(M2)\n\n\n\n\n\n\n\nFigura 2: Correlación Cohesión barrial\n\n\n\n\n\nLa Figura 2 muestra que la asociación de las cuatro variables de cohesión barrial es alta y positiva, según Cohen (1998). En general el tamaño de efecto varía entre 0.58 la más baja y 0.69 la más alta.\n\nConstrucción de escala\n\npsych::alpha(dplyr::select(proc_data, ideal, integracion, identificacion, pertenencia))\n\n\nReliability analysis   \nCall: psych::alpha(x = dplyr::select(proc_data, ideal, integracion, \n    identificacion, pertenencia))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n      0.87      0.87    0.84      0.63 6.8 0.0039  2.6 0.85     0.62\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.86  0.87  0.88\nDuhachek  0.86  0.87  0.88\n\n Reliability if an item is dropped:\n               raw_alpha std.alpha G6(smc) average_r S/N alpha se   var.r med.r\nideal               0.85      0.85    0.80      0.66 5.8   0.0047 0.00086  0.65\nintegracion         0.84      0.84    0.78      0.63 5.1   0.0053 0.00299  0.61\nidentificacion      0.83      0.83    0.76      0.62 4.9   0.0055 0.00015  0.61\npertenencia         0.83      0.83    0.76      0.62 4.8   0.0055 0.00121  0.61\n\n Item statistics \n                  n raw.r std.r r.cor r.drop mean   sd\nideal          2926  0.83  0.83  0.73   0.69  2.6 1.02\nintegracion    2923  0.85  0.85  0.78   0.73  2.6 1.00\nidentificacion 2923  0.86  0.86  0.80   0.74  2.5 0.99\npertenencia    2925  0.86  0.86  0.80   0.75  2.6 0.99\n\nNon missing response frequency for each item\n                  0    1    2    3    4 miss\nideal          0.04 0.14 0.13 0.55 0.14    0\nintegracion    0.04 0.15 0.14 0.56 0.12    0\nidentificacion 0.04 0.15 0.16 0.55 0.10    0\npertenencia    0.03 0.14 0.12 0.57 0.13    0\n\n\nLa consistencia interna de una posible escala entre estos cuatro ítems es de 0.87, lo que representa una alta consistencia interna. Si quitaramos alguno de estos ítems la consistencia interna solo bajaría, así que podemos construir una escala con los cuatro ítems.\n\nproc_data &lt;- proc_data %&gt;% \n  rowwise() %&gt;% \n  mutate(cohesion_barrial = sum(ideal, integracion, identificacion, pertenencia))\nsummary(proc_data$cohesion_barrial)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    8.00   12.00   10.33   12.00   16.00      10",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 07: Regresión lineal I"
    ]
  },
  {
    "objectID": "assignment/07-practico.html#medias-condicionales",
    "href": "assignment/07-practico.html#medias-condicionales",
    "title": "Práctico 7. Regresión lineal I",
    "section": "Medias condicionales",
    "text": "Medias condicionales\nAntes de avanzar desde la correlación al método de regresión es importante conocer el concepto de media condicional.\nImaginemos un juego de tacataca con dos variables: cantidad de juegos previos y puntos obtenidos en un partido. En estas variables, el promedio de puntos es 4. Es decir, si conocemos a algún individuo que pertence al grupo de “datos”, sabemos que su puntaje se encuentra probablemente cercano a 4. ¿Podemos mejorar nuestra estimación utilizando el puntaje de X? Si el sujeto nos dice que ha jugado antes 6 veces, probablemente vamos a estimar un puntaje superior de puntos, tal vez más cercano a 6.\nLo que estamos haciendo es utilizar la información que conocemos de X para dar una estimación de Y, que sea más precisa que el promedio bruto.\n\nMirando el gráfico de nube de puntos, sabemos que tres personas han jugado antes una vez, pero una de ellas tuvo 2 puntos, otra 3 y otra 4. Con estos datos podemos calcular la media de Y para X=1, que sería igual a 3. En otras palabras, la media condicional de Y cuando X=1 es 3. Con esto, uno podría calcular la media condicional para cada punto de X y hacer una estimación más precisa de Y. Sin embargo, este proceso todavía no nos permite generalizar más eficientemte la relación entre X e Y.\n¿Cuántos puntos (Y) se obtienen según la experiencia previa de juego (X)? Esta pregunta nos conduce al cálculo de una recta que atraviese los puntos y que generalice la relación entre X e Y.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 07: Regresión lineal I"
    ]
  },
  {
    "objectID": "assignment/07-practico.html#residuos",
    "href": "assignment/07-practico.html#residuos",
    "title": "Práctico 7. Regresión lineal I",
    "section": "Residuos",
    "text": "Residuos\nEn el gráfico anterior vemos que la línea resume la relación entre X e Y, pero claramente es una simplificación que no abarca toda la variabilidad de los datos.\nPor ejemplo, para el sujeto cuya experiencia es haber jugado 1 vez y luego gana 3 puntos, esta línea predice exáctamente su puntaje basada en su experiencia. Sin embargo, el sujeto que ha jugado 3 veces y saca 6 puntos se encuentra más lejos de la línea y por lo tanto esta línea o “modelo predictivo” no representa tan bien su puntaje. A esto se refieren los residuos, que es la diferencia entre el valor predicho (o \\(\\widehat{Y}\\)) y el observado \\(Y\\), siendo los valores predichos de Y los que pasan por la recta a la altura de cada valor de X. Por lo tanto, la mejor recta será aquella que minimice al máximo los residuos.\n\nEl sentido de la recta que resume de mejor manera la relación entre dos variables es que minimice la suma de todos los residuos. ¿Cómo realizar este procedimiento?\n\nPara realizar la suma de los residuos estos se elevan al cuadrado, lo que se denomina Suma de residuos al cuadrado o \\(SS_{residual}\\). Se eleva al cuadrado ya que como hay residuos positivos y negativos, unos cancelarían a otros y la suma seía 0, tal como sucede en la formula de la varianza.\nDe la infinita cantidad de rectas que se pueden trazar, siempre hay una que tiene un valor menor de \\(SS_{residual}\\). Este procedimiento es el que da nombre al proceso de estimación: mínimos (residuos) cuadrados ordinarios, o OLS (Ordinary Least Squares).\n\n¿Cómo funciona esto con nuestro ejemplo?\n\n#Grafico x1 = ACT\ngraph1 &lt;- ggplot(proc_data, aes(x = m0_edad, y = cohesion_barrial)) +\n  geom_point(size = 1) +  # Puntos\n  geom_smooth(method = \"lm\", se = FALSE) +  # Recta de regresión\n  labs(x = \"Edad\", y = \"Cohesión Barrial\")  # Etiquetas de ejes\n\n# Gráfico 2\ngraph2 &lt;- ggplot(proc_data, aes(x = m01, y = cohesion_barrial)) +\n  geom_point(size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Educación\", y = \"Cohesión Barrial\")\nggarrange(graph1, graph2, nrow = 1) # Unir graficos\n\n\n\n\n\n\n\n\nCon el gráfico anterior podemos notar que, si bien ambas variables tienen una asociación distinta con la cohesión barrial, el tamaño efecto de esta relación es distinto. Edad tiene una asociación positiva, mientras que educación tiene una asociación negativa. El tamaño de efecto de edad es ‘grande’, mientras que el tamaño de educación es casi nulo.",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 07: Regresión lineal I"
    ]
  },
  {
    "objectID": "assignment/07-practico.html#regresiones",
    "href": "assignment/07-practico.html#regresiones",
    "title": "Práctico 7. Regresión lineal I",
    "section": "Regresiones",
    "text": "Regresiones\nPara facilitar la interpretación de los coeficientes de regresión vamos a recodificar la variable de educación (10 categorías) en tres categorías (básica, media y universitaria).\nAdemás, nos aseguramos que las variables categóricas estén como variables categóricas con as_factor. De esta forma nos aseguramos que la estimación de los modelos sea correcta ya que no se úede interpretar educación como si fuera una variable numérica.\n\nproc_data$educacion &lt;- car::recode(proc_data$m01, \"c(1,2,3)=1; c(4,5)=2; c(6,7,8,9,10)=3\")\n\nproc_data$educacion &lt;- set_labels(proc_data$educacion,\n            labels=c( \"Educacion básica\"=1,\n                      \"Educación media\"=2,\n                      \"Educación superior\"=3))\n\nfrq(proc_data$educacion)\n\nNivel educacional (x) &lt;numeric&gt; \n# total N=2927 valid N=2925 mean=2.12 sd=0.75\n\nValue |              Label |    N | Raw % | Valid % | Cum. %\n------------------------------------------------------------\n    1 |   Educacion básica |  656 | 22.41 |   22.43 |  22.43\n    2 |    Educación media | 1251 | 42.74 |   42.77 |  65.20\n    3 | Educación superior | 1018 | 34.78 |   34.80 | 100.00\n &lt;NA&gt; |               &lt;NA&gt; |    2 |  0.07 |    &lt;NA&gt; |   &lt;NA&gt;\n\nproc_data$educacion &lt;- as_factor(proc_data$educacion)\nproc_data$sexo &lt;- as_factor(proc_data$m0_sexo)\n\nproc_data &lt;- na.omit(proc_data)\n\nreg1 &lt;- lm(cohesion_barrial ~ 1, data=proc_data)\n\nstargazer(reg1, type=\"text\")\n\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                         cohesion_barrial      \n-----------------------------------------------\nConstant                     10.336***         \n                              (0.063)          \n                                               \n-----------------------------------------------\nObservations                   2,915           \nR2                             0.000           \nAdjusted R2                    0.000           \nResidual Std. Error      3.397 (df = 2914)     \n===============================================\nNote:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n¿Qué valor toma una regresión lineal cuando no incluímos predictores en nuestro modelo?\nEn este caso, lo que nos interesa observar es el intercepto. Un intercepto de 10.336 nos indica la media de la cohesión barrial.\n\nRegresión lineal simple\nUna regresión lineal simple es aquel modelo que incluye solo un predictor. En este caso construiremos tres modelos distintos con tres variables independientes, es decir, reg2 que incluye como predictor ‘edad’, reg3 incluye educación y reg4 incluye sexo.\nreg2 &lt;- lm(cohesion_barrial ~ m0_edad, data=proc_data)\nreg3 &lt;- lm(cohesion_barrial ~ educacion, data=proc_data)\nreg4 &lt;- lm(cohesion_barrial ~ sexo, data=proc_data)\n\nknitreg(list(reg2, reg3, reg4), \n        custom.model.names = c(\"Modelo 1\",\n                               \"Modelo 2\",\n                               \"Modelo 3\"),\n        custom.note = \"*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\",\n        custom.coef.names = c(\"Intercepto\", \n                              \"Edad\",\n                              \"Educación media &lt;br&gt; &lt;i&gt;(Ref. Ed. básica)&lt;/i&gt;\", \n                              \"Educación superior\", \n                              \"Mujer &lt;br&gt; &lt;i&gt;(Ref. Hombre)&lt;/i&gt;\"),\n        caption = \"Cohesión barrial\",\n        caption.above = TRUE)\n\n\nCohesión barrial\n\n\n\n\n \n\n\nModelo 1\n\n\nModelo 2\n\n\nModelo 3\n\n\n\n\n\n\nIntercepto\n\n\n8.42***\n\n\n10.51***\n\n\n10.49***\n\n\n\n\n \n\n\n(0.20)\n\n\n(0.13)\n\n\n(0.10)\n\n\n\n\nEdad\n\n\n0.04***\n\n\n \n\n\n \n\n\n\n\n \n\n\n(0.00)\n\n\n \n\n\n \n\n\n\n\nEducación media  (Ref. Ed. básica)\n\n\n \n\n\n-0.13\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.16)\n\n\n \n\n\n\n\nEducación superior\n\n\n \n\n\n-0.35*\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.17)\n\n\n \n\n\n\n\nMujer  (Ref. Hombre)\n\n\n \n\n\n \n\n\n-0.26*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.13)\n\n\n\n\nR2\n\n\n0.03\n\n\n0.00\n\n\n0.00\n\n\n\n\nAdj. R2\n\n\n0.03\n\n\n0.00\n\n\n0.00\n\n\n\n\nNum. obs.\n\n\n2915\n\n\n2915\n\n\n2915\n\n\n\n\n\n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\n\n\nLa interpretación de una tabla de regresión debe seguir el orden de presentación de los modelos y el orden de los coeficientes de regresión. En este ejemplo se dará el paso a paso de cómo interpretar las tablas:\nEn el Modelo 1 se incluye edad como predictor, que tiene un coeficiente de regresión de 0,04. Esto indica que por cada unidad que aumenta edad, la cohesión barrial aumenta en promedio 0,04 unidades, efecto que podemos extrapolar a la población con un 99,9% de confianza (p&lt;0,001). El intercepto es de 8,42, lo que indica que (teóricamente) una persona con edad 0 tendría un promedio de cohesión barrial de 8,42. Finalmente, el modelo 1 logra explicar el 3% de la varianza de la variable dependiente (R2=0,03).\nEl Modelo 2 incluye la edad de los/as encuestados como variable independiente, teniendo la categoría de ‘educación básica’ como categoría de referencia. Este Modelo indica que las personas con educación media tienen en promedio -0,13 unidades en el índice de cohesión barrial que las personas con educación básica, sin embargo, este coeficiente no es estadísticamente significativo. En cuanto a las personas con educación superior, estas tendrían en promedio -0,35 unidades en la escala de cohesión barrial en comparación con las personas con educación básica, efecto que es estadísticamente significativo (p&lt;0,05). Si observamos el intercepto, este nos indica que el promedio de cohesión barrial para las personas con educación básica es de 10,51, por lo que el promedio de cohesión barrial para las personas con educación media sería de 10,38 y para las personas con educación superior sería 10,16.\nEl modelo 3 indica que las mujeres tendrían -0,26 unidades en la escala de cohesión barrial que los hombres, efecto que podemos extrapolar a la población con un 95% de confianza. El intercepto indica que el promedio de cohesión barrial de los hombres es 10,49, por lo que el promedio para las mujeres sería de 10,23.\n\n\nRegresión lineal múltiple\nUna regresión lineal múltiple es aquel modelo que incluye más de un predictor en las estimaciones. Idealmente, la inclusión de nuevas variables independientes, así como el orden de presentación de los modelos debe seguir un sentido teórico y/o acorde a las hipótesis de investigación. En este caso, y solo como ejemplo, construiremos cuatro modelos distintos que incluyen todas las combinaciones de variables posibles para ver cómo cambian los efectos según el control estadístico (parcialización)\nreg5 &lt;- lm(cohesion_barrial ~ m0_edad + educacion, data=proc_data)\nreg6 &lt;- lm(cohesion_barrial ~ m0_edad + sexo, data=proc_data)\nreg7 &lt;- lm(cohesion_barrial ~ educacion + sexo, data=proc_data)\nreg8 &lt;- lm(cohesion_barrial ~ m0_edad + educacion + sexo, data=proc_data)\n\nknitreg(list(reg5, reg6, reg7, reg8), \n        custom.model.names = c(\"Modelo 1\",\n                               \"Modelo 2\",\n                               \"Modelo 3\",\n                               \"Modelo 4\"),\n        custom.note = \"*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\",\n        custom.coef.names = c(\"Intercepto\", \n                              \"Edad\",\n                              \"Educación media &lt;br&gt; &lt;i&gt;(Ref. Ed. básica)&lt;/i&gt;\", \n                              \"Educación superior\", \n                              \"Mujer &lt;br&gt; &lt;i&gt;(Ref. Hombre)&lt;/i&gt;\"),\n        caption = \"Cohesión barrial\",\n        caption.above = TRUE)\n\n\nCohesión barrial\n\n\n\n\n \n\n\nModelo 1\n\n\nModelo 2\n\n\nModelo 3\n\n\nModelo 4\n\n\n\n\n\n\nIntercepto\n\n\n7.99***\n\n\n8.60***\n\n\n10.71***\n\n\n8.19***\n\n\n\n\n \n\n\n(0.28)\n\n\n(0.21)\n\n\n(0.16)\n\n\n(0.29)\n\n\n\n\nEdad\n\n\n0.04***\n\n\n0.04***\n\n\n \n\n\n0.05***\n\n\n\n\n \n\n\n(0.00)\n\n\n(0.00)\n\n\n \n\n\n(0.00)\n\n\n\n\nEducación media  (Ref. Ed. básica)\n\n\n0.35*\n\n\n \n\n\n-0.16\n\n\n0.33\n\n\n\n\n \n\n\n(0.17)\n\n\n \n\n\n(0.16)\n\n\n(0.17)\n\n\n\n\nEducación superior\n\n\n0.36*\n\n\n \n\n\n-0.38*\n\n\n0.33\n\n\n\n\n \n\n\n(0.18)\n\n\n \n\n\n(0.17)\n\n\n(0.18)\n\n\n\n\nMujer  (Ref. Hombre)\n\n\n \n\n\n-0.34**\n\n\n-0.28*\n\n\n-0.33**\n\n\n\n\n \n\n\n \n\n\n(0.13)\n\n\n(0.13)\n\n\n(0.13)\n\n\n\n\nR2\n\n\n0.04\n\n\n0.04\n\n\n0.00\n\n\n0.04\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n0.04\n\n\n0.00\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n2915\n\n\n2915\n\n\n2915\n\n\n2915\n\n\n\n\n\n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\n\n\nEl Modelo 1 incluye edad y educación como variables independientes. Este modelo indica que por cada unidad que aumenta la edad, la cohesión barrial aumenta en 0,04 unidades, manteniendo la educación constante, efecto que es estadísticamente significativo (p&lt;0,001). En cuanto a la educación, tener educación media (b=0,35) y tener educación superior (b=0,36) tienen un efecto positivo sobre la cohesión barrial en comparación con quienes tienen educación básica, menteniendo la edad constante, efecto que es estadísticamente significativo (p&lt;0,05).\nEn el Modelo 2 el efecto de edad se mantiene igual que en el modelo 1. Al incluir edad (y no educación) este modelo indica que las mujeres tendrían -0,34 unidades en la escala de cohesión barrial que los hombres, manteniendo la edad constante, efecto que podemos extrapolar a la población con un 99% de confianza.\nEl Modelo 3 incluye las variables educación y sexo, por lo que es interesante notar que al no controlar por edad, el efecto de la educación cambia de positivo a negativo y solo encontramos diferencias estadísticamente significativas al tener educación superior. El efecto del sexo disminuye, pero mantiene su sentido y significancia.\nEl Modelo 4 incluye todas las variables independientes. Este modelo indica que por cada unidad que aumenta la edad, la cohesión barrial aumenta en 0,05 unidades, manteniendo el resto de las variables constantes, efecto que es estadísticamente significativo (p&lt;0,001). En cuanto a la educación, tener educación media (b=0,33) y tener educación superior (b=0,33) tienen un efecto positivo sobre la cohesión barrial en comparación con quienes tienen educación básica, menteniendo el resto de las variables constantes, sin embargo, estas diferencias no son estadísticamente significativas. Finalmente, las mujeres tendrían -0,33 unidades en la escala de cohesión barrial que los hombres, manteniendo el resto de variables constantes, efecto que podemos extrapolar a la población con un 99% de confianza.\nLos modelos 1, 2 y 4 logran explicar el 4% de la varianza de la variable dependiente (R2=0,04).\n\n\nGraficar\n\nplot_model(reg8, \n            title = \"\", #quitar titulo\n            show.values = TRUE, #mostrar valor de efectos\n            dot.size = 3, #tamaño circulos\n            line.size = 1, #tamaño CI\n            value.size = 4, #tamaño valor efectoss\n            spacing = 1, #espacio entre efectos\n            vline.color = \"red\", # linea roja en punto neutro (0)\n            axis.labels = rev(c(\"Edad\",\n                              \"Educación media\", \n                              \"Educación superior\", \n                              \"Mujer\")), #con rev porque automatico los tira en otro orden\n            show.legend = FALSE) + # variables dependientes\n  theme_bw()",
    "crumbs": [
      "Prácticos",
      "Sesiones",
      "Práctico 07: Regresión lineal I"
    ]
  },
  {
    "objectID": "resource/07-resource.html",
    "href": "resource/07-resource.html",
    "title": "Práctica 6 Correlación y regresión",
    "section": "",
    "text": "La siguiente práctica tiene el objetivo de repasar en la interpretación de coeficientes de correlación y la construcción de índices, así como también en la interpretación de coeficientes de regresión lineal y logística. Para ello, utilizaremos la base de datos de la tercera ola del Estudio Longitudinal Social del Chile 2018 con el objetivo de analizar los determinantes de la Participación Ciudadana.\nLa versión original de este ejercicio proviene del curso de Estadística multivariada versión 2022."
  },
  {
    "objectID": "resource/07-resource.html#explorar-datos",
    "href": "resource/07-resource.html#explorar-datos",
    "title": "Práctica 6 Correlación y regresión",
    "section": "Explorar datos",
    "text": "Explorar datos\nA partir de la siguiente tabla se obtienen estadísticos descriptivos que luego serán relevantes para realizar las transformaciones y análisis posteriores.\n\nview_df(elsoc,max.len = 50)\n\n\nData frame: elsoc\n\n\n\n\n\n\n\n\n\nID\nName\nLabel\nValues\nValue Labels\n\n\n1\nsexo\nSexo entrevistado\n0\n1\nHombre\nMujer\n\n\n2\nedad\nEdad entrevistado\nrange: 18-90\n\n\n3\neduc\nNivel educacional\n1\n2\n3\n4\n5\nPrimaria incompleta menos\nPrimaria y secundaria baja\nSecundaria alta\nTerciaria ciclo corto\nTerciaria y Postgrado\n\n\n4\npospol\nAutoubicacion escala izquierda-derecha\n1\n2\n3\n4\nDerecha\nCentro\nIzquierda\nIndep./Ninguno\n\n\n5\npart01\nFrecuencia: Firma carta o peticion apoyando causa\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n6\npart02\nFrecuencia: Asiste a mbackground-color:#eeeeeeha o manifestacion\npacifica\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n7\npart03\nFrecuencia: Participa en huelga\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n8\npart04\nFrecuencia: Usa redes sociales para opinar en\ntemas publicos\n1\n2\n3\n4\n5\nNunca\nCasi nunca\nA veces\nFrecuentemente\nMuy frecuentemente\n\n\n9\ninghogar\nIngreso total del hogar\nrange: 30000-17000000\n\n\n10\ninghogar_t\nIngreso total del hogar (en tramos)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nMenos de $220.000 mensuales liquidos\nDe $220.001 a $280.000 mensuales liquidos\nDe $280.001 a $330.000 mensuales liquidos\nDe $330.001 a $380.000 mensuales liquidos\nDe $380.001 a $420.000 mensuales liquidos\nDe $420.001 a $470.000 mensuales liquidos\nDe $470.001 a $510.000 mensuales liquidos\nDe $510.001 a $560.000 mensuales liquidos\nDe $560.001 a $610.000 mensuales liquidos\nDe $610.001 a $670.000 mensuales liquidos\nDe $670.001 a $730.000 mensuales liquidos\nDe $730.001 a $800.000 mensuales liquidos\nDe $800.001 a $890.000 mensuales liquidos\nDe $890.001 a $980.000 mensuales liquidos\nDe $980.001 a $1.100.000 mensuales liquidos\nDe $1.100.001 a $1.260.000 mensuales liquidos\nDe $1.260.001 a $1.490.000 mensuales liquidos\nDe $1.490.001 a $1.850.000 mensuales liquidos\nDe $1.850.001 a $2.700.000 mensuales liquidos\nMas de $2.700.000 a mensuales liquidos\n\n\n11\ntamhogar\nHabitantes del hogar\nrange: 1-14"
  },
  {
    "objectID": "resource/07-resource.html#variable-dependiente-participación-política",
    "href": "resource/07-resource.html#variable-dependiente-participación-política",
    "title": "Práctica 6 Correlación y regresión",
    "section": "Variable dependiente: participación política",
    "text": "Variable dependiente: participación política\n\nplot_stackfrq(elsoc[,c(\"part01\",\"part02\",\"part03\",\"part04\")]) + theme(legend.position=\"bottom\")\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(cor(select(elsoc,part01,part02,part03,part04),\n                   use = \"complete.obs\"))\n\n\n\n\n\n\n\n\n\nelsoc &lt;- elsoc %&gt;% mutate(partpol=rowSums(select(., part01,part02,part03,part04)))\nsummary(elsoc$partpol)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  4.000   4.000   4.000   5.473   6.000  20.000       8"
  },
  {
    "objectID": "resource/07-resource.html#variable-independiente-ingresos",
    "href": "resource/07-resource.html#variable-independiente-ingresos",
    "title": "Práctica 6 Correlación y regresión",
    "section": "Variable independiente: ingresos",
    "text": "Variable independiente: ingresos\ningresos hogar variable continua\n\nsummary(elsoc$inghogar)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n   30000   300000   500000   678843   800000 17000000      668 \n\n\ningreso hogar en tramos\n\nsjmisc::frq(elsoc$inghogar_t,\n            out = \"txt\",\n            show.na = T) %&gt;% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nval\nlabel\nfrq\nraw.prc\nvalid.prc\ncum.prc\n\n\n\n\n1\nMenos de $220.000 mensuales liquidos\n62\n1.65\n13.00\n13.00\n\n\n2\nDe $220.001 a $280.000 mensuales liquidos\n46\n1.23\n9.64\n22.64\n\n\n3\nDe $280.001 a $330.000 mensuales liquidos\n57\n1.52\n11.95\n34.59\n\n\n4\nDe $330.001 a $380.000 mensuales liquidos\n40\n1.07\n8.39\n42.98\n\n\n5\nDe $380.001 a $420.000 mensuales liquidos\n38\n1.01\n7.97\n50.94\n\n\n6\nDe $420.001 a $470.000 mensuales liquidos\n37\n0.99\n7.76\n58.70\n\n\n7\nDe $470.001 a $510.000 mensuales liquidos\n27\n0.72\n5.66\n64.36\n\n\n8\nDe $510.001 a $560.000 mensuales liquidos\n15\n0.40\n3.14\n67.51\n\n\n9\nDe $560.001 a $610.000 mensuales liquidos\n24\n0.64\n5.03\n72.54\n\n\n10\nDe $610.001 a $670.000 mensuales liquidos\n12\n0.32\n2.52\n75.05\n\n\n11\nDe $670.001 a $730.000 mensuales liquidos\n15\n0.40\n3.14\n78.20\n\n\n12\nDe $730.001 a $800.000 mensuales liquidos\n16\n0.43\n3.35\n81.55\n\n\n13\nDe $800.001 a $890.000 mensuales liquidos\n8\n0.21\n1.68\n83.23\n\n\n14\nDe $890.001 a $980.000 mensuales liquidos\n14\n0.37\n2.94\n86.16\n\n\n15\nDe $980.001 a $1.100.000 mensuales liquidos\n14\n0.37\n2.94\n89.10\n\n\n16\nDe $1.100.001 a $1.260.000 mensuales liquidos\n10\n0.27\n2.10\n91.19\n\n\n17\nDe $1.260.001 a $1.490.000 mensuales liquidos\n7\n0.19\n1.47\n92.66\n\n\n18\nDe $1.490.001 a $1.850.000 mensuales liquidos\n11\n0.29\n2.31\n94.97\n\n\n19\nDe $1.850.001 a $2.700.000 mensuales liquidos\n14\n0.37\n2.94\n97.90\n\n\n20\nMas de $2.700.000 a mensuales liquidos\n10\n0.27\n2.10\n100.00\n\n\nNA\nNA\n3271\n87.27\nNA\nNA\n\n\n\n\n\n\n\n\n\npodemos obtener la mediana de cada tramo\n\nelsoc$inghogar_t[elsoc$inghogar_t==1] &lt;-(       220000 )    # [1]  \"Menos de $220.000 mensuales liquidos\"          \nelsoc$inghogar_t[elsoc$inghogar_t==2] &lt;-(220001 +280000 )/2 # [2]  \"De $220.001 a $280.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==3] &lt;-(280001 +330000 )/2 # [3]  \"De $280.001 a $330.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==4] &lt;-(330001 +380000 )/2 # [4]  \"De $330.001 a $380.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==5] &lt;-(380001 +420000 )/2 # [5]  \"De $380.001 a $420.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==6] &lt;-(420001 +470000 )/2 # [6]  \"De $420.001 a $470.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==7] &lt;-(470001 +510000 )/2 # [7]  \"De $470.001 a $510.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==8] &lt;-(510001 +560000 )/2 # [8]  \"De $510.001 a $560.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==9] &lt;-(560001 +610000 )/2 # [9]  \"De $560.001 a $610.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==10]&lt;-(610001 +670000 )/2 # [10] \"De $610.001 a $670.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==11]&lt;-(670001 +730000 )/2 # [11] \"De $670.001 a $730.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==12]&lt;-(730001 +800000 )/2 # [12] \"De $730.001 a $800.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==13]&lt;-(800001 +890000 )/2 # [13] \"De $800.001 a $890.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==14]&lt;-(890001 +980000 )/2 # [14] \"De $890.001 a $980.000 mensuales liquidos\"       \nelsoc$inghogar_t[elsoc$inghogar_t==15]&lt;-(980001 +1100000)/2 # [15] \"De $980.001 a $1.100.000 mensuales liquidos\"      \nelsoc$inghogar_t[elsoc$inghogar_t==16]&lt;-(1100001+1260000)/2 # [16] \"De $1.100.001 a $1.260.000 mensuales liquidos\"    \nelsoc$inghogar_t[elsoc$inghogar_t==17]&lt;-(1260001+1490000)/2 # [17] \"De $1.260.001 a $1.490.000 mensuales liquidos\"    \nelsoc$inghogar_t[elsoc$inghogar_t==18]&lt;-(1490001+1850000)/2 # [18] \"De $1.490.001 a $1.850.000 mensuales liquidos\"    \nelsoc$inghogar_t[elsoc$inghogar_t==19]&lt;-(1850001+2700000)/2 # [19] \"De $1.850.001 a $2.700.000 mensuales liquidos\"    \nelsoc$inghogar_t[elsoc$inghogar_t==20]&lt;-(2700000)           # [20] \"Mas de $2.700.000 a mensuales liquidos\"\n\ny luego imputar este valor medio a los casos NA\n\nelsoc$inghogar_i &lt;- ifelse(test = (is.na(elsoc$inghogar)), #¿existen NA en ingresos?\n                           yes = elsoc$inghogar_t,         #VERDADERO, remplazar con la media del tramo\n                           no = elsoc$inghogar)            #FALSE, mantener la variable original.\n\nelsoc$inghogar_i &lt;- set_label(elsoc$inghogar_i,\"Ingreso total del hogar (imputada)\")\n\n\nelsoc$ing_pcap &lt;- elsoc$inghogar_i/elsoc$tamhogar\nelsoc$ing_pcap &lt;- set_label(elsoc$ing_pcap,\"Ingreso per cápita del hogar\")\n\n\nelsoc$quintile&lt;- dplyr::ntile(x = elsoc$ing_pcap,\n                              n = 5) # n de categorias, para quintiles usamos 5 \nelsoc$quintile &lt;- factor(elsoc$quintile,c(1,2,3,4,5), c(\"Quintil 1\",\"Quintil 2\",\"Quintil 3\",\"Quintil 4\",\"Quintil 5\")) \nelsoc %&gt;% \n  group_by(quintile) %&gt;% \n  summarise(n=n(),\n            Media=mean(ing_pcap,na.rm = T),\n            Mediana=median(ing_pcap,na.rm = T)) %&gt;% \n  knitr::kable()\n\n\n\n\nquintile\nn\nMedia\nMediana\n\n\n\n\nQuintil 1\n711\n62859.09\n66666.67\n\n\nQuintil 2\n711\n112218.97\n111250.12\n\n\nQuintil 3\n710\n167748.23\n166666.67\n\n\nQuintil 4\n710\n262710.27\n250000.50\n\n\nQuintil 5\n710\n710246.41\n500000.00\n\n\nNA\n196\nNaN\nNA\n\n\n\n\n\n\nelsoc$quintilemiss &lt;- factor(elsoc$quintile,ordered = T)\nelsoc$quintilemiss &lt;- ifelse(test=is.na(elsoc$quintilemiss),yes = 6,no = elsoc$quintilemiss)\nelsoc$quintilemiss &lt;- factor(elsoc$quintilemiss ,levels = c(1,2,3,4,5,6),labels =  c(\"Quintil 1\",\"Quintil 2\",\"Quintil 3\",\"Quintil 4\",\"Quintil 5\",\"Missing\")) \nelsoc %&gt;% group_by(quintilemiss) %&gt;% summarise(n=n())\n\n# A tibble: 6 × 2\n  quintilemiss     n\n  &lt;fct&gt;        &lt;int&gt;\n1 Quintil 1      711\n2 Quintil 2      711\n3 Quintil 3      710\n4 Quintil 4      710\n5 Quintil 5      710\n6 Missing        196"
  },
  {
    "objectID": "resource/07-resource.html#variables-dummy",
    "href": "resource/07-resource.html#variables-dummy",
    "title": "Práctica 6 Correlación y regresión",
    "section": "Variables dummy",
    "text": "Variables dummy\nUna forma de pasar una variable categórica a dummies es con la función dummy_cols del paquete fastDummies\n\nelsoc &lt;- dummy_cols(elsoc, select_columns = \"quintilemiss\")\nhead(elsoc[,16:22])\n\n  quintilemiss quintilemiss_Quintil 1 quintilemiss_Quintil 2\n1    Quintil 1                      1                      0\n2    Quintil 5                      0                      0\n3    Quintil 1                      1                      0\n4    Quintil 5                      0                      0\n5      Missing                      0                      0\n6    Quintil 3                      0                      0\n  quintilemiss_Quintil 3 quintilemiss_Quintil 4 quintilemiss_Quintil 5\n1                      0                      0                      0\n2                      0                      0                      1\n3                      0                      0                      0\n4                      0                      0                      1\n5                      0                      0                      0\n6                      1                      0                      0\n  quintilemiss_Missing\n1                    0\n2                    0\n3                    0\n4                    0\n5                    1\n6                    0\n\n\n¿cómo hacerlo para una variable numérica?\nTambién existen muchas formas, como por ejemplo establecer como punto de corte la media o la mediana, o ver la distribución de las respuestas y tratar de establecer una distribución homogénea entre las dos nuevas categorías.\nSi recordamos la distribución de nuestra variable dependiente antes de construir el índice de participación:\n\nplot_stackfrq(elsoc[,c(\"part01\",\"part02\",\"part03\",\"part04\")]) + theme(legend.position=\"bottom\")\n\n\n\n\n\n\n\n\ny luego en el índice de participación\n\nsummary(elsoc$partpol)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  4.000   4.000   4.000   5.473   6.000  20.000       8 \n\n\nPodemos notar que la mayoría de las respuestas se agrupan en la categoría “nunca” de las variables por separado y luego en el índice la mediana también corresponde al valor mínimo posible de “4” que es la suma de todas las personas que nunca han participado en ninguna de las opciones. Por lo tanto, tenemos dos criterios que nos permiten decidir que nuestra variable dependiente puede ser considera como dummy bajo los valores 0=nunca ha participado; y 1=si ha participado.\nUna forma de hacer esta agrupación de valores es con la función case_when del paquete dplyr (similar a ifelse)\n\nelsoc &lt;- elsoc %&gt;% rowwise() %&gt;%  mutate(partpol_dummy = case_when(partpol==4~0,\n                                                                   partpol&gt;4~1,\n                                                                   TRUE ~ NA))\ntable(elsoc$partpol_dummy)\n\n\n   0    1 \n2074 1666"
  },
  {
    "objectID": "resource/07-resource.html#objetivo",
    "href": "resource/07-resource.html#objetivo",
    "title": "Práctica 6 Correlación y regresión",
    "section": "Objetivo",
    "text": "Objetivo\nEn el ejemplo de esta práctica, que utiliza solo casos para Chile entre 2005 y 2022, se intentará responder la pregunta ¿existe una relación entre la afiliación a sindicatos y la participación en marchas?\nDebido a la naturaleza de la variable dependiente participación en marchas (si/no), el objetivo de esta práctica es estimar modelos de regresión logística binaria."
  },
  {
    "objectID": "resource/07-resource.html#explorar-datos-1",
    "href": "resource/07-resource.html#explorar-datos-1",
    "title": "Práctica 6 Correlación y regresión",
    "section": "Explorar datos",
    "text": "Explorar datos\n\nsummary(WVS_2005_2022_Chl) #con comando de paquete haven\n\n   Unionized      demonstr_dummy   petition_dummy       Wave    \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Wave 5:373  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   Wave 6:516  \n Median :0.0000   Median :0.0000   Median :0.0000   Wave 7:568  \n Mean   :0.1984   Mean   :0.2073   Mean   :0.1846               \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.0000               \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000               \n                                                                \n           pol_pos     pol_pos_left    politicization civic_involvement\n left          :360   Min.   :0.0000   Min.   :0.00   Min.   :0.0000   \n center        :509   1st Qu.:0.0000   1st Qu.:1.00   1st Qu.:0.0000   \n right         :215   Median :0.0000   Median :2.00   Median :0.0000   \n Not identified:  0   Mean   :0.2471   Mean   :1.95   Mean   :0.6905   \n NA's          :373   3rd Qu.:0.0000   3rd Qu.:3.00   3rd Qu.:1.0000   \n                      Max.   :1.0000   Max.   :6.00   Max.   :3.0000   \n                                                                       \n      X003       age         Female       Educ    private_sector  \n Min.   :18.00   1:128   Min.   :0.0000   1:123   Min.   :0.0000  \n 1st Qu.:31.00   2:341   1st Qu.:0.0000   2:924   1st Qu.:1.0000  \n Median :41.00   3:400   Median :0.0000   3:410   Median :1.0000  \n Mean   :41.44   4:352   Mean   :0.4084           Mean   :0.8593  \n 3rd Qu.:50.00   5:188   3rd Qu.:1.0000           3rd Qu.:1.0000  \n Max.   :80.00   6: 48   Max.   :1.0000           Max.   :1.0000  \n                                                                  \n    gvt_resp         tax_rich        unempl_aid      state_inc_eq   \n Min.   : 1.000   Min.   : 1.000   Min.   : 1.000   Min.   : 1.000  \n 1st Qu.: 5.000   1st Qu.: 5.000   1st Qu.: 5.000   1st Qu.: 5.000  \n Median : 6.000   Median : 7.000   Median : 7.000   Median : 7.000  \n Mean   : 6.457   Mean   : 6.446   Mean   : 7.103   Mean   : 6.689  \n 3rd Qu.: 9.000   3rd Qu.: 9.000   3rd Qu.:10.000   3rd Qu.: 9.000  \n Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :10.000  \n NA's   :16       NA's   :87       NA's   :62       NA's   :439     \n\ndescribe(WVS_2005_2022_Chl) #con comando de paquete psych\n\n                  vars    n  mean    sd median trimmed   mad min max range\nUnionized            1 1457  0.20  0.40      0    0.12  0.00   0   1     1\ndemonstr_dummy       2 1457  0.21  0.41      0    0.13  0.00   0   1     1\npetition_dummy       3 1457  0.18  0.39      0    0.11  0.00   0   1     1\nWave*                4 1457  2.13  0.79      2    2.17  1.48   1   3     2\npol_pos*             5 1084  1.87  0.72      2    1.83  1.48   1   3     2\npol_pos_left         6 1457  0.25  0.43      0    0.18  0.00   0   1     1\npoliticization       7 1457  1.95  1.61      2    1.81  1.48   0   6     6\ncivic_involvement    8 1457  0.69  0.98      0    0.49  0.00   0   3     3\nX003                 9 1457 41.44 12.25     41   41.13 13.34  18  80    62\nage*                10 1457  3.19  1.27      3    3.18  1.48   1   6     5\nFemale              11 1457  0.41  0.49      0    0.39  0.00   0   1     1\nEduc*               12 1457  2.20  0.57      2    2.23  0.00   1   3     2\nprivate_sector      13 1457  0.86  0.35      1    0.95  0.00   0   1     1\ngvt_resp            14 1441  6.46  2.58      6    6.61  2.97   1  10     9\ntax_rich            15 1370  6.45  2.72      7    6.64  2.97   1  10     9\nunempl_aid          16 1395  7.10  2.53      7    7.37  2.97   1  10     9\nstate_inc_eq        17 1018  6.69  2.60      7    6.90  2.97   1  10     9\n                   skew kurtosis   se\nUnionized          1.51     0.28 0.01\ndemonstr_dummy     1.44     0.08 0.01\npetition_dummy     1.62     0.64 0.01\nWave*             -0.24    -1.37 0.02\npol_pos*           0.20    -1.04 0.02\npol_pos_left       1.17    -0.63 0.01\npoliticization     0.56    -0.43 0.04\ncivic_involvement  1.31     0.51 0.03\nX003               0.21    -0.63 0.32\nage*               0.15    -0.66 0.03\nFemale             0.37    -1.86 0.01\nEduc*             -0.02    -0.28 0.01\nprivate_sector    -2.06     2.26 0.01\ngvt_resp          -0.32    -0.71 0.07\ntax_rich          -0.34    -0.81 0.07\nunempl_aid        -0.62    -0.43 0.07\nstate_inc_eq      -0.40    -0.61 0.08"
  },
  {
    "objectID": "resource/07-resource.html#tabla-de-contingencia-bivariado-sindicalización-participación-en-marchas",
    "href": "resource/07-resource.html#tabla-de-contingencia-bivariado-sindicalización-participación-en-marchas",
    "title": "Práctica 6 Correlación y regresión",
    "section": "Tabla de contingencia bivariado: sindicalización / participación en marchas",
    "text": "Tabla de contingencia bivariado: sindicalización / participación en marchas\n\nWVS_2005_2022_Chl &lt;- WVS_2005_2022_Chl %&gt;%\n  mutate(Unionized = labelled(.$Unionized,c(\"No\"=0,\"Sí\"=1)),\n         demonstr_dummy = labelled(.$demonstr_dummy,c(\"No\"=0,\"Sí\"=1)))\n\nWVS_2005_2022_Chl &lt;- as.data.frame(WVS_2005_2022_Chl) #para que la base quede como data frame (necesario para las figuras)\n\nfrq(WVS_2005_2022_Chl$Unionized)\n\nx &lt;numeric&gt; \n# total N=1457 valid N=1457 mean=0.20 sd=0.40\n\nValue | Label |    N | Raw % | Valid % | Cum. %\n-----------------------------------------------\n    0 |    No | 1168 | 80.16 |   80.16 |  80.16\n    1 |    Sí |  289 | 19.84 |   19.84 | 100.00\n &lt;NA&gt; |  &lt;NA&gt; |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nfrq(WVS_2005_2022_Chl$demonstr_dummy)\n\nx &lt;integer&gt; \n# total N=1457 valid N=1457 mean=0.21 sd=0.41\n\nValue | Label |    N | Raw % | Valid % | Cum. %\n-----------------------------------------------\n    0 |    No | 1155 | 79.27 |   79.27 |  79.27\n    1 |    Sí |  302 | 20.73 |   20.73 | 100.00\n &lt;NA&gt; |  &lt;NA&gt; |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nsjPlot::tab_xtab(var.col = WVS_2005_2022_Chl$Unionized, \n                 var.row = WVS_2005_2022_Chl$demonstr_dummy, \n                 title = \"Participación en marchas según afiliación sindical\", \n                 show.col.prc = TRUE,\n                 value.labels = TRUE,\n                 encoding = \"UTF-8\")\n\nWarning: `valueLables` needs to be a `list`-object.\n\n\n\nParticipación en marchas según afiliación sindical\n \n demonstr_dummy\n Unionized\n Total\n \n \n\n No\n Sí\n \n \n \nNo\n94180.6 %\n21474 %\n115579.3 % \n\n \n \nSí\n22719.4 %\n7526 %\n30220.7 % \n\n \n \nTotal\n1168100 %\n289100 %\n1457100 % \n\nχ2=5.598 · df=1 · φ=0.064 · p=0.018 \n\n \n\n\n\n\nOdds\n\\[Odds_{participar} = \\frac{0.207}{0.793} = 0.26\\]\nLas chances de participar en una marcha son de 0,26, respecto a las chances de no participar\n\nEn otras palabras: por cada 1 persona, hay sólo 0,26 personas que participan en marchas.\nO más intuitivamente, por cada 100 personas, hay sólo 26 personas que participan\n\n¿Cambian las chances de participar según se esté afiliado/a a un sindicato\n\\[Odds_{sindical} = \\frac{0.26}{0.74} = 0.35\\]\n\\[Odds_{no.sindical} = \\frac{0.194}{0.806} = 0.24\\]\n¿Cómo se interpretan los odds?\n\nValores bajo 1 indican que las chances de que ocurra un evento son negativas\nValores iguales a 1 indican chances iguales\nValores sobre 1 indican chances positivas\n\n\n\nOdds ratios (razones de chances)\n\nCálculo que permite reflejar asociación entre dos variables dicotómicas, a partir de una comparación entre chances\nSiguiendo con el ejemplo anterior, ¿tienen los/as sindicalizados más chances de participar en marchas que quienes no están sindicalizados/as?\n\n\\[OR = \\frac{P_{sindical}/(1-P_{sindical})}{P_{no.sindical}/(1-P_{no.sindical})}\\]\n\\[OR = \\frac{0.26/0.74}{0.194/0.806} = \\frac{0.35}{0.24} = 1.46\\]\nLas chances de participar en marchas de los/as sindicalizados/as son 1,5 veces más que las de quienes no están sindicalizados/as\n\nImplicancias:\n\nEl odds ratio o razones de chances es útil porque nos permite expresar en un número la relación entre dos variables categóricas\nEn las regresiones logísticas, el odds ratio es la primera manera de aproximarnos a relación entre variables\nSin embargo, falta un paso más necesario para construir modelos de regresión logística"
  },
  {
    "objectID": "resource/07-resource.html#logit",
    "href": "resource/07-resource.html#logit",
    "title": "Práctica 6 Correlación y regresión",
    "section": "Logit",
    "text": "Logit\n\nEs una unidad de medida de la relación entre dos variables (VD: dicotómica), que en regresión logística se calcula a partir del logaritmo natural de los odds\nEsta transformación logarítmica es la base de la estimación de parámetros en la regresión logística:\n\nLa mejor combinación lineal de predictores no se obtiene a través de MCO, sino a través del procedimiento de máxima verosimilitud\n\nA diferencia de los odds ratio, los coeficientes logit tienen valores que van de –a +"
  },
  {
    "objectID": "resource/07-resource.html#modelo-de-probabilidad-lineal",
    "href": "resource/07-resource.html#modelo-de-probabilidad-lineal",
    "title": "Práctica 6 Correlación y regresión",
    "section": "Modelo de probabilidad lineal",
    "text": "Modelo de probabilidad lineal\nPrimero, solo para comparación, estimamos un modelo de probabilidad lineal.\n\nm1mpl &lt;- lm(demonstr_dummy ~ Unionized + Wave, data = WVS_2005_2022_Chl)\n\nhtmlreg(m1mpl,\n          custom.model.names = \"Modelo de Prob Lineal\",\n          digits = 3, \n          stars = c(0.001, 0.01, 0.05, 0.1),symbol = \"†\")\n\n\nStatistical models\n\n\n\n\n \n\n\nModelo de Prob Lineal\n\n\n\n\n\n\n(Intercept)\n\n\n0.167***\n\n\n\n\n \n\n\n(0.022)\n\n\n\n\nUnionized\n\n\n0.072**\n\n\n\n\n \n\n\n(0.027)\n\n\n\n\nWaveWave 6\n\n\n0.079**\n\n\n\n\n \n\n\n(0.027)\n\n\n\n\nWaveWave 7\n\n\n-0.005\n\n\n\n\n \n\n\n(0.027)\n\n\n\n\nR2\n\n\n0.013\n\n\n\n\nAdj. R2\n\n\n0.011\n\n\n\n\nNum. obs.\n\n\n1457\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05; †p &lt; 0.1"
  },
  {
    "objectID": "resource/07-resource.html#modelo-de-regresión-logística",
    "href": "resource/07-resource.html#modelo-de-regresión-logística",
    "title": "Práctica 6 Correlación y regresión",
    "section": "Modelo de regresión logística",
    "text": "Modelo de regresión logística\n\nm0log &lt;- glm(demonstr_dummy~ Unionized, data = WVS_2005_2022_Chl, family = \"binomial\"(link = \"logit\"))\nm1log &lt;- glm(demonstr_dummy~ Unionized + Wave, data = WVS_2005_2022_Chl, family = \"binomial\"(link = \"logit\"))\n#nota: \"logit\" viene por defecto en la opción \"binomial\", por eso no es necesario \n#incluirla explícitamente en el código (tal como lo hago en los modelos sgtes)\nm2log &lt;- glm(demonstr_dummy~ Unionized + Female + X003 + Educ + private_sector + Wave, data = WVS_2005_2022_Chl,family = \"binomial\")\nm3log &lt;- glm(demonstr_dummy~ Unionized + Female + X003 + Educ + private_sector + politicization + Wave, data = WVS_2005_2022_Chl,family = \"binomial\")\n\nhtmlreg(list(m1mpl, m1log,m2log,m3log),\n          custom.model.names = c(\"M1 (m prob lineal)\",\"M1 (log odds)\",\"M2 (log odds)\",\"M3 (log odds)\"),\n          digits = 3, \n          stars = c(0.001, 0.01, 0.05, 0.1),symbol = \"†\")\n\n\nStatistical models\n\n\n\n\n \n\n\nM1 (m prob lineal)\n\n\nM1 (log odds)\n\n\nM2 (log odds)\n\n\nM3 (log odds)\n\n\n\n\n\n\n(Intercept)\n\n\n0.167***\n\n\n-1.601***\n\n\n-1.023*\n\n\n-1.379**\n\n\n\n\n \n\n\n(0.022)\n\n\n(0.140)\n\n\n(0.424)\n\n\n(0.437)\n\n\n\n\nUnionized\n\n\n0.072**\n\n\n0.418**\n\n\n0.390*\n\n\n0.312†\n\n\n\n\n \n\n\n(0.027)\n\n\n(0.155)\n\n\n(0.157)\n\n\n(0.161)\n\n\n\n\nWaveWave 6\n\n\n0.079**\n\n\n0.470**\n\n\n0.498**\n\n\n0.496**\n\n\n\n\n \n\n\n(0.027)\n\n\n(0.168)\n\n\n(0.171)\n\n\n(0.174)\n\n\n\n\nWaveWave 7\n\n\n-0.005\n\n\n-0.031\n\n\n-0.102\n\n\n-0.121\n\n\n\n\n \n\n\n(0.027)\n\n\n(0.174)\n\n\n(0.181)\n\n\n(0.184)\n\n\n\n\nFemale\n\n\n \n\n\n \n\n\n-0.020\n\n\n0.100\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.136)\n\n\n(0.139)\n\n\n\n\nX003\n\n\n \n\n\n \n\n\n-0.007\n\n\n-0.012*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.006)\n\n\n(0.006)\n\n\n\n\nEduc2\n\n\n \n\n\n \n\n\n0.042\n\n\n-0.053\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.268)\n\n\n(0.272)\n\n\n\n\nEduc3\n\n\n \n\n\n \n\n\n0.521†\n\n\n0.259\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.283)\n\n\n(0.289)\n\n\n\n\nprivate_sector\n\n\n \n\n\n \n\n\n-0.516**\n\n\n-0.445*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.175)\n\n\n(0.180)\n\n\n\n\npoliticization\n\n\n \n\n\n \n\n\n \n\n\n0.282***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.042)\n\n\n\n\nR2\n\n\n0.013\n\n\n \n\n\n \n\n\n \n\n\n\n\nAdj. R2\n\n\n0.011\n\n\n \n\n\n \n\n\n \n\n\n\n\nNum. obs.\n\n\n1457\n\n\n1457\n\n\n1457\n\n\n1457\n\n\n\n\nAIC\n\n\n \n\n\n1475.887\n\n\n1460.059\n\n\n1415.850\n\n\n\n\nBIC\n\n\n \n\n\n1497.024\n\n\n1507.616\n\n\n1468.691\n\n\n\n\nLog Likelihood\n\n\n \n\n\n-733.944\n\n\n-721.030\n\n\n-697.925\n\n\n\n\nDeviance\n\n\n \n\n\n1467.887\n\n\n1442.059\n\n\n1395.850\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05; †p &lt; 0.1\n\n\n\n\nEn el Modelo 1 (M1), el log-odds de participación en marchas para afiliados a sindicatos aumenta en 0.418 en comparación con los no sindicalizados (p&lt;0.01). Este resultado mantiene su significación estadística en el Modelo 2 y baja su significación a p&lt;0.1 en el Modelo 3, al controlar por las demás variables independientes.\nEn el Modelo 2, En comparación a los/as trabajadores/as del sector público (categoría de referencia), el log-odds de participación en marchas para los/as del sector privado disminuye en 0,52 (p &lt; 0,01), manteniendo el resto de variables constantes.\nEn el Modelo 3, por cada unidad de aumento en la escala de politización, el log-odds de participación en marchas aumenta en 0,28 (p &lt; 0,001), manteniendo el resto de las variables constantes.\n\nProblemas de interpretación\nA pesar de sus ventajas, los coeficientes logit son difíciles de interpretar:\n\nLos coef. logit son el resultado de una transformación de la escala original\nEllos no muestran directamente probabilidades\nEntonces: Volver a la escala original de odds ratio mediante la exponenciación de los coeficientes (la función exponencial es la inversa del logaritmo)\n\n\\[logit_x = log(odds)\\]\n\\[e^{logit} = odds_x\\]\n\\[e^{0.39} = odds_x = 1.477\\]\nLas chances (odds) de participar en marchas de los/as sindicalizados/as son 1,5 veces más que las de quienes no están sindicalizados/as, controlando por las otras variables incluidas en el modelo"
  },
  {
    "objectID": "resource/07-resource.html#estimación-de-odds-ratios",
    "href": "resource/07-resource.html#estimación-de-odds-ratios",
    "title": "Práctica 6 Correlación y regresión",
    "section": "Estimación de odds ratios",
    "text": "Estimación de odds ratios\n\nexp(coef(m1log)) #comando básico\n\n(Intercept)   Unionized  WaveWave 6  WaveWave 7 \n  0.2016888   1.5185264   1.5996549   0.9694021 \n\n### Cálculo de OR para cada modelo\nm0log_OR &lt;- exp(coef(m0log))\nm1log_OR &lt;- exp(coef(m1log))\nm2log_OR &lt;- exp(coef(m2log))\nm3log_OR &lt;- exp(coef(m3log))\n\n##Odds ratios en tabla de texreg\nhtmlreg(list(m1log,m2log,m3log), \n          override.coef = list(m1log_OR,m2log_OR,m3log_OR), # Sobreescribir coeficientes\n          custom.model.names = c(\"m1 (OR)\",\"m2 (OR)\",\"m3 (OR)\"),\n          digits = 3, \n          stars = c(0.001, 0.01, 0.05, 0.1),symbol = \"†\")\n\n\nStatistical models\n\n\n\n\n \n\n\nm1 (OR)\n\n\nm2 (OR)\n\n\nm3 (OR)\n\n\n\n\n\n\n(Intercept)\n\n\n0.202***\n\n\n0.359*\n\n\n0.252**\n\n\n\n\n \n\n\n(0.140)\n\n\n(0.424)\n\n\n(0.437)\n\n\n\n\nUnionized\n\n\n1.519**\n\n\n1.477*\n\n\n1.366†\n\n\n\n\n \n\n\n(0.155)\n\n\n(0.157)\n\n\n(0.161)\n\n\n\n\nWaveWave 6\n\n\n1.600**\n\n\n1.646**\n\n\n1.642**\n\n\n\n\n \n\n\n(0.168)\n\n\n(0.171)\n\n\n(0.174)\n\n\n\n\nWaveWave 7\n\n\n0.969\n\n\n0.903\n\n\n0.886\n\n\n\n\n \n\n\n(0.174)\n\n\n(0.181)\n\n\n(0.184)\n\n\n\n\nFemale\n\n\n \n\n\n0.980\n\n\n1.106\n\n\n\n\n \n\n\n \n\n\n(0.136)\n\n\n(0.139)\n\n\n\n\nX003\n\n\n \n\n\n0.993\n\n\n0.988*\n\n\n\n\n \n\n\n \n\n\n(0.006)\n\n\n(0.006)\n\n\n\n\nEduc2\n\n\n \n\n\n1.042\n\n\n0.949\n\n\n\n\n \n\n\n \n\n\n(0.268)\n\n\n(0.272)\n\n\n\n\nEduc3\n\n\n \n\n\n1.684†\n\n\n1.296\n\n\n\n\n \n\n\n \n\n\n(0.283)\n\n\n(0.289)\n\n\n\n\nprivate_sector\n\n\n \n\n\n0.597**\n\n\n0.641*\n\n\n\n\n \n\n\n \n\n\n(0.175)\n\n\n(0.180)\n\n\n\n\npoliticization\n\n\n \n\n\n \n\n\n1.326***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.042)\n\n\n\n\nAIC\n\n\n1475.887\n\n\n1460.059\n\n\n1415.850\n\n\n\n\nBIC\n\n\n1497.024\n\n\n1507.616\n\n\n1468.691\n\n\n\n\nLog Likelihood\n\n\n-733.944\n\n\n-721.030\n\n\n-697.925\n\n\n\n\nDeviance\n\n\n1467.887\n\n\n1442.059\n\n\n1395.850\n\n\n\n\nNum. obs.\n\n\n1457\n\n\n1457\n\n\n1457\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05; †p &lt; 0.1\n\n\n\n\n#Nota: errores estándares en esta tabla NO tienen  sentido (no están calculados a partir de OR, sino de log odds)\n#Es mejor no reportarlos si solo se van a presentar odds ratios\nSin embargo, los coeficientes de un modelo de reg. logística (log-odds u odds-ratios) no son comparables con los coeficientes de otro modelo"
  },
  {
    "objectID": "resource/07-resource.html#cálculo-de-probabilidades-predichas",
    "href": "resource/07-resource.html#cálculo-de-probabilidades-predichas",
    "title": "Práctica 6 Correlación y regresión",
    "section": "Cálculo de probabilidades predichas",
    "text": "Cálculo de probabilidades predichas\n# Tabla básica: sólo sindicalizacion como vble independ\nhtmlreg(m0log,\n          custom.model.names = c(\"m0 (log odds)\"),\n          digits = 3, \n          stars = c(0.001, 0.01, 0.05, 0.1),symbol = \"†\")\n\n\nStatistical models\n\n\n\n\n \n\n\nm0 (log odds)\n\n\n\n\n\n\n(Intercept)\n\n\n-1.422***\n\n\n\n\n \n\n\n(0.074)\n\n\n\n\nUnionized\n\n\n0.374*\n\n\n\n\n \n\n\n(0.153)\n\n\n\n\nAIC\n\n\n1485.340\n\n\n\n\nBIC\n\n\n1495.908\n\n\n\n\nLog Likelihood\n\n\n-740.670\n\n\n\n\nDeviance\n\n\n1481.340\n\n\n\n\nNum. obs.\n\n\n1457\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05; †p &lt; 0.1\n\n\n\n\nA partir de este modelo se pueden predecir log-odds y, más importante aún, probabilidades para personas con distintos atributos controlados en el modelo (ej., sindicalizadas o no)\n\\[logit(prob.marcha) = 𝛼+ 𝛽_1X_1 \\]\n\\[logit(prob.marcha)_{sindical} = -1.422 + (0.374 * Unionized=1) = -1.048 \\]\n\\[logit(prob.marcha)_{no.sindical} = -1.422 + (0.374 * Unionized=0) = -1.422 \\]\nEste “puntaje predicho” (log-odds) no tiene interpretación, por lo que hay que pasarlo a Odds\n\\[Odds_x = e^{𝛼+𝛽_jX_j}\\]\n\\[Odds_{sindicalizados} = e^{-1.048} = 0.35\\]\n\\[Odds_{no.sindicalizados} = e^{-1.422} = 0.24\\]\nFinalmente, habiendo calculado los odds para cada tipo de persona se pueden calcular sus probabilidades predichas\n\\[p = \\frac{e^{𝛼+𝛽_jX_j}}{1+e^{𝛼+𝛽_jX_j}} = \\frac{odds_{xj}}{1+odds_{xj}}\\]\n\\[p_{sindicalizados} = \\frac{0.35}{1+0.35} = \\frac{0.35}{1.35} = 0.26\\]\n\\[p_{no.sindicalizados} = \\frac{0.24}{1+0.24} = \\frac{0.24}{1.24} = 0.19\\]\nLa probabilidad de que un/a sindicalizado participe en marchas es del 26%, mientras que la probabilidad de que alguien que no esté sindicalizado/a es del 19%"
  },
  {
    "objectID": "resource/07-resource.html#cálculo-de-probabilidades-predichas-en-r",
    "href": "resource/07-resource.html#cálculo-de-probabilidades-predichas-en-r",
    "title": "Práctica 6 Correlación y regresión",
    "section": "Cálculo de probabilidades predichas en R",
    "text": "Cálculo de probabilidades predichas en R\n\nPaquete ggeffects de R: últil para estimar probabilidades predichas a partir de modelos de regresión logísticas\nCombinado con ggplot2, se pueden generar gráficos que muestran de modo más intuitivo la relación entre variables\n\nGráfico de probabilidades predichas para sindicalizados/as y no sindicalizados/as\n\nFigSind_1_Prob &lt;- ggeffects::ggpredict(m3log, terms = c(\"Unionized\")) %&gt;%\n  ggplot(aes(x=x, y=predicted)) +\n  geom_bar(stat=\"identity\", color=\"grey\", fill=\"grey\")+\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width=.1) +\n  labs(title=\"Sindicalización\", x = \"\", y = \"\") +\n  theme_bw() +\n  theme(plot.title = element_text(size = 12), \n        axis.text.x = element_text(angle = 0, vjust = 0.5, size = 12),\n        axis.text.y = element_text(vjust = 0.5, size = 10)) +\n  scale_x_continuous(name = \"\",\n                     breaks = c(0,1),\n                     labels = c(\"Non-union members\", \"Union members\")) +\n  scale_y_continuous(limits = c(0,0.35), \n                     breaks = seq(0,0.35, by = 0.05),\n                     labels = scales::percent_format(accuracy = 1L))\n\nFigSind_1_Prob\n\n\n\n\n\n\n\n\nGráfico de probabilidades predichas para variable politización\n\nFigPolit_1_Prob&lt;- ggeffects::ggpredict(m3log, terms=\"politicization\") %&gt;%\n  ggplot(mapping=aes(x = x, y=predicted)) +\n  labs(title=\"Politización\", x = \"\", y = \"\")+\n  theme_bw() +\n  geom_smooth()+\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2, fill = \"black\") +\n  theme(plot.title = element_text(size = 12), \n        axis.text.x = element_text(angle = 0, vjust = 0.5, size = 10),\n        axis.text.y = element_text(vjust = 0.5, size = 10))+\n  scale_x_continuous(breaks = seq(0,6, by = 1))+\n  scale_y_continuous(limits = c(0,0.6), breaks=seq(0,0.6, by = 0.1),\n                     labels = scales::percent_format(accuracy = 1L))\n\nFigPolit_1_Prob\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "resource/07-resource.html#bondad-de-ajuste-comando-de-paquete-lmtest",
    "href": "resource/07-resource.html#bondad-de-ajuste-comando-de-paquete-lmtest",
    "title": "Práctica 6 Correlación y regresión",
    "section": "Bondad de ajuste (comando de paquete lmtest)",
    "text": "Bondad de ajuste (comando de paquete lmtest)\n\nRazón de verosimilitudes\n\n\nanova(m1log, m2log, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: demonstr_dummy ~ Unionized + Wave\nModel 2: demonstr_dummy ~ Unionized + Female + X003 + Educ + private_sector + \n    Wave\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1      1453     1467.9                          \n2      1448     1442.1  5   25.828 9.635e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(m2log, m3log, test = \"Chisq\") \n\nAnalysis of Deviance Table\n\nModel 1: demonstr_dummy ~ Unionized + Female + X003 + Educ + private_sector + \n    Wave\nModel 2: demonstr_dummy ~ Unionized + Female + X003 + Educ + private_sector + \n    politicization + Wave\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1      1448     1442.1                          \n2      1447     1395.8  1    46.21 1.063e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlrtest(m1log, m2log) #likelihood ratio test / Prueba de razón de verosimilitud (comparación m1-m2)\n\nLikelihood ratio test\n\nModel 1: demonstr_dummy ~ Unionized + Wave\nModel 2: demonstr_dummy ~ Unionized + Female + X003 + Educ + private_sector + \n    Wave\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   4 -733.94                         \n2   9 -721.03  5 25.828  9.635e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlrtest(m2log, m3log) #likelihood ratio test / Prueba de razón de verosimilitud (comparación m2-m3)\n\nLikelihood ratio test\n\nModel 1: demonstr_dummy ~ Unionized + Female + X003 + Educ + private_sector + \n    Wave\nModel 2: demonstr_dummy ~ Unionized + Female + X003 + Educ + private_sector + \n    politicization + Wave\n  #Df  LogLik Df Chisq Pr(&gt;Chisq)    \n1   9 -721.03                        \n2  10 -697.92  1 46.21  1.063e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nPseudo R2 (McFadden)\n\n\nm1log_R2&lt;-DescTools::PseudoR2(m1log)\nm2log_R2&lt;-DescTools::PseudoR2(m2log)\nm3log_R2&lt;-DescTools::PseudoR2(m3log)\n\n#Misma tabla, en log odds, con Pseudo R2\nhtmlreg(list(m1log,m2log,m3log),\n          custom.model.names = c(\"m1 (log odds)\",\"m2 (log odds)\",\"m3 (log odds)\"),\n          custom.gof.rows=list(\"Pseudo R2\" = c(m1log_R2, m2log_R2,m3log_R2)),\n          digits = 3, \n          stars = c(0.001, 0.01, 0.05, 0.1),symbol = \"†\")\n\n\nStatistical models\n\n\n\n\n \n\n\nm1 (log odds)\n\n\nm2 (log odds)\n\n\nm3 (log odds)\n\n\n\n\n\n\n(Intercept)\n\n\n-1.601***\n\n\n-1.023*\n\n\n-1.379**\n\n\n\n\n \n\n\n(0.140)\n\n\n(0.424)\n\n\n(0.437)\n\n\n\n\nUnionized\n\n\n0.418**\n\n\n0.390*\n\n\n0.312†\n\n\n\n\n \n\n\n(0.155)\n\n\n(0.157)\n\n\n(0.161)\n\n\n\n\nWaveWave 6\n\n\n0.470**\n\n\n0.498**\n\n\n0.496**\n\n\n\n\n \n\n\n(0.168)\n\n\n(0.171)\n\n\n(0.174)\n\n\n\n\nWaveWave 7\n\n\n-0.031\n\n\n-0.102\n\n\n-0.121\n\n\n\n\n \n\n\n(0.174)\n\n\n(0.181)\n\n\n(0.184)\n\n\n\n\nFemale\n\n\n \n\n\n-0.020\n\n\n0.100\n\n\n\n\n \n\n\n \n\n\n(0.136)\n\n\n(0.139)\n\n\n\n\nX003\n\n\n \n\n\n-0.007\n\n\n-0.012*\n\n\n\n\n \n\n\n \n\n\n(0.006)\n\n\n(0.006)\n\n\n\n\nEduc2\n\n\n \n\n\n0.042\n\n\n-0.053\n\n\n\n\n \n\n\n \n\n\n(0.268)\n\n\n(0.272)\n\n\n\n\nEduc3\n\n\n \n\n\n0.521†\n\n\n0.259\n\n\n\n\n \n\n\n \n\n\n(0.283)\n\n\n(0.289)\n\n\n\n\nprivate_sector\n\n\n \n\n\n-0.516**\n\n\n-0.445*\n\n\n\n\n \n\n\n \n\n\n(0.175)\n\n\n(0.180)\n\n\n\n\npoliticization\n\n\n \n\n\n \n\n\n0.282***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.042)\n\n\n\n\nPseudo R2\n\n\n0.013\n\n\n0.030\n\n\n0.061\n\n\n\n\nAIC\n\n\n1475.887\n\n\n1460.059\n\n\n1415.850\n\n\n\n\nBIC\n\n\n1497.024\n\n\n1507.616\n\n\n1468.691\n\n\n\n\nLog Likelihood\n\n\n-733.944\n\n\n-721.030\n\n\n-697.925\n\n\n\n\nDeviance\n\n\n1467.887\n\n\n1442.059\n\n\n1395.850\n\n\n\n\nNum. obs.\n\n\n1457\n\n\n1457\n\n\n1457\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05; †p &lt; 0.1"
  },
  {
    "objectID": "resource/07-resource.html#efectos-de-interacción",
    "href": "resource/07-resource.html#efectos-de-interacción",
    "title": "Práctica 6 Correlación y regresión",
    "section": "Efectos de interacción",
    "text": "Efectos de interacción\nsindicalizacion - sector privado\n\nm3.1log &lt;- glm(demonstr_dummy~ Unionized + Female + X003 + Educ + private_sector + politicization + Wave + Unionized*private_sector, \n               data = WVS_2005_2022_Chl,family = \"binomial\")\n\nhtmlreg(list(m1log,m2log,m3log,m3.1log),\n          custom.model.names = c(\"M1 (log odds)\",\"M2 (log odds)\",\"M3 (log odds)\",\n                                 \"M3.1 (log odds)\"),\n          digits = 3, \n          stars = c(0.001, 0.01, 0.05, 0.1),symbol = \"†\")\n\n\nStatistical models\n\n\n\n\n \n\n\nM1 (log odds)\n\n\nM2 (log odds)\n\n\nM3 (log odds)\n\n\nM3.1 (log odds)\n\n\n\n\n\n\n(Intercept)\n\n\n-1.601***\n\n\n-1.023*\n\n\n-1.379**\n\n\n-1.452**\n\n\n\n\n \n\n\n(0.140)\n\n\n(0.424)\n\n\n(0.437)\n\n\n(0.447)\n\n\n\n\nUnionized\n\n\n0.418**\n\n\n0.390*\n\n\n0.312†\n\n\n0.569\n\n\n\n\n \n\n\n(0.155)\n\n\n(0.157)\n\n\n(0.161)\n\n\n(0.353)\n\n\n\n\nWaveWave 6\n\n\n0.470**\n\n\n0.498**\n\n\n0.496**\n\n\n0.496**\n\n\n\n\n \n\n\n(0.168)\n\n\n(0.171)\n\n\n(0.174)\n\n\n(0.174)\n\n\n\n\nWaveWave 7\n\n\n-0.031\n\n\n-0.102\n\n\n-0.121\n\n\n-0.122\n\n\n\n\n \n\n\n(0.174)\n\n\n(0.181)\n\n\n(0.184)\n\n\n(0.184)\n\n\n\n\nFemale\n\n\n \n\n\n-0.020\n\n\n0.100\n\n\n0.094\n\n\n\n\n \n\n\n \n\n\n(0.136)\n\n\n(0.139)\n\n\n(0.140)\n\n\n\n\nX003\n\n\n \n\n\n-0.007\n\n\n-0.012*\n\n\n-0.012*\n\n\n\n\n \n\n\n \n\n\n(0.006)\n\n\n(0.006)\n\n\n(0.006)\n\n\n\n\nEduc2\n\n\n \n\n\n0.042\n\n\n-0.053\n\n\n-0.044\n\n\n\n\n \n\n\n \n\n\n(0.268)\n\n\n(0.272)\n\n\n(0.272)\n\n\n\n\nEduc3\n\n\n \n\n\n0.521†\n\n\n0.259\n\n\n0.259\n\n\n\n\n \n\n\n \n\n\n(0.283)\n\n\n(0.289)\n\n\n(0.289)\n\n\n\n\nprivate_sector\n\n\n \n\n\n-0.516**\n\n\n-0.445*\n\n\n-0.360†\n\n\n\n\n \n\n\n \n\n\n(0.175)\n\n\n(0.180)\n\n\n(0.210)\n\n\n\n\npoliticization\n\n\n \n\n\n \n\n\n0.282***\n\n\n0.281***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.042)\n\n\n(0.042)\n\n\n\n\nUnionized:private_sector\n\n\n \n\n\n \n\n\n \n\n\n-0.324\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.397)\n\n\n\n\nAIC\n\n\n1475.887\n\n\n1460.059\n\n\n1415.850\n\n\n1417.185\n\n\n\n\nBIC\n\n\n1497.024\n\n\n1507.616\n\n\n1468.691\n\n\n1475.310\n\n\n\n\nLog Likelihood\n\n\n-733.944\n\n\n-721.030\n\n\n-697.925\n\n\n-697.592\n\n\n\n\nDeviance\n\n\n1467.887\n\n\n1442.059\n\n\n1395.850\n\n\n1395.185\n\n\n\n\nNum. obs.\n\n\n1457\n\n\n1457\n\n\n1457\n\n\n1457\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05; †p &lt; 0.1\n\n\n\n\n\nLa interacción también se puede graficar según probabilidades predichas\n\n\n# ojo que la relación sindicalización x sector privado no es significativa\nFigSindSector_int&lt;-ggeffects::ggpredict(m3.1log, terms = c(\"Unionized\", \"private_sector\")) %&gt;%\n  ggplot(aes(x=x, y=predicted, shape = group, color = group)) +\n  geom_line(aes(group=group,linetype = group),position = position_dodge(.1)) + \n  geom_point(size = 2.5,position = position_dodge(.1))+\n  scale_x_continuous(name = \"\", breaks=c(0,1), labels = c(\"No sindicalizados/as\", \"Sindicalizados/as\")) + \n  scale_shape_discrete(name = \"Sector de empleo\",\n                       limits = c(\"0\", \"1\"),\n                       labels = c(\"Público\", \"Privado\")) +\n  scale_color_manual(name = \"Sector de empleo\",\n                     limits = c(\"0\", \"1\"),\n                     labels = c(\"Público\", \"Privado\"),\n                     values = c(\"black\", \"black\")) +\n  scale_linetype_manual(name = \"Sector de empleo\",\n                        limits = c(\"0\", \"1\"),\n                        labels = c(\"Público\", \"Privado\"),\n                        values = c(\"solid\", \"dashed\")) +\n  scale_y_continuous(limits = c(0,0.40), breaks=seq(0,0.40, by = 0.05),\n                     labels = scales::percent_format(accuracy = 1L)) +\n  theme_bw() +\n  labs(title=\"\", y = \"\") + \n  theme(plot.title = element_text(size = 11),\n        axis.text=element_text(size=11))\n\nFigSindSector_int\n\n\n\n\n\n\n\n\n\nSector de empleo - politización\n\n\nm3.2log &lt;- glm(demonstr_dummy~ Unionized + Female + X003 + Educ + private_sector \n               + politicization + Wave + private_sector*politicization,\n               data = WVS_2005_2022_Chl,family = \"binomial\")\n\nhtmlreg(list(m1log,m2log,m3log,m3.1log,m3.2log),\n          custom.model.names = c(\"M1 (log odds)\",\"M2 (log odds)\",\"M3 (log odds)\",\n                                 \"M3.1 (log odds)\",\"M3.2 (log odds)\"),\n          digits = 3, \n          stars = c(0.001, 0.01, 0.05, 0.1),symbol = \"†\")\n\n\nStatistical models\n\n\n\n\n \n\n\nM1 (log odds)\n\n\nM2 (log odds)\n\n\nM3 (log odds)\n\n\nM3.1 (log odds)\n\n\nM3.2 (log odds)\n\n\n\n\n\n\n(Intercept)\n\n\n-1.601***\n\n\n-1.023*\n\n\n-1.379**\n\n\n-1.452**\n\n\n-1.416**\n\n\n\n\n \n\n\n(0.140)\n\n\n(0.424)\n\n\n(0.437)\n\n\n(0.447)\n\n\n(0.480)\n\n\n\n\nUnionized\n\n\n0.418**\n\n\n0.390*\n\n\n0.312†\n\n\n0.569\n\n\n0.310†\n\n\n\n\n \n\n\n(0.155)\n\n\n(0.157)\n\n\n(0.161)\n\n\n(0.353)\n\n\n(0.161)\n\n\n\n\nWaveWave 6\n\n\n0.470**\n\n\n0.498**\n\n\n0.496**\n\n\n0.496**\n\n\n0.495**\n\n\n\n\n \n\n\n(0.168)\n\n\n(0.171)\n\n\n(0.174)\n\n\n(0.174)\n\n\n(0.174)\n\n\n\n\nWaveWave 7\n\n\n-0.031\n\n\n-0.102\n\n\n-0.121\n\n\n-0.122\n\n\n-0.122\n\n\n\n\n \n\n\n(0.174)\n\n\n(0.181)\n\n\n(0.184)\n\n\n(0.184)\n\n\n(0.184)\n\n\n\n\nFemale\n\n\n \n\n\n-0.020\n\n\n0.100\n\n\n0.094\n\n\n0.100\n\n\n\n\n \n\n\n \n\n\n(0.136)\n\n\n(0.139)\n\n\n(0.140)\n\n\n(0.139)\n\n\n\n\nX003\n\n\n \n\n\n-0.007\n\n\n-0.012*\n\n\n-0.012*\n\n\n-0.012*\n\n\n\n\n \n\n\n \n\n\n(0.006)\n\n\n(0.006)\n\n\n(0.006)\n\n\n(0.006)\n\n\n\n\nEduc2\n\n\n \n\n\n0.042\n\n\n-0.053\n\n\n-0.044\n\n\n-0.053\n\n\n\n\n \n\n\n \n\n\n(0.268)\n\n\n(0.272)\n\n\n(0.272)\n\n\n(0.272)\n\n\n\n\nEduc3\n\n\n \n\n\n0.521†\n\n\n0.259\n\n\n0.259\n\n\n0.258\n\n\n\n\n \n\n\n \n\n\n(0.283)\n\n\n(0.289)\n\n\n(0.289)\n\n\n(0.289)\n\n\n\n\nprivate_sector\n\n\n \n\n\n-0.516**\n\n\n-0.445*\n\n\n-0.360†\n\n\n-0.396\n\n\n\n\n \n\n\n \n\n\n(0.175)\n\n\n(0.180)\n\n\n(0.210)\n\n\n(0.315)\n\n\n\n\npoliticization\n\n\n \n\n\n \n\n\n0.282***\n\n\n0.281***\n\n\n0.298**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.042)\n\n\n(0.042)\n\n\n(0.093)\n\n\n\n\nUnionized:private_sector\n\n\n \n\n\n \n\n\n \n\n\n-0.324\n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.397)\n\n\n \n\n\n\n\nprivate_sector:politicization\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.019\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.103)\n\n\n\n\nAIC\n\n\n1475.887\n\n\n1460.059\n\n\n1415.850\n\n\n1417.185\n\n\n1417.814\n\n\n\n\nBIC\n\n\n1497.024\n\n\n1507.616\n\n\n1468.691\n\n\n1475.310\n\n\n1475.939\n\n\n\n\nLog Likelihood\n\n\n-733.944\n\n\n-721.030\n\n\n-697.925\n\n\n-697.592\n\n\n-697.907\n\n\n\n\nDeviance\n\n\n1467.887\n\n\n1442.059\n\n\n1395.850\n\n\n1395.185\n\n\n1395.814\n\n\n\n\nNum. obs.\n\n\n1457\n\n\n1457\n\n\n1457\n\n\n1457\n\n\n1457\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05; †p &lt; 0.1\n\n\n\n\n\nFigPolitSector_int&lt;-ggeffects::ggpredict(m3.2log, terms = c(\"politicization\", \"private_sector\")) %&gt;%\n  ggplot(aes(x=x, y=predicted, shape = group, color = group)) +\n  geom_line(aes(group=group,linetype = group),position = position_dodge(.1)) + \n  geom_point(size = 2.5,position = position_dodge(.1))+\n  scale_x_continuous(breaks=seq(0,6, by = 1), name = \"\") + \n  scale_shape_discrete(name = \"Sector de empleo\",\n                       limits = c(\"0\", \"1\"),\n                       labels = c(\"Público\", \"Privado\")) +\n  scale_color_manual(name = \"Sector de empleo\",\n                     limits = c(\"0\", \"1\"),\n                     labels = c(\"Público\", \"Privado\"),\n                     values = c(\"black\", \"black\")) +\n  scale_linetype_manual(name = \"Sector de empleo\",\n                        limits = c(\"0\", \"1\"),\n                        labels = c(\"Público\", \"Privado\"),\n                        values = c(\"solid\", \"dashed\")) +\n  scale_y_continuous(limits = c(0,0.8), breaks=seq(0,0.8, by = 0.1),\n                     labels = scales::percent_format(accuracy = 1L)) +\n  theme_bw() +\n  labs(title=\"\", y = \"\") + \n  theme(plot.title = element_text(size = 11),\n        axis.text=element_text(size=11))\nFigPolitSector_int"
  }
]